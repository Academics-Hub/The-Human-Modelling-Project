{"path":".obsidian/plugins/text-extractor/cache/9098a9cdda9efeb379e82b58b319a954.json","text":"The Mathematics of Medical Imaging Charles L. Epstein November 6, 2001 ii Charles L. Epstein Department of Mathematics University of Pennsylvania Philadelphia, PA 19104 cle@math.upenn.edu c⃝ Charles L. Epstein, 2001. All rights reserved. iii This book is dedicated to my wife, Jane and our children, Leo and Sylvia. They make it all worthwhile. iv Preface Over the past several decades advanced mathematics has quietly insinuated itself into many facets of our day to day life. Mathematics is at the heart of technologies from cellular tele- phones and satellite positioning systems to online banking and metal detectors. Arguably no technology has had a more positive and profound e\u000bect on our lives than advances in medical imaging and in no technology is the role of mathematics more pronounced. X-ray tomography, ultrasound, positron emission tomography and magnetic resonance imaging have fundamentally altered the practice of medicine. At the core of each modality is a mathematical model to interpret the measurements and a numerical algorithm to recon- struct an image. While each modality operates on a di\u000berent physical principle and probes a di\u000berent aspect of our anatomy or physiology, there is a large overlap in the mathematics used to model the measurements, design reconstruction algorithms and analyze the e\u000bects of noise. In this text we provide a tool kit, with detailed operating instructions, to work on the sorts of mathematical problems which arise in medical imaging. Our treatment steers a course midway between a complete, rigorous, abstract mathematical discussion and a cookbook engineering approach. The target audience for this book is junior or senior math undergraduates with a \frm command of calculus and linear algebra. The book is written in the language of mathemat- ics, which, as I have learned is quite distinct from the language of physics or the language of engineering. Nonetheless, the discussion of every topic begins at an elementary level and the book should, with a little translation, be usable by advanced science and engineering students with a good mathematical background. A large part of the background material is surveyed in two appendices. Our presentation of these topics is non-standard, situating them in the context of measurement and practical computation. This is a book for people who like to know a little more than they have to know. Our emphasis is squarely on mathematical concepts; only the particulars of X-ray to- mography are discussed in any detail. X-ray tomography is employed as a pedagogical machine, similar in spirit to the elaborate devices used to illustrate the principles of New- tonian mechanics. The physical principles at work in X-ray tomography are simple to describe and require little formal background in physics to understand. This is not the case in any of the other modalities described above or in less developed modalities like infrared imaging and impedence tomography. The mathematical problems that arise in X-ray to- mography and the tools used to solve them have a great deal in common with those used in the other imaging modalities. This is why our title is \\The Mathematics of Medical Imaging\" instead of \\The Mathematics of X-ray tomography.\"A student with a thorough understanding of the material in this book should be well prepared, mathematically for further investigations in any sub\feld of medical imaging. Very good treatments of the v vi PREFACE physical principles underlying the other modalities can be found in Radiological Imaging by Harrison H. Barrett and William Swindell, [4], Principles of Computerized Tomographic Imaging by Avinash C. Kak and Malcolm Slaney, [39], Foundations of Medical Imaging by Cho, Jones, Singh, [86], Image reconstruction from Projections by Gabor T. Herman, [24] and Magnetic Resonance Imaging by E. Mark Haacke, Robert W. Brown, Michael R. Thompson, Ramesh Venkatesan, [80]. Indeed these books were invaluable sources as I learned the subject myself. My treatment of many topics doubtless owes a great deal to these books. Graduate level treatments of the mathematics and algorithms can be found in The Mathematics of Computerized Tomography by Frank Natterer, [50] and Mathematical Methods in Image Reconstruction by Frank Natterer and Frank W¨ubbelling, [51]. The book begins with an introduction to the idea of using a mathematical model as a tool to extract the physical state of system from feasible measurements. In medical imaging, the \\state of the system\" in question is the anatomy and physiology of a living human being. To probe it non-destructively requires considerable ingenuity and sophisticated mathematics. After considering a variety of examples, each a toy problem for some aspect of medical imaging we turn to a description of X-ray tomography. This leads us to our \frst mathematical topic, integral transforms. The transform of immediate interest is the Radon transform, though we are quickly led to the Abel transform, Hilbert transform and the Fourier transform. Our study of the Fourier transform is dictated by the applications we have in mind, with a strong emphasis on the connection between the smoothness of a function and the decay of its Fourier transform and vice versa. Many of the basic ideas of functional analysis appear as we consider these examples. The concept of a weak derivative, which is ubiquitous in the engineering literature and essential to a precise understanding of the Radon inversion formula is described in detail. This part of the book culminates in a study of the Radon inversion formula. A major theme in these chapters is the di\u000berence between \fnite and in\fnite dimensional linear algebra. The next topics we consider are sampling and \fltering theory. These form the basis for applying the mathematics of the Fourier transform to real world problems. In the chapter on sampling theory we discuss the Nyquist theorem, the Shannon-Whittaker interpolation formula, the Poisson summation formula and the consequences of undersampling. In the chapter on \fltering theory we recast Fourier analysis as a tool for image and signal pro- cessing. We then discuss the mathematics of approximating continuous time, linear shift invariant \flters on \fnitely sampled data, using the \fnite Fourier transform. The chapter concludes with an overview of image processing and a linear systems analysis of some basic imaging hardware. In Chapter eight these tools are applied to the problem of image reconstruction in X-ray tomography. Most of the chapter is devoted to the \fltered backprojection algorithm, though other methods are brieﬂy considered. After deriving the reconstruction algorithms we analyze the point spread function and modulation transfer function of the full measurement and reconstruction process. We also use this formalism to analyze a variety of imaging artifacts. Chapter nine contains a brief description of \\algebraic reconstruction techniques,\" which are essentially methods for solving large, sparse systems of linear equations The \fnal topic is noise in the backprojection algorithm. This part of the book begins with an introduction to probability theory. Our presentation uses the ideas of measure theory, though in a metaphoric rather than a technical way. The chapter concludes with a study of speci\fc probability distributions that are important in imaging. The next chapter vii introduces the ideas of random processes and their role in signal and image processing. Again the focus is on those processes needed to analyze noise in X-ray imaging. A student with a good grasp of Riemann integration should not have di\u000eculty with the material in these chapters. In the \fnal chapter we study the e\u000bects of noise on the image reconstruction process. This chapter culminates with the famous resolution-dosage fourth power relation, which shows that to double the resolution in a CT-image, keeping the SNR constant, the radiation dosage must be increased by a factor of 16! Each chapter builds from elementary material to more advanced material as is also the case with the longer sections. The elementary material in each chapter (or section) depends only on the elementary material in previous chapters. Sections which cover noticeably more advanced material, with more prerequisites, are marked with an asterisk. Several of these sections assume a familiarity with the elementary parts of the theory of function of a complex variable. All can be omitted without any loss in continuity. Many sections begin with a box containing a list of sections in the appendices, these are recommended background readings. A one semester course in the mathematics of medical imaging can be fashioned from Chapter 1, 2.1-2.4 (omitting the\u0003-sections), 2.5 (up to 2.5.1), 3.1, 3.2 (up to 3.2.9), 3.3 (up to 3.3.2), 4.1-4.5 (omitting the \u0003-sections), 5.1-5.5 (omitting the\u0003-sections), 6.1-6.2.2, 6.3, 7.1 (up to 7.1.7), 7.3, 7.5, 8.1-8.5 (omitting the \u0003-sections). Though given the diversity of students interested in this \feld the subject matter must be tailored, each semester to the suit the needs of those actually sitting in the room. Exercises are collected at the ends of the sections and sub-sections. Most develop ideas presented in the text, only a few are of a standard, computational character. Acknowledgments Perhaps the best reward for writing a book of this type is the opportunity it a\u000bords for thanking the many people who contributed to it in one way or another. There are a lot of people to thank and I address them in roughly chronological order. First I would like to thank my parents, Jean and Herbert Epstein, for their encour- agement to follow my dreams and the very high standards they set for me from earliest childhood. I would also like to thank my father and Robert M. Goodman for giving me the idea that, through careful thought and observation, the world can be understood and the importance of expressing ideas simply but carefully. My years as an undergraduate at MIT not only provided me with a solid background in mathematics, physics and engineering but also reinforced my belief in the unity of scienti\fc enquiry. I am especially grateful for the time and attention Jerry Lettvin lavished on me. My interest in the intricacies of physical measurement surely grew out of our many conversations. I was fortunate to be a graduate student at the Courant Institute, one of the few places where mathematics and its appli- cations lived together in harmony. In both word and deed, my thesis advisor, Peter Lax placed mathematics and its applications on an absolutely equal footing. It was a privilege to be his student. I am very grateful for the enthusiasm that he and his late wife, Anneli showed for turning my lecture notes into a book. Coming closer to the present day, I would like to thank Dennis Deturck for his unﬂagging support, both material (in the form of NSF grant DUE95-52464) and emotional for the development of my course on medical imaging and this book. The \frst version of these notes were transcribed from my lectures in the spring of 1999 by Hyunsuk Kang. Without her hard work it is very unlikely I would ever have embarked on this project. I am very grateful to my viii PREFACE colleagues in the Radiology department Gabor Herman, Peter Joseph and Felix Wehrli for sharing with me their profound, \frst hand knowledge of medical imaging. Gabor Herman's computer program, SNARK93 was used to make the simulated reconstructions in this book; Peter Joseph and Felix Wehrli provided many other images. I am most appreciative for the X-ray spectrum provided by Dr. Andrew Karellas (\fgure 2.7). I would like to thank John D'Angelo and Phil Nelson for their help with typesetting, publishing and the writing process itself and Fred Villars for sharing with me his insights on medicine, imaging and a host of other topics. The con\fdence my editor, George Lobell, expressed in the importance of this project was an enormous help in the long months it took to \fnish it. Finally I would like to thank my wife, Jane and our children, Leo and Sylvia for their love, constant support and daily encouragement through the many, moody months. With- out them I would have given up a long time ago. Charles L. Epstein Philadelphia, PA October 17, 2001 Contents Preface v 1 Measurements and modeling 1 1.1 Mathematical modeling ... ... .... ... ... .... ... .... .. 3 1.1.1 Finitely many degrees of freedom . ... ... .... ... .... .. 3 1.1.2 In\fnitely many degrees of freedom . . . . . . .... ... .... .. 7 1.2 A simple model problem for image reconstruction .. .... ... .... .. 13 1.2.1 The space of lines in the plane .. ... ... .... ... .... .. 14 1.2.2 Reconstructing an object from its shadows . .... ... .... .. 15 1.2.3 Approximate reconstructions ... ... ... .... ... .... .. 18 1.2.4 Can an object be reconstructed from its width? . . . . . . .... .. 20 1.3 Linearity .. ... ... .... ... .... ... ... .... ... .... .. 21 1.3.1 Solving linear equations .. .... ... ... .... ... .... .. 23 1.3.2 In\fnite dimensional linear algebra . . . . . . .... ... .... .. 28 1.4 Conclusion ... ... .... ... .... ... ... .... ... .... .. 31 2 A basic model for tomography 33 2.1 Tomography ... ... .... ... .... ... ... .... ... .... .. 33 2.1.1 Beer's law and X-ray tomography . . . . . . .... ... .... .. 36 2.2 Analysis of a point source device . .... ... ... .... ... .... .. 41 2.3 Some physical considerations ... .... ... ... .... ... .... .. 44 2.4 The de\fnition of the Radon transform .. ... ... .... ... .... .. 46 2.4.1 Appendix: Proof of Lemma 2.4.1* ... ... .... ... .... .. 51 2.4.2 Continuity of the Radon transform\u0003 .. ... .... ... .... .. 52 2.4.3 The backprojection formula .... ... ... .... ... .... .. 55 2.5 The Radon transform of a radially symmetric function ... ... .... .. 56 2.5.1 The range of the radial Radon transform\u0003 .. .... ... .... .. 57 2.5.2 The Abel transform\u0003 . ... .... ... ... .... ... .... .. 59 2.5.3 Fractional derivatives\u0003 ... .... ... ... .... ... .... .. 61 2.5.4 Volterra equations of the \frst kind\u0003 .. ... .... ... .... .. 62 3 Introduction to the Fourier transform 67 3.1 The complex exponential function. .... ... ... .... ... .... .. 67 3.2 Functions of a single variable ... .... ... ... .... ... .... .. 68 3.2.1 Absolutely integrable functions . . ... ... .... ... .... .. 69 ix x CONTENTS 3.2.2 Appendix: The Fourier transform of a Gaussian\u0003 .. ... .... .. 72 3.2.3 Regularity and decay . ... .... ... ... .... ... .... .. 73 3.2.4 Fourier transform on L2(R) .... ... ... .... ... .... .. 79 3.2.5 Basic properties of the Fourier Transform onR... ... .... .. 83 3.2.6 Convolution .. .... ... .... ... ... .... ... .... .. 84 3.2.7 Convolution equations ... .... ... ... .... ... .... .. 90 3.2.8 The \u000e-function .... ... .... ... ... .... ... .... .. 92 3.2.9 Windowing and resolution . .... ... ... .... ... .... .. 94 3.2.10 Functions with L2-derivatives\u0003 .. ... ... .... ... .... .. 97 3.2.11 Fractional derivatives and L2-derivatives\u0003 .. .... ... .... .. 99 3.2.12 Some re\fned properties of the Fourier transform\u0003 .. ... .... .. 101 3.2.13 The Fourier transform of generalized functions\u0003 ... ... .... .. 105 3.2.14 The Paley-Wiener theorem\u0003 .... ... ... .... ... .... .. 111 3.3 Functions of several variables ... .... ... ... .... ... .... .. 112 3.3.1 L1-case . ... .... ... .... ... ... .... ... .... .. 113 3.3.2 Regularity and decay . ... .... ... ... .... ... .... .. 116 3.3.3 L2-theory ... .... ... .... ... ... .... ... .... .. 119 3.3.4 Basic properties of the Fourier Transform onRn .. ... .... .. 121 3.3.5 Convolution .. .... ... .... ... ... .... ... .... .. 122 3.3.6 The support of f \u0003 g: . ... .... ... ... .... ... .... .. 125 3.3.7 L2-derivatives\u0003 .... ... .... ... ... .... ... .... .. 126 3.3.8 The failure of localization in higher dimensions\u0003 .. ... .... .. 130 4 The Radon transform 131 4.1 The Radon transform .... ... .... ... ... .... ... .... .. 131 4.2 Inversion of the Radon Transform .... ... ... .... ... .... .. 135 4.2.1 The Central slice theorem . .... ... ... .... ... .... .. 135 4.2.2 The Radon Inversion Formula ... ... ... .... ... .... .. 138 4.2.3 Backprojection\u0003 ... ... .... ... ... .... ... .... .. 141 4.2.4 Filtered Backprojection . . .... ... ... .... ... .... .. 143 4.2.5 Inverting the Radon transform, two examples .... ... .... .. 145 4.2.6 An alternate formula for the Radon inverse\u0003 . .... ... .... .. 148 4.3 The Hilbert transform .... ... .... ... ... .... ... .... .. 149 4.3.1 Mapping properties of the Hilbert transform\u0003 .... ... .... .. 154 4.4 Approximate inverses for the Radon transform ... .... ... .... .. 154 4.4.1 Addendum\u0003 .. .... ... .... ... ... .... ... .... .. 156 4.5 The range of the Radon transform .... ... ... .... ... .... .. 157 4.5.1 Data with bounded support .... ... ... .... ... .... .. 158 4.5.2 More general data\u0003 .. ... .... ... ... .... ... .... .. 160 4.6 Continuity of the Radon transform and its inverse . .... ... .... .. 164 4.6.1 Bounded support . . . . . . .... ... ... .... ... .... .. 164 4.6.2 Estimates for the inverse transform\u0003 .. ... .... ... .... .. 166 4.7 The higher dimensional Radon transform\u0003 ... ... .... ... .... .. 170 4.8 The Hilbert transform and complex analysis\u0003 . ... .... ... .... .. 173 CONTENTS xi 5 Introduction to Fourier series 177 5.1 Fourier series in one dimension .. .... ... ... .... ... .... .. 177 5.2 Decay of Fourier coe\u000ecients . ... .... ... ... .... ... .... .. 183 5.3 L2-theory . ... ... .... ... .... ... ... .... ... .... .. 186 5.3.1 Geometry in L2([0; 1]): ... .... ... ... .... ... .... .. 186 5.3.2 Bessel's inequality . . ... .... ... ... .... ... .... .. 191 5.3.3 L2-derivatives\u0003 .... ... .... ... ... .... ... .... .. 193 5.4 General periodic functions . ... .... ... ... .... ... .... .. 196 5.4.1 Convolution and partial sums ... ... ... .... ... .... .. 197 5.4.2 Dirichlet kernel.... ... .... ... ... .... ... .... .. 199 5.5 The Gibbs Phenomenon ... ... .... ... ... .... ... .... .. 200 5.5.1 The general Gibbs phenomenon . . ... ... .... ... .... .. 204 5.5.2 Fejer means . . .... ... .... ... ... .... ... .... .. 206 5.5.3 Resolution ... .... ... .... ... ... .... ... .... .. 209 5.6 The localization principle\u0003 .. ... .... ... ... .... ... .... .. 211 5.7 Higher dimensional Fourier series . .... ... ... .... ... .... .. 213 5.7.1 L2-theory ... .... ... .... ... ... .... ... .... .. 216 6 Sampling 219 6.1 Sampling and Nyquist's theorem . .... ... ... .... ... .... .. 220 6.1.1 Nyquist's theorem .. ... .... ... ... .... ... .... .. 220 6.1.2 Shannon-Whittaker Interpolation . ... ... .... ... .... .. 222 6.2 The Poisson Summation Formula . .... ... ... .... ... .... .. 225 6.2.1 The Poisson summation formula . ... ... .... ... .... .. 225 6.2.2 Undersampling and aliasing .... ... ... .... ... .... .. 228 6.2.3 Sub-sampling . .... ... .... ... ... .... ... .... .. 234 6.2.4 Sampling periodic functions .... ... ... .... ... .... .. 235 6.2.5 Quantization errors .. ... .... ... ... .... ... .... .. 237 6.3 Higher dimensional sampling ... .... ... ... .... ... .... .. 239 7 Filters 243 7.1 Basic de\fnitions ... .... ... .... ... ... .... ... .... .. 243 7.1.1 Examples of \flters . . ... .... ... ... .... ... .... .. 244 7.1.2 Linear \flters . .... ... .... ... ... .... ... .... .. 246 7.1.3 Shift invariant \flters . ... .... ... ... .... ... .... .. 248 7.1.4 Harmonic components ... .... ... ... .... ... .... .. 249 7.1.5 The transfer function ... .... ... ... .... ... .... .. 252 7.1.6 The \u000e-function revisited .. .... ... ... .... ... .... .. 254 7.1.7 Causal \flters . .... ... .... ... ... .... ... .... .. 256 7.1.8 Bandpass \flters .... ... .... ... ... .... ... .... .. 257 7.1.9 Resolution\u0003 .. .... ... .... ... ... .... ... .... .. 260 7.1.10 Cascaded \flters .... ... .... ... ... .... ... .... .. 263 7.1.11 The resolution of a cascade of \flters\u0003 .. ... .... ... .... .. 265 7.1.12 Filters and RLC-circuits\u0003 . .... ... ... .... ... .... .. 267 7.2 Filtering periodic signals ... ... .... ... ... .... ... .... .. 275 7.2.1 Resolution of periodic \flters\u0003 ... ... ... .... ... .... .. 278 xii CONTENTS 7.2.2 The comb \flter and Poisson summation ... .... ... .... .. 279 7.3 The inverse \flter ... .... ... .... ... ... .... ... .... .. 281 7.4 Higher dimensional \flters . . ... .... ... ... .... ... .... .. 285 7.5 Implementing shift invariant \flters .... ... ... .... ... .... .. 290 7.5.1 Sampled data . .... ... .... ... ... .... ... .... .. 291 7.5.2 The \fnite Fourier transform .... ... ... .... ... .... .. 293 7.5.3 Approximation of Fourier coe\u000ecients .. ... .... ... .... .. 295 7.5.4 Implementing periodic convolutions on sampled data ... .... .. 297 7.5.5 Implementing \flters on \fnitely sampled data .... ... .... .. 298 7.5.6 Zero padding reconsidered . .... ... ... .... ... .... .. 301 7.5.7 Higher dimensional \flters . .... ... ... .... ... .... .. 302 7.5.8 Appendix: The Fast Fourier Transform ... .... ... .... .. 306 7.6 Image processing ... .... ... .... ... ... .... ... .... .. 308 7.6.1 Basic concepts and operations ... ... ... .... ... .... .. 309 7.6.2 Discretized images . . ... .... ... ... .... ... .... .. 318 7.7 General linear \flters\u0003 . .... ... .... ... ... .... ... .... .. 323 7.8 Linear \flter analysis of imaging hardware\u0003 ... ... .... ... .... .. 324 7.8.1 The transfer function of the scanner . . ... .... ... .... .. 325 7.8.2 The resolution of an imaging system .. ... .... ... .... .. 329 7.8.3 Collimators . . .... ... .... ... ... .... ... .... .. 331 8 Reconstruction in X-ray tomography 337 8.1 Reconstruction formulˆ ... ... .... ... ... .... ... .... .. 340 8.2 Scanner geometries . . .... ... .... ... ... .... ... .... .. 342 8.3 Reconstruction algorithms for a parallel beam machine ... ... .... .. 347 8.3.1 Direct Fourier inversion .. .... ... ... .... ... .... .. 347 8.3.2 Filtered backprojection . . .... ... ... .... ... .... .. 348 8.3.3 Ram-Lak \flters .... ... .... ... ... .... ... .... .. 350 8.3.4 Shepp-Logan analysis of the Ram-Lak \flters .... ... .... .. 352 8.3.5 Sample spacing in a parallel beam machine . .... ... .... .. 356 8.4 Filtered backprojection in the fan-beam case . . . . .... ... .... .. 358 8.4.1 Fan beam geometry . ... .... ... ... .... ... .... .. 358 8.4.2 Fan beam \fltered backprojection . . . . . . . .... ... .... .. 361 8.4.3 Implementing the fan beam algorithm . ... .... ... .... .. 363 8.4.4 Data collection for a fan beam scanner . . . . .... ... .... .. 364 8.4.5 Rebinning ... .... ... .... ... ... .... ... .... .. 366 8.5 The e\u000bect of a \fnite width X-ray beam . ... ... .... ... .... .. 366 8.5.1 A non-linear e\u000bect . . . . . .... ... ... .... ... .... .. 368 8.5.2 The partial volume e\u000bect . .... ... ... .... ... .... .. 369 8.5.3 Some mathematical remarks\u0003 ... ... ... .... ... .... .. 371 8.6 The PSF . . ... ... .... ... .... ... ... .... ... .... .. 373 8.6.1 The PSF without sampling .... ... ... .... ... .... .. 374 8.6.2 The PSF with sampling . . .... ... ... .... ... .... .. 381 8.6.3 View sampling .... ... .... ... ... .... ... .... .. 385 8.6.4 Bad rays versus bad views . .... ... ... .... ... .... .. 393 8.6.5 Beam hardening ... ... .... ... ... .... ... .... .. 398 CONTENTS xiii 8.7 The gridding method . .... ... .... ... ... .... ... .... .. 401 8.8 Concluding remarks . .... ... .... ... ... .... ... .... .. 404 9 Algebraic reconstruction techniques 407 9.1 Algebraic reconstruction ... ... .... ... ... .... ... .... .. 407 9.2 Kaczmarz' method .. .... ... .... ... ... .... ... .... .. 411 9.3 A Bayesian estimate . .... ... .... ... ... .... ... .... .. 417 9.4 Variants of the Kaczmarz method . .... ... ... .... ... .... .. 418 9.4.1 Relaxation parameters . . . .... ... ... .... ... .... .. 418 9.4.2 Other related algorithms . . .... ... ... .... ... .... .. 420 10 Probability and Random Variables 423 10.1 Measure theory . ... .... ... .... ... ... .... ... .... .. 424 10.1.1 Allowable events ... ... .... ... ... .... ... .... .. 424 10.1.2 Measures and probability . .... ... ... .... ... .... .. 427 10.1.3 Integration .. .... ... .... ... ... .... ... .... .. 429 10.1.4 Independent events .. ... .... ... ... .... ... .... .. 436 10.1.5 Conditional probability . . .... ... ... .... ... .... .. 437 10.2 Random variables ... .... ... .... ... ... .... ... .... .. 439 10.2.1 Cumulative distribution function . ... ... .... ... .... .. 441 10.2.2 The variance . .... ... .... ... ... .... ... .... .. 444 10.2.3 The characteristic function .... ... ... .... ... .... .. 445 10.2.4 A pair of random variables .... ... ... .... ... .... .. 446 10.2.5 Several random variables . .... ... ... .... ... .... .. 452 10.3 Some important random variables .... ... ... .... ... .... .. 454 10.3.1 Bernoulli Random Variables .... ... ... .... ... .... .. 455 10.3.2 Poisson Random Variables . .... ... ... .... ... .... .. 455 10.3.3 Gaussian Random Variables .... ... ... .... ... .... .. 456 10.3.4 The Central Limit Theorem .... ... ... .... ... .... .. 459 10.3.5 Limits of random variables .... ... ... .... ... .... .. 461 10.3.6 Modeling a source-detector pair . . . . . . . . .... ... .... .. 465 10.3.7 Beer's Law . . .... ... .... ... ... .... ... .... .. 466 10.4 Statistics and measurements . ... .... ... ... .... ... .... .. 469 11 Random Processes 473 11.1 Random processes in measurements ... ... ... .... ... .... .. 473 11.2 Basic de\fnitions ... .... ... .... ... ... .... ... .... .. 474 11.2.1 Statistical properties of random processes . . .... ... .... .. 477 11.2.2 Stationary random processes ... ... ... .... ... .... .. 478 11.2.3 Independent and stationary increments ... .... ... .... .. 482 11.3 Examples of random processes . . .... ... ... .... ... .... .. 482 11.3.1 Gaussian random process . .... ... ... .... ... .... .. 483 11.3.2 The Poisson counting process ... ... ... .... ... .... .. 483 11.3.3 Poisson arrival process ... .... ... ... .... ... .... .. 486 11.3.4 Fourier coe\u000ecients for periodic processes ... .... ... .... .. 488 11.3.5 White noise . . .... ... .... ... ... .... ... .... .. 491 xiv CONTENTS 11.4 Random inputs to linear systems . .... ... ... .... ... .... .. 493 11.4.1 The autocorrelation of the output ... ... .... ... .... .. 494 11.4.2 Thermal or Johnson noise . .... ... ... .... ... .... .. 496 11.4.3 Optimal \flters .... ... .... ... ... .... ... .... .. 498 12 Resolution and noise 501 12.1 The continuous case . .... ... .... ... ... .... ... .... .. 502 12.2 Sampled data .. ... .... ... .... ... ... .... ... .... .. 504 12.3 A computation of the variance .. .... ... ... .... ... .... .. 507 12.3.1 The variance of the Radon transform .. ... .... ... .... .. 508 12.3.2 The variance in the reconstructed image . . . .... ... .... .. 510 12.3.3 Signal-to-noise ratio, dosage and contrast . . .... ... .... .. 512 A Background material 515 A.1 Numbers .. ... ... .... ... .... ... ... .... ... .... .. 515 A.1.1 Integers . ... .... ... .... ... ... .... ... .... .. 515 A.1.2 Rational numbers ... ... .... ... ... .... ... .... .. 517 A.1.3 Real numbers . .... ... .... ... ... .... ... .... .. 520 A.1.4 Cauchy sequences .. ... .... ... ... .... ... .... .. 523 A.2 Vector spaces . . . . . .... ... .... ... ... .... ... .... .. 524 A.2.1 Euclidean n-space .. ... .... ... ... .... ... .... .. 525 A.2.2 General vector spaces . . . .... ... ... .... ... .... .. 528 A.2.3 Linear Transformations and matrices .. ... .... ... .... .. 531 A.2.4 Norms and Metrics . . . . . .... ... ... .... ... .... .. 536 A.2.5 Inner product structure .. .... ... ... .... ... .... .. 540 A.2.6 Linear transformations and linear equations . .... ... .... .. 545 A.2.7 Linear algebra with uncertainties . ... ... .... ... .... .. 547 A.2.8 The least squares method . .... ... ... .... ... .... .. 549 A.2.9 Complex numbers and the Euclidean plane . .... ... .... .. 550 A.2.10 Complex vector spaces . . . .... ... ... .... ... .... .. 553 A.3 Functions, theory and practice .. .... ... ... .... ... .... .. 554 A.3.1 In\fnite series . .... ... .... ... ... .... ... .... .. 556 A.3.2 Partial summation .. ... .... ... ... .... ... .... .. 559 A.3.3 Power series .. .... ... .... ... ... .... ... .... .. 560 A.3.4 Binomial formula ... ... .... ... ... .... ... .... .. 563 A.3.5 The Gamma function ... .... ... ... .... ... .... .. 565 A.3.6 Bessel functions .... ... .... ... ... .... ... .... .. 567 A.4 Spaces of functions . . .... ... .... ... ... .... ... .... .. 569 A.4.1 Examples of function spaces .... ... ... .... ... .... .. 569 A.4.2 Completeness . .... ... .... ... ... .... ... .... .. 573 A.4.3 Linear functionals . . ... .... ... ... .... ... .... .. 575 A.4.4 Measurement, linear functionals and weak convergence . . .... .. 577 A.4.5 The L2-case . . .... ... .... ... ... .... ... .... .. 579 A.4.6 Generalized functions onR.... ... ... .... ... .... .. 581 A.4.7 Generalized functions onRn : ... ... ... .... ... .... .. 587 A.5 Bounded linear operators . . . . . .... ... ... .... ... .... .. 589 CONTENTS xv A.6 Functions in the real world . ... .... ... ... .... ... .... .. 594 A.6.1 Approximation .... ... .... ... ... .... ... .... .. 594 A.6.2 Sampling and Interpolation .... ... ... .... ... .... .. 600 A.7 Numerical techniques for di\u000berentiation and integration .. ... .... .. 603 A.7.1 Numerical integration ... .... ... ... .... ... .... .. 605 A.7.2 Numerical di\u000berentiation . .... ... ... .... ... .... .. 607 B Basic analysis 613 B.1 Sequences . ... ... .... ... .... ... ... .... ... .... .. 613 B.2 Rules for Limits . ... .... ... .... ... ... .... ... .... .. 614 B.3 Existence of limits . . .... ... .... ... ... .... ... .... .. 614 B.4 Series ... ... ... .... ... .... ... ... .... ... .... .. 615 B.5 Limits of Functions and Continuity .... ... ... .... ... .... .. 618 B.6 Di\u000berentiability . . . . .... ... .... ... ... .... ... .... .. 620 B.7 Higher Order Derivatives and Taylor's Theorem ... .... ... .... .. 621 B.8 Integration ... ... .... ... .... ... ... .... ... .... .. 621 B.9 Improper integrals .. .... ... .... ... ... .... ... .... .. 624 xvi CONTENTS List of Figures 1.1 The world of old fashioned X-rays. .... ... ... .... ... .... .. 1 1.2 Using to trigonometry to \fnd the height of a mountain. . . ... .... .. 4 1.3 A more realistic measurement. . . .... ... ... .... ... .... .. 5 1.4 Not exactly what we predicted! . . .... ... ... .... ... .... .. 6 1.5 Using refraction to determine the height of an interface. . . . . . .... .. 7 1.6 Convex and non-convex regions. .. .... ... ... .... ... .... .. 8 1.7 Using particle scattering to determine the boundary of a convex region. . . 9 1.8 Using sound to measure depth. . . .... ... ... .... ... .... .. 10 1.9 The shadow of a convex region . . .... ... ... .... ... .... .. 13 1.10 Parameterization of oriented lines in the plane. . . . .... ... .... .. 15 1.11 The measurement of the shadow . .... ... ... .... ... .... .. 16 1.12 Two regions of constant width 2 . .... ... ... .... ... .... .. 21 2.1 Parallel slices of an object. . . . . . .... ... ... .... ... .... .. 35 2.2 Analysis of an isotropic point source. ... ... ... .... ... .... .. 38 2.3 The failure of ordinary X-rays to distinguish objects..... ... .... .. 40 2.4 A di\u000berent projection. .... ... .... ... ... .... ... .... .. 40 2.5 A point source device for measuring line integrals of the absorption coe\u000ecient. 41 2.6 Collecting data from many views. . .... ... ... .... ... .... .. 43 2.7 A typical X-ray source spectral function, courtesy Dr. Andrew Kavellas. . . 44 2.8 Back-projection does not work! . . .... ... ... .... ... .... .. 56 3.1 Furry functions . ... .... ... .... ... ... .... ... .... .. 76 3.2 A furry function at smaller scales . .... ... ... .... ... .... .. 76 3.3 Graphs of '\u000f; with \u000f = :5; 2; 8. ... .... ... ... .... ... .... .. 87 3.4 Approximate \u000e-functions ... ... .... ... ... .... ... .... .. 93 3.5 Approximate \u000e-functions convolved with ˜[−1;1]: ... .... ... .... .. 94 3.6 Illustration of the FWHM de\fnition of resolution .. .... ... .... .. 95 3.7 FWHM vs. side-lobes .... ... .... ... ... .... ... .... .. 96 3.8 Real and imaginary parts of exp(ih(x; y); (1; 1)i) ... .... ... .... .. 114 3.9 Real and imaginary parts of exp(ih(x; y); (2; 0)i) ... .... ... .... .. 114 3.10 f is smeared into the \u000f-neighborhood of supp(f ): .. .... ... .... .. 126 4.1 Graphs of ^ and k with W =40;C =5: ... ... .... ... .... .. 167 4.2 Radial graph of k \u0003 ˜D1; with W =40;C =5: ... .... ... .... .. 168 xvii xviii LIST OF FIGURES 5.1 Periodic extension may turn a continuous function into discontinuous function.184 5.2 Graph of the Dirichlet kernel, D3(x) ... ... ... .... ... .... .. 199 5.3 An example of the Gibbs phenomenon . . ... ... .... ... .... .. 201 5.4 Detail showing equi-oscillation property in Gibbs phenomenon . .... .. 204 5.5 Graph of the Fejer kernel, F5(x) . .... ... ... .... ... .... .. 207 5.6 Graphs comparing the partial sums and Fejer means. .... ... .... .. 210 5.7 Expanded view showing the loss of resolution in the Fejer means. .... .. 210 5.8 Expanded view showing Gibbs phenomenon in the partial sums. .... .. 211 5.9 Illustration of the 2d-Gibbs phenomenon . ... ... .... ... .... .. 215 6.1 Window functions in Fourier space and the ordinary sinc-pulse. . .... .. 224 6.2 Shannon-Whittaker interpolation functions with second order smoothed win- dows. ... ... ... .... ... .... ... ... .... ... .... .. 224 6.3 Aliasing in MRI . ... .... ... .... ... ... .... ... .... .. 227 6.4 The two faces of aliasing, d = :05: . .... ... ... .... ... .... .. 230 6.5 Partial Fourier inverse and Shannon-Whittaker interpolant. ... .... .. 230 6.6 What aliasing looks like for a smoother function, d = :1: .. ... .... .. 231 6.7 What aliasing looks like for a furry function, d = :1;:05;:025: .. .... .. 231 7.1 The e\u000bects of errors in the amplitude and phase of the Fourier transform on a reconstructed image. .... ... .... ... ... .... ... .... .. 250 7.2 Using magnetic resonance to determine the vibrational modes of a molecule. 251 7.3 Transfer function for a tent \flter. . .... ... ... .... ... .... .. 258 7.4 Pointspread functions for lowpass \flters . ... ... .... ... .... .. 259 7.5 A network . ... ... .... ... .... ... ... .... ... .... .. 268 7.6 Standard symbols for passive circuit elements . ... .... ... .... .. 268 7.7 Simple RLC-circuits . .... ... .... ... ... .... ... .... .. 269 7.8 The amplitude and phase of the transfer function of an RC-\flter..... .. 270 7.9 The amplitude and phase of the transfer function of an RL-\flter .... .. 272 7.10 A resonant circuit. . . .... ... .... ... ... .... ... .... .. 273 7.11 An RLC-circuit. ... .... ... .... ... ... .... ... .... .. 273 7.12 The amplitude of the transfer function. . ... ... .... ... .... .. 274 7.13 A second RLC-circuit. .... ... .... ... ... .... ... .... .. 275 7.14 Modi\fed Fourier Transform of the rectangle function .... ... .... .. 283 7.15 Impulse responses for 2-dimensional low pass \flters. .... ... .... .. 286 7.16 Low pass \flters in two dimensions. .... ... ... .... ... .... .. 287 7.17 Half maximum curves for 2d low pass \flters. .. ... .... ... .... .. 289 7.18 Bad interpolation using formula 7.55. ... ... ... .... ... .... .. 296 7.19 The Fourier transform of an image is not usually an image. ... .... .. 309 7.20 Removing geometric distortion. . . .... ... ... .... ... .... .. 310 7.21 The impulse response and transfer function for A:25. .... ... .... .. 315 7.22 Output of Laplacian edge detection \flters. . . . . . . .... ... .... .. 316 7.23 A CT-head phantom showing the e\u000bect of rescaling grey values. . .... .. 317 7.24 The Moir\u0013e e\u000bect is directional aliasing. . . . . . . . . .... ... .... .. 318 7.25 Arrangement of an imaging device with a source, object and detector. . . . 325 7.26 Computing the solid angle. . ... .... ... ... .... ... .... .. 326 LIST OF FIGURES xix 7.27 The similar triangle calculation. . . .... ... ... .... ... .... .. 327 7.28 A pinhole camera. . . .... ... .... ... ... .... ... .... .. 328 7.29 The image of two dots. .... ... .... ... ... .... ... .... .. 330 7.30 Beam spreading. ... .... ... .... ... ... .... ... .... .. 331 7.31 The geometry of a collimator . . . .... ... ... .... ... .... .. 332 7.32 Evaluating the point spread function of a collimator. .... ... .... .. 333 7.33 The graph of p(rs; z); for a \fxed z: .... ... ... .... ... .... .. 334 8.1 A 3-dimensional X-ray beam. ... .... ... ... .... ... .... .. 339 8.2 The reconstruction grid. ... ... .... ... ... .... ... .... .. 340 8.3 A parallel beam scanner and sample set. . ... ... .... ... .... .. 343 8.5 Parameters for a fan beam machine. . . . . . . . . . .... ... .... .. 344 8.4 The two di\u000berent divergent beam geometries. . ... .... ... .... .. 345 8.6 An example of a sinogram. . ... .... ... ... .... ... .... .. 346 8.7 The impulse response for a RamLak \flter (solid) and a continuous approxi- mation (dotted). . . . .... ... .... ... ... .... ... .... .. 352 8.8 Ram-Lak \flters applied to Rf1: .. .... ... ... .... ... .... .. 354 8.9 Ram-Lak \flters applied to Rf1: .. .... ... ... .... ... .... .. 354 8.10 How to choose sample spacings. .. .... ... ... .... ... .... .. 357 8.11 Fan beam geometry. . .... ... .... ... ... .... ... .... .. 359 8.12 Quantities used in the fan beam, \fltered backprojection algorithm. . . . . . 360 8.13 Collecting data for fan beam scanners. . . . . . . . . .... ... .... .. 365 8.14 Absorbing square. .. .... ... .... ... ... .... ... .... .. 370 8.15 Rectangle with small inclusion . . .... ... ... .... ... .... .. 371 8.16 Relative errors with small inclusion .... ... ... .... ... .... .. 371 8.17 A mathematical phantom... ... .... ... ... .... ... .... .. 374 8.18 Examples of PSF and MTF with band limited regularization. . .... .. 376 8.19 Limits for the PSF and MTF in the \fltered backprojection algorithm. . . . 377 8.20 Examples of PSF and MTF with exponential regularization. . . .... .. 378 8.21 Examples of PSF and MTF with Shepp-Logan regularization. . .... .. 379 8.22 Resolution phantoms are used to gauge the resolution of a CT-machine or reconstruction algorithm. . . ... .... ... ... .... ... .... .. 380 8.23 Reconstructions of a mathematical phantom using \fltered backprojection algorithms. ... ... .... ... .... ... ... .... ... .... .. 381 8.24 The e\u000bect of ray sampling on the PSF. .. ... ... .... ... .... .. 384 8.25 Filtered backprojection reconstruction of elliptical phantom . . . .... .. 385 8.26 Parameters describing the Radon transform of ˜E: . .... ... .... .. 386 8.27 Filtered backprojection reconstruction of square phantom . . . . .... .. 388 8.28 View sampling artifacts with \u0001\u0012 = 2ˇ xx LIST OF FIGURES 8.36 Due to beam hardening, dense objects produce dark streaks. . . .... .. 401 8.37 Mathematical analysis has led to enormous improvements in CT-images. . . 405 9.1 Pixel basis . ... ... .... ... .... ... ... .... ... .... .. 408 9.2 Method of projections .... ... .... ... ... .... ... .... .. 412 9.3 Examples where the projection algorithm does not converge. . . .... .. 413 9.4 One step in the Kaczmarz algorithm. ... ... ... .... ... .... .. 414 9.5 Reconstructions using ART . ... .... ... ... .... ... .... .. 415 9.6 Ranges relaxation parameters. . . . .... ... ... .... ... .... .. 419 10.1 Comparisons of Bernoulli and Gaussian distribution functions with p = :1. . 463 10.2 Comparisons of Bernoulli and Gaussian distribution functions with p = :5. . 463 10.3 Comparisons of Poisson and Gaussian distribution functions. .. .... .. 465 11.1 An RL-circuit. . ... .... ... .... ... ... .... ... .... .. 497 12.1 Comparison of the image variance using di\u000berent models for the variance in the measurements. .. .... ... .... ... ... .... ... .... .. 511 A.1 Multiplication of complex numbers .... ... ... .... ... .... .. 552 A.2 Some J-Bessel functions. ... ... .... ... ... .... ... .... .. 568 A.3 Polynomial interpolants for jx − 1 Chapter 1 Measurements and modeling A quantitative model of a physical system is expressed in the language of mathematics. A qualitative model often precedes a quantitative model. For many years clinicians used medical X-rays without employing a precise quantitative model. X-rays were thought of as high frequency `light' with three very useful properties: (1). If X-rays are incident on a human body, some fraction of the incident radiation is absorbed, though a sizable fraction is transmitted. The fraction absorbed is propor- tional to the total `density' of the material encountered. (2). A `beam' of X-ray light travels in a straight line. (3). X-rays darken photographic \flm. Taken together, these properties mean that using X-rays one could \\see through\" a human body to obtain a shadow or projection of the internal anatomy on a sheet of \flm. \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u0001 X−ray source Film plane object (a) Depth information is lost in a projection.? (b) A old fashioned X-ray. Figure 1.1: The world of old fashioned X-rays. 1 2 CHAPTER 1. MEASUREMENTS AND MODELING The model was adequate given the available technology. In their time, X-rays led to a revolution in the practice of medicine because they opened the door to non-destructive examination of internal anatomy. They are still useful for locating bone fractures, den- tal caries and foreign objects but their ability to visualize soft tissues and more detailed anatomic structure is very limited. There are several reasons for this. The X-ray image is a two dimensional projection of a three dimensional object which renders it impossible to deduce the spatial ordering in the missing third dimension. Photographic \flm is not very sensitive to X-rays. To get a usable image, a light emitting phosphor is sandwiched with the \flm. This increases the sensitivity of the overall `detector,' but even so, large changes in the intensity of the incident X-rays still produce small di\u000berences in the density of \flm. This means that the contrast between di\u000berent soft tissues is poor. Because of these limitations a qualitative theory was adequate for the interpretation of X-ray images. A desire to improve upon this situation led Alan Cormack and Godrey Houns\feld, to independently develop X-ray tomography or slice imaging, see [30] and [10]. The \frst step in their work was to use a quantitative theory for the absorption of X-rays. Such a theory already existed and is little more than a quantitative restatement of (1) and (2). It was not needed for old fashioned X-rays because they are read \\by eye,\" no further processing is done after the \flm is developed, Both Cormack and Houns\feld realized that mathematics could be used to infer 3-dimensional anatomic structure from a large collection of di\u000berent two dimensional projections. The possibility for making this idea work relied on two technological advances: 1. The availability of scintillation crystals to use as detectors. 2. Powerful, digital computers to process the tens of thousands of measurements needed to form a usable image. A detector using a scintillation crystal is about a hundred times more sensitive than \flm which makes possible much \fner distinctions. As millions of arithmetic operations are needed for each image, fast computers are a necessity for reconstructing an image from the available measurements. It is an interesting historical note that the mathematics underlying X-ray tomography was done in 1917 by Johan Radon, see [59]. It had been largely forgotten and both Houns\feld and Cormack worked out solutions to the problem of reconstructing an image from its projections. Indeed, this problem had arisen and been solved in contexts as diverse as radio astronomy and statistics. This book is a detailed exploration of the mathematics which underpins the reconstruc- tion of images in X-ray tomography. The list of mathematical topics covered is dictated by their importance and utility in medical imaging. While our emphasis is on understanding these mathematical foundations, we constantly return to the practicalities of X-ray tomog- raphy and explore the relationship of the mathematical formulation of a problem and its solution, to the realities of computation and physical measurement. There are many di\u000ber- ent imaging modalities in common use today, X-ray computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), ultrasound, optical imag- ing, impedence imaging, etc. Because each relies on a di\u000berent physical principle, each provides di\u000berent information. In every case the mathematics needed to process and inter- pret the data has a large overlap with that used in X-ray CT. We concentrate on X-ray CT because of the simplicity and clarity of the physical principles underlying the measurement process. Detailed descriptions of the other modalities can be found in [39] or [4]. 1.1. MATHEMATICAL MODELING 3 1.1 Mathematical modeling Mathematics is the language in which any quantitative theory or model is eventually ex- pressed. In this introductory chapter we consider a variety of examples of physical systems, measurement processes and the mathematical models used to describe them. These mod- els illustrate di\u000berent aspects of more complicated models used in medical imaging. The chapter concludes with a consideration of linear models. Mathematics is used to model physical systems from the formation of the universe to the structure of the atomic nucleus, from the function of the kidney to the opinions of voters. The \frst step in giving a mathematical description of a \\system\" is to isolate that system from the universe in which it sits. While it is no doubt true that a butterﬂy ﬂapping its wings in Siberia in mid-summer will e\u000bect the amount of rainfall in the Amazon rain forest a decade hence, it is surely a tiny e\u000bect, impossible to accurately quantify. To obtain a practical model such e\u000bects are ignored, though they may come back to haunt the model, as measurement error and noise. After delineating a system, we need to \fnd a collection of numerical parameters which describe its state. In this generality these parameters are called state variables. In the idealized world of an isolated system the exact measurement of the state parameters would uniquely determine the state of the system. In general the natural state parameters are not directly measurable. The model then describes relations between the state variables which suggest feasible measurements with which one might determine the state of the system. 1.1.1 Finitely many degrees of freedom 4 CHAPTER 1. MEASUREMENTS AND MODELING the point (0; 0); then the state variables satisfy the relation x2 + y2 = r2: (1.2) Imagine now that one dimensional creatures, living on the x-axis fy =0g can observe a shadow of the ball, cast by very distant light sources so that the rays of light are perpen- dicular to the x-axis. The line creatures want to predict whether or not the ball is about to collide with their world. Locating the shadow determines the x-coordinate of the ball, using equation (1.2) gives y = \u0006p 1.1. MATHEMATICAL MODELING 5 The notation O(\u0001\u00122) refers to an error term which is bounded by a constant times \u0001\u00122; as \u0001\u0012 goes to zero. The height predicted from the measurement of the angle is hm = l tan(\u0012 +\u0001\u0012)= l(tan \u0012 + \u0001\u0012 6 CHAPTER 1. MEASUREMENTS AND MODELING \u000f If the shape of a mountain looks like that in \fgure 1.4and we measure the distance and angle at the point P , we are certainly not \fnding the real height of the mountain. Some apriori information is always incorporated in a mathematical model. P Figure 1.4: Not exactly what we predicted! \u000f The curvature of the earth is ignored. A more sophisticated geometric model is needed to correct for such errors. This becomes a signi\fcant problem as soon as the distances, l; l1;l2 are large compared to the distance to the horizon (about 25km for a 2 meter tall person). The approximations used in the model must be adapted to the actual physical conditions of the measurements. \u000f The geometry of the underlying measurements could be very di\u000berent from the simple Euclidean geometry used in the model. To measure the angles \u00121;\u00122 one would normally use a transit to sight the peak of the mountain. If the mountain is far away then the light travels on a path from the mountain to the transit which passes through air of varying density. The light is refracted by the air and therefore the ray path is not the straight line assumed in the model. To include this e\u000bect would vastly complicate the model. This is an important consideration in the very similar problem of creating a map of the sky from earth based observations of stars. Analogous problems arise in medical imaging. If the wavelength of the energy used to probe the human anatomy is very small compared to the size of the structures that are present then it is reasonable to assume that the waves are not refracted by the medium through which they pass, i.e. X-rays can be assumed to travel along straight lines. However for energies with wavelengths comparable to the size of structures present in the human anatomy, this assumption is simply wrong. The waves are then bent and di\u000bracted by the medium and the di\u000eculty of modeling the ray paths is considerable and, in fact largely unsolved! This is an important issue in ultrasound imaging. Example 1.1.4. Refraction provides another example of a simple physical system. Suppose that we have two ﬂuids in a tank as shown in the \fgure and would like to determine the height of the interface between them. Suppose \frst of all that the refractive indices of the ﬂuids are known. Let n1 be the refractive index of the upper ﬂuid and n2 the refractive index of the lower one, Snell's law states that sin(\u00121) 1.1. MATHEMATICAL MODELING 7 h1 h2 n1 n2 θ1 θ2 l Figure 1.5: Using refraction to determine the height of an interface. Let h denote the total height of the ﬂuid, then h1 + h2 = h: The measurement we make is the total displacement l; of the light ray as it passes through the ﬂuids. It satis\fes the relationship h1 tan(\u00121)+ h2 tan(\u00122)= l: Using these three relations h1 and h2 are easily determined. The assumption that we know n1 implies, by Snell's law that we can determine \u00121 from a measurement of the angle of the light ray above the ﬂuid. If n2 is also known, then using these observations we can determine \u00122 as well: sin(\u00122)= n1 8 CHAPTER 1. MEASUREMENTS AND MODELING these examples, the problem of determining the state of the system from feasible measure- ments reduces to solving systems of \fnitely many equations in \fnitely many unknowns. In imaging applications the state of a system is usually described by a function or functions of continuous variables. These are systems with in\fnitely many degrees of freedom. In this section we consider several examples of this type. Example 1.1.5. Suppose that we would like to determine the shape of a planar object, D that cannot be seen. The object is lying inside a disk and we can \fre particles at the object which bounce o\u000b. Assume that this scattering process is very simple: each particle strikes the object once and is then scattered along a straight line o\u000b to in\fnity. The outline of the object can be determined by knowing the correspondence between incoming lines, lin and outgoing lines, lout: Each intersection point lin \\ lout lies on the boundary of the object. Measuring flj outg for \fnitely many incoming directions flj ing determines \fnitely many points flj in \\ lj outg on the boundary of D: In order to use this \fnite collection of points to make any assertions about the rest of the boundary of D; more information is required. If we know that D consists of a single piece or component then these points would lie on a single closed curve, though it might be di\u000ecult to decide in what order they should appear on the curve. Convex obstacles is a class which satisfy these simple assumptions and for which a \fnite number of points on the boundary might carry a lot of useful information. A region D in the plane is convex if it has the following property: for each pair of points p and q lying in D the line segment 1.1. MATHEMATICAL MODELING 9 its tangent lines. By intersecting the half planes de\fned by the tangent lines through the points f(li in;li out)g we obtain another convex polygon, P out N which contains D: Thus with these N -measurements we obtain the both an inner and outer approximation to D : P in N ˆ D ˆ P out N : It is clear that the boundary of a convex region has in\fnitely many degrees of freedom as it is conveniently described as the image of a map s 7! (x(s);y(s)); where s lies in an interval [0; 1]: On the other hand the images of such maps can be approximated by polygons. Once the number of sides is \fxed, then we are again considering a system with \fnitely many degrees of freedom. In all practical problems, a system with in\fnitely many degrees of freedom must eventually be approximated by a system with \fnitely many degrees of freedom. θ θ (a) The angle of incidence equals the angle of reﬂec- tion. –2 –1 0 1 2 –2 –1 1 2 (b) The outer approximation as an intersection of half spaces. Figure 1.7: Using particle scattering to determine the boundary of a convex region. Remark 1.1.1. For a non-convex body the above method does not work as the correspon- dence between incoming and outgoing lines can be quite complicated: some incoming lines may undergo multiple reﬂections before escaping, in fact some lines might become perma- nently trapped. Example 1.1.6. Suppose that the surface of a sea is mapped by coordinates (x; y) belonging to a region D ˆR2 : The depth of the bottom of the sea is described by a function h(x; y): One way to determine h would be to drop a weighted string until it hits the bottom. There are problems with this method: 1. It is di\u000ecult to tell when the weight hits the bottom. 2. Unknown, underwater currents may carry the string so that it does not go straight down. A somewhat less direct approach would be to use sonar to measure the distance to the bottom. The physical principle underlying the measurement is that the speed of sound is determined by the density and temperature of the water which are in turn determined by the depth. Let c(z)denote the known speed of sound, as a function of the depth. A speaker underneath the boat emits a loud, short pulse of sound and the time it takes for the sound to return is measured. Here we assume that the sound travels in a straight line 10 CHAPTER 1. MEASUREMENTS AND MODELING outgoing pulse measured echo Figure 1.8: Using sound to measure depth. to the bottom and the microphone only detects the direct reﬂection, traveling back along the straight line. Using c(z) the transit time can be related to the depth. A simple model, valid for shallow seas, is that the speed of sound is a constant, c: The measurement, T is the time it takes for the sound pulse to go down and back, 2h = cT: This assumes that the boat is stationary from the time the pulse is emitted until the return is received. With such a measurement, T (x; y) for each position (x; y) 2 D; the depth is determined, everywhere by h(x; y)= cT (x; y) 1.1. MATHEMATICAL MODELING 11 Example 1.1.7. Now assume that the sea, in the previous example is one dimensional, but that the sound speed is not constant. To use the measurements described above to determine the depth h(x) requires more mathematical apparatus. Let z(t) denote the depth of the sound pulse at a time t after it is emitted. Using calculus we can express the assertion that the `speed of sound at depth z is c(z)' as a di\u000berential equation dz 12 CHAPTER 1. MEASUREMENTS AND MODELING Example 1.1.8. The one dimensional model in the previous example can be used to solve the two dimensional problem. Suppose that the area we are interested in mapping corresponds to the rectangle [−1; 1]\u0002[−1; 1] in the (x; y)-map coordinates. For each y de\fne the function of one variable hy(x) d = h(x; y): Knowing the collection of functions fhy(x): y 2 [−1; 1]g for x 2 [−1; 1] is evidently exactly the same thing as a knowing h(x; y); for (x; y) 2 [−1; 1] \u0002 [−1; 1]: Because the measuring apparatus only observes the sound returning on the straight line from the boat to the bottom of the sea, the analysis in the previous example applies to allow the determination of hy(x) from measurements of Ty(x); h(x; y)= hy(x)= G −1 \u0012 Ty(x) 1.2. A SIMPLE MODEL PROBLEM FOR IMAGE RECONSTRUCTION 13 Exercise 1.1.8. In the examples above it is assumed that all returns not arriving on the straight line path from the bottom of the ocean are ignored. Analyze the problems that result if return signals are accepted from all directions. What impact would this have on using the slicing method to reduce the dimensionality of the problem? Exercise 1.1.9. Repeat the analysis in example 1.1.7 assuming that the boat is traveling at constant velocity v: Continue assuming that only returns meeting the bottom of the boat at right angles are detected. 1.2 A simple model problem for image reconstruction The problem of image reconstruction in X-ray tomography is sometimes described as re- constructing an object from its \\projections.\" Of course these are projections under the illumination of X-ray \\light.\" In this section we consider the analogous, but simpler prob- lem, of determining the outline of an object from its shadows. As is also the case in medical applications, we consider a two dimensional problem. Let D be the convex region in the plane. Imagine that a light source is placed very far away from the body. Since the light source is very far away, the rays of light are all traveling in essentially the same direction. We can think of the rays of light as a collection of parallel lines. We want to measure the shadow that D casts for each position of the light source. To describe the measurements imagine that a screen is placed on the \\other side\" of D perpendicular to the direction of the light rays, see the \fgure below. The screen is the detector, in a real apparatus sensors would be placed on the screen, allowing us to determine where the shadow begins and ends. Shadow D Figure 1.9: The shadow of a convex region The region, D blocks a certain collection of light rays and allows the rest to pass. Measuring the shadow is therefore determining the \\\frst\" and \\last\" lines in this family of parallel lines to intersect D: To completely describe the object we need to rotate the source and detector through 180\u000e; measuring, at each angle, where the shadow begins and ends. The \frst and last lines to intersect a region just meet it along its boundary. These lines are therefore tangent to the boundary of D: The problem of reconstructing a region from its shadows is mathematically the same as the problem of reconstructing a region from a knowledge of the tangent lines to its boundary. As a \frst step in this direction we need a 14 CHAPTER 1. MEASUREMENTS AND MODELING good way to organize our measurements. To that end we give a description for the space of all lines in the plane. 1.2.1 The space of lines in the plane A line in the plane is the set of points which satisfy an equation of the form ax + by = c where a2 + b2 6=0: We get the same set of points if we replace this equation by a 1.2. A SIMPLE MODEL PROBLEM FOR IMAGE RECONSTRUCTION 15 unit circle t θ l ω ^ θ+π/2 ω Figure 1.10: Parameterization of oriented lines in the plane. The vector ! is the direction perpendicular to the line and the number t is called the a\u000ene parameter of the line, jtj is the distance from the line to the origin of the coordinate system. The pair (t; !) de\fnes two half planes H + t;! = f(x; y) j (x; y) \u0001 !> tg and H − t;! = f(x; y) j (x; y) \u0001 !< tg; the line lt;! is the common boundary of these half planes. Facing along the line lt;! in the direction speci\fed by ^!; the half plane H − t;! lies to the left. To summarize, the pairs (t; !) 2R1 \u0002S1 parametrize the oriented lines in the plane, which we sometimes call the space of oriented lines. Exercise 1.2.1. Show that jtj =minf p 16 CHAPTER 1. MEASUREMENTS AND MODELING 0 Shadow D ω t 1 t Figure 1.11: The measurement of the shadow see \fgure 1.11. Examining the diagram it is clear that the orientation of the boundary at the point of tangency and that of the line agree, for t1 and are opposite for t0: For !(\u0012) = (cos(\u0012); sin(\u0012)) de\fne hD(\u0012)= t1: We call hD(\u0012)the shadow function for D: The mathematical formulation of reconstruction problem is: Can the boundary of the region D be determined from its shadow function? The line lhD(\u0012);!(\u0012) is given parametrically by fhD(\u0012)(cos(\u0012); sin(\u0012)) + s(− sin(\u0012); cos(\u0012)) j s 2 (−1; 1)g: To determine the boundary of D it would su\u000ece to determine the point of tangency of lhD(\u0012);!(\u0012) with the boundary of D; in other words we would like to \fnd the function s(\u0012) so that for each \u0012; (x(\u0012);y(\u0012)) = hD(\u0012)(cos(\u0012); sin(\u0012)) + s(\u0012)(− sin(\u0012); cos(\u0012)) (1.8) is a point on the boundary of D: The function s(\u0012) is found by recalling that, at the point of tangency, the direction of the tangent line to D is ^!(\u0012): For a curve in the plane given parametrically by (x(\u0012);y(\u0012)) the direction of the tangent line at a point \u00120 is the same as that of the vector (x0(\u00120);y0(\u00120)): Di\u000berentiating the expression given in (1.8) and using the fact that @\u0012! =^! we \fnd that (x 0(\u0012);y0(\u0012)) = (h 0 D(\u0012) − s(\u0012))!(\u0012)+ (hD(\u0012)+ s0(\u0012))^!(\u0012): (1.9) Since the tangent line at (x(\u0012);y(\u0012)) is parallel to ^!(\u0012) it follows from (1.9) that h 0 D(\u0012) − s(\u0012)= 0: (1.10) This gives a parametric representation for the boundary of a convex region in terms of its shadow function: If the shadow function is hD(\u0012) then the boundary of D is given parametrically by (x(\u0012);y(\u0012)) = hD(\u0012)!(\u0012)+ h 0 D(\u0012)^!(\u0012): (1.11) 1.2. A SIMPLE MODEL PROBLEM FOR IMAGE RECONSTRUCTION 17 Note that we have assumed that hD(\u0012) is a di\u000berentiable function. This is not always true, for example if the region D is a polygon then the shadow function is not everywhere di\u000berentiable. Let D denote a convex region and hD its shadow function. We can think of D ! hD as a mapping from convex regions in the plane to 2ˇ periodic functions. It is reasonable to enquire if every 2ˇ periodic function is the shadow function of a convex region. The answer to this question is no. For strictly convex regions with smooth boundaries we are able to characterize the range of this mapping. If h is twice di\u000berentiable then the tangent vector to the curve de\fned by (x(\u0012);y(\u0012)) = h(\u0012)!(\u0012)+ h 0(\u0012)^!(\u0012) (1.12) is given by (x0(\u0012);y0(\u0012)) = (h 00(\u0012)+ h(\u0012))^!(\u0012): In our construction of the shadow function we observed that the tangent vector to the curve at (x(\u0012);y(\u0012)) and ^!(\u0012) point in the same direction. From our formula for the tangent vector we see that this implies that h 00(\u0012)+ h(\u0012) > 0 for all \u0012 2 [0; 2ˇ]: (1.13) This gives a necessary condition for a twice di\u000berentiable function h to be the shadow function for a strictly convex region with a smooth boundary. Mathematically we are determining the of the map that takes a convex body D ˆR2 to its shadow function hD; under the assumption that hD is twice di\u000berentiable. This is a convenient mathematical assumption, though in an applied context it is likely to be overly restrictive. Exercise 1.2.5. Suppose that Dn is a regular n-gon. Determine the shadow function hDh(\u0012): Exercise 1.2.6. Suppose that h(\u0012)is 2ˇ-periodic, twice di\u000berentiable function which sat- is\fes (1.13). Show that the curve given by (1.12) is the boundary of a strictly convex region. Exercise 1.2.7. \u0003 Find a characterizations of those functions which are shadow functions of convex regions without assuming that they are twice di\u000berentiable or that the region is strictly convex. Exercise 1.2.8. If h(\u0012) is any di\u000berentiable function then equation (1.12) de\fnes a curve, by plotting examples, determine what happens if the condition (1.13) is not satis\fed. Exercise 1.2.9. Suppose that hD is a function satisfying (1.13). Show that the area enclosed by Γh is given by the Area(Dh)= 1 18 CHAPTER 1. MEASUREMENTS AND MODELING Exercise 1.2.10. Let h be a smooth 2ˇ-periodic function which satis\fes (1.13). Prove that the curvature of the boundary of the region with this shadow function, at the point h(\u0012)!(\u0012)+ h0(\u0012)^!(\u0012)is given by \u0014(\u0012)= 1 1.2. A SIMPLE MODEL PROBLEM FOR IMAGE RECONSTRUCTION 19 be used to construct an approximation to the region D; which cast these shadows? We consider two di\u000berent strategies both of which rely on the special geometric properties of convex regions. Recall that a convex region always lies in one of the half planes determined by the support line at any point of its boundary. The half plane lying \\below\" the oriented line lt;! is the set de\fned by H − t;! = f(x; y) j (x; y) \u0001 !< tg: Since the boundary of D and lh(\u0012);!(\u0012) have the same orientation at the point of contact, it follows that D lies in each of the half planes H − h(\u0012j);!(\u0012j );j =1;::: ;m: As D lies in each of these half planes it also lies in their intersection. This de\fnes a polygon Pm = m\\ j=1 Hh(\u0012j);!(\u0012j ): Pm is a convex polygon which contains D; this then provides one sort of approximation for D from the measurement of a \fnite set of shadows. This is a stable approximation to D as small changes in the measurements of either the angles \u0012j or the corresponding a\u000ene parameters h(\u0012j) lead to small changes in the approximating polygon. The di\u000eculty with using the exact reconstruction formula (1.11) is that h is only known at \fnitely many values, f\u0012jg: From this information it is not possible to exactly compute the derivatives, h0(\u0012j): We could use a \fnite di\u000berence approximation for the derivative to determine a \fnite set of points which approximate points on the boundary of D : (x(\u0012j);y(\u0012j)) = h(\u0012j)!(\u0012j)+ h(\u0012j) − h(\u0012j+1) 20 CHAPTER 1. MEASUREMENTS AND MODELING measurements. One assumes that the errors in individual measurements have \\mean zero:\" if the same measurement is repeated many times then the average of the individual measure- ments should approach the true value. This is the approach taken in magnetic resonance imaging. Another possibility is to make a large number of measurements at closely spaced angles f(hj ;j\u0001\u0012): j =1;::: ;N g which are then \\averaged\" to give less noisy approxi- mations on a coarser grid. There are many ways to do the averaging. One way is to \fnd a di\u000berentiable function, H belonging to a family of functions of dimension M< N and minimizes the square error e(H)= NX j=1(hj − H(j\u0001\u0012)) 2: For example H could be taken to be a polynomial of degree M − 1; or a continuously di\u000berentiable, piecewise cubic function. Using values of H one can \fnd an approximation to the boundary of D which is hopefully less corrupted by noise. Fine structure in the boundary is also blurred by such a procedure. This is closer to the approach used in X-ray CT. Exercise 1.2.16. Suppose that the angles f\u0012jg can be measured exactly but there is an uncertainty of size \u000f in the measurement of the a\u000ene parameters, h(\u0012j): Find a polygon Pm;\u000f which gives the best possible approximation to D which certainly contains D: Exercise 1.2.17. Suppose that we know that jh00(\u0012)j <M; and the measurement errors are bounded by \u000f> 0: For what angle spacing is the error in using a \fnite di\u000berence approximation for h0 due to the uncertainty in the measurements equal to that caused by the non-linearity of h itself. 1.2.4 Can an object be reconstructed from its width? To measure the location of the shadow requires an expensive detector which can accurately locate a transition from light to dark. It would be much cheaper to build a device, similar to the exposure meter in a camera, to measure the length of the shadow region without determining its precise location. It is therefore an interesting question whether or not the boundary of a region can be reconstructed from measurements of the widths of its shadows. Let wD(\u0012) denote the width of the shadow in direction \u0012; a moments consideration shows that wD(\u0012)= hD(\u0012)+ hD(\u0012 + ˇ): From this formula it follows that wD does not determine D: Let e(\u0012) 6\u0011 0 be a function that satis\fes e(\u0012)+ e(\u0012 + ˇ)= 0: (1.15) From the discussion in section 1.2.2 we know that if hD has two derivatives such that h00 D + hD > 0then hD(\u0012) is the shadow function of a strictly convex region. Let e be a smooth function satisfying (1.15) such that h 00 D + hD + e 00 + e> 0 1.3. LINEARITY 21 as well, then hD + e is also the shadow function for a di\u000berent strictly convex region. Observe that hD(\u0012)+ e(\u0012) is the shadow function for a di\u000berent region, D0 which has the same width of shadow for each direction as D: That is wD(\u0012)= (hD(\u0012)+ e(\u0012)) + (hD(\u0012 + ˇ)+ e(\u0012 + ˇ)) = wD0(\u0012): To complete this discussion note that any function with a Fourier representation of the form e(\u0012)= 1X j=0[aj sin(2j +1)\u0012 + bj cos(2j +1)\u0012] satis\fes (1.15). This is an in\fnite dimensional space of functions. This implies that if wD(\u0012) is the \\width of the shadow\" function for a convex region D then there is an in\fnite dimensional set of regions with the same \\width of the shadow\" function. Consequently the simpler measurement is inadequate to reconstruct the boundary of a convex region. The \fgure below shows the unit disk and another region which has constant \\shadow width\" equal to 2. Figure 1.12: Two regions of constant width 2 Exercise 1.2.18. Show that the width function satis\fes w00 D + wD > 0: Exercise 1.2.19. Is it true that every twice di\u000berentiable, ˇ-periodic function, w satisfying w00 + w> 0 is the width function of a convex domain? Exercise 1.2.20. Our motivation for considering whether or not a convex body is de- termined by the width of its shadows was to replace our expensive detector, which can determine where a shadow begins and ends, with a less expensive detector. The cheaper detector can only measure the width of the covered region. Can you \fnd a way to use a detector which only measures the length of an illuminated region to locate the edge of the shadow? Hint: Only cover half of the detector with photosensitive material. 1.3 Linearity 22 CHAPTER 1. MEASUREMENTS AND MODELING As we have seen, using measurements to determine other quantities requires the solution of systems of equations. Sometimes we need to solve di\u000berential equations and sometimes algebraic equations. In almost all practical applications one is eventually reduced to solving systems of linear equations. This is true even for physical systems which are described by non-linear equations. Non-linear equations are usually solved iteratively where the iteration step involves the solution of linear equations. There are many reasons why linear equations and linear models are ubiquitous. From the pragmatic point of view, there is a \\complete\" mathematical theory for systems of linear equations. One has necessary and su\u000ecient conditions for linear equations to have solutions, a description of the space of solutions when they exist and practical algorithms for \fnding them. This is not true even for the simplest families on non-linear equations. On a more conceptual level, for systems with some sort of intrinsic smoothness, a linear model often su\u000eces to describe small deviations from an equilibrium state. We give a quick overview of the theory of systems of linear, algebraic equations. This is not intended to serve as a text on linear algebra, merely a review of some important concepts. Detailed discussions of various aspect of this material can be found in [43] or [78]. Consider a system of m equations for n unknowns, (x1;::: ;xn): a11x1 + a12x2+ \u0001\u0001\u0001 + a1nxn = y1 a21x1 + a22x2+ \u0001\u0001\u0001 + a2nxn = y2 ... ... ... am1x1 + am2x2+ \u0001\u0001\u0001 + amnxn = ym: (1.16) There are four questions which require answers: Existence: For a given m-vector (y1;::: ;ym)doesthere existan n-vector (x1;::: ;xn)which satis\fes the equations in (1.16)? Uniqueness: When a solution exists is it unique? More generally, describe the space of solutions. Solve in practice: Give an algorithm to \fnd approximations for the solutions of (1.16) and criteria to select a solution when there is more than one. Stability: How sensitive is the solution to small variations in the coe\u000ecients (aij)orthe right hand side (yj)? It is a somewhat unexpected, but very important fact that these issues are in practice, rather independent of one another. Before proceeding with our analysis we \frst need to simplify the notation. It is very cumbersome to have to work with complicated expressions like (1.16), instead we use stan- dard matrix and vector notation. Let a denote the m\u0002n array of numbers (aij)i=1:::m;j=1:::n and x denote the column n-vector. The result of multiplying x by a is a column m-vector whose ith entry is (ax)i = ai1x1 + ai2x2 + \u0001\u0001\u0001 + ainxn: 1.3. LINEARITY 23 The system of equations (1.16) is concisely expressed as ax = y; Here y is a column m-vector with entries (y1;::: ;ym): We brieﬂy recall the properties of matrix multiplication, let x1 and x2 be n-vectors, then a(x 1 + x 2)= ax 1 + ax 2 and for any number c a(cx 1)= c(ax 1): These are the properties that characterize linearity. 1.3.1 Solving linear equations Suppose that x0 is a solution of the equation ax =0 and x1 is a solution of the equation ax1 = y then the rules above show that for any number c we have a(cx 0 + x 1)= cax 0 + ax 1 = ax 1 = y: If y = 0 as well then we conclude that the set of solutions to the equation ax =0 is a linear space, that is if x0 and x1 solve this equation then so does x0 + x1 as well as cx0; for any number c: This space is called the null space or kernel of a: It is denoted by ker(a) and always contains, at least the zero vector 0 =(0;::: ; 0): These observations answer the question above about uniqueness. Theorem 1.3.1. Let a be an m \u0002 n matrix. Given a vector y; if x1 satis\fes ax1 = y then every other solution to this equation is of the form x1 + x0 where x0 2 ker(a): Moreover, every vector of this form solves the equation ax = y: As a simple corollary it follows that the solution of the equation ax = y is unique only if the null space of a contains only the 0-vector. In order to answer the question of existence it is convenient to introduce the notion of a \\dot\" or inner product. If x and y are two n-vectors then de\fne hx; yi = nX j=1 xjyj = x \u0001 y: Suppose that a is an m \u0002 n-matrix, x is an n-vector and y is an m-vector then ax is an m-vector and hax; yi = mX i=1 nX j=1 yiaijxj: The transpose of the matrix a is the n \u0002 m matrix at whose ij-entry is aji: From the previous formula it follows that that hax; yi = hx; atyi: 24 CHAPTER 1. MEASUREMENTS AND MODELING Suppose that y is a non-zero vector in the null space of at (note that here we are using the transpose!) and the equation ax = b has a solution. Using the calculations above we see that hb; yi = hax; yi = hx; atyi =0: The last equality follows from the fact that aty =0: This gives a necessary condition for existence of a solution to the equation ax = b; the vector b must satisfy hb; yi =0 for every solution of the homogeneous equation aty =0: This also turns out to be su\u000ecient. Theorem 1.3.2. Let a be an m \u0002 n-matrix and b and m-vector. The equation ax = b has a solution if and only if hb; yi =0 for every vector y satisfying the homogeneous equation aty =0: Putting these two results together we obtain that Corollary 1.3.1. Let a be an m \u0002 n-matrix the equation ax = b has a unique solution for any vector b if and only if ker(a)= f0g and ker(at)= f0g: In a physical situation the vector x describes the state of a system and the entries of the vector b are results of measurements made on the system while it is in this state. The matrix a is a model for the measurement process: it is the assertion that if the physical object is described by the parameters, x then the results of the experiments performed should be the vector b = ax: This is a linear model because the map from the state of the system to the measurements is a linear map. The problem of determining the state of the system from the measurements is precisely the problem of solving this system of linear equations. Example 1.3.1. Suppose we have a collection of photons sources, labeled by 1 \u0014 i \u0014 n and an array of detectors, labeled by1 \u0014 j \u0014 m: The matrix P has entries 0 \u0014 pij \u0014 1: The ij-entry is the probability that a particle emitted from source i is detected by detector j: Since a given photon can be detected by at most one detector it follows that mX j=1 pij \u0014 1for i =1;::: n: If dj;j =1;::: ;m is the number of photons detected at detector j and si;i =1;::: ;n is the number of photons emitted by source i then our model predicts that Ps = d: If m = n and P is an invertible matrix then we can use the measurements d to obtain a unique vector s: Since the model is probabilistic this should be regarded as an expected value for the distribution of sources. If m>n then we have more measurements than unknowns, so any measurement errors or ﬂaws in the model could make it impossible to \fnd a vector s so that Ps = d: This is a frequent situation in image reconstruction problems. One 1.3. LINEARITY 25 chooses a way to measure the error, usually a function of the form e(Ps − d) and seeks a vector s which minimizes the error. Finally we may have more sources than detectors. The measurements are then inadequate, in principle to determine their distribution. This is also a common circumstance in image reconstruction problems and is resolved by making some a priori assumptions about the allowable distribution of sources to obtain a determined (or even overdetermined) problem. As illustrated by this example and explained in the theorem there are essentially 3 types of linear models for systems with \fnitely many degrees of freedom. Determined: The simplest case arises when the number of independent measurements and pa- rameters describing the state of the system are the same. This implies that n = m: In this case the measurements uniquely determine the state of the system. Mathe- matically we say that the matrix, a is invertible. In the situation that n = m this is equivalent to the statement that the homogeneous equation, ax = 0 has only the trivial solution, x =0: The inverse matrix is denoted by a−1; it is both a left and a right inverse to a; a−1a =Idn = aa−1: Here Idn denotes the n \u0002 n identity matrix, that in (Idn)ij = ( 1if i = j; 0if i 6= j: From the mathematical point of view, the unique solution is obtained by setting x = a−1y: Except in special cases, the inverse matrix a−1 is not computed directly. Overdetermined: In this case we have more measurements than parameters, i.e. m> n.If the model and measurements are perfect then there should be a unique x with ax = y: In general, neither is true and there will not exist any x exactly satisfying this equation. Having more measurements than parameters can be used to advantage in several di\u000berent ways. In example 1.3.2 we explain how to use the conditions for solvability given in Theorem 1.3.2 to determine physical parameters. Often times measurements are noisy. A model for the noise in the measurements can be used to select a criterion for a \\best approximate solution.\" The error function is usually de\fned by picking a norm k\u0001k on the space of measurements. We then try to \fnd the vector x which minimizes the error, e(x)= kax − yk: The most commonly used error function is that de\fned by the square norm kyk 2 2 = mX j=1 y2 j : There are two reasons why this measure of the error is often employed: 1. It is a natural choice if the noise is normally distributed, 2. The problem of minimizing kax − yk2 can be reduced to the problem of solving a system of linear equations. 26 CHAPTER 1. MEASUREMENTS AND MODELING Underdetermined: Most of the problems in image reconstruction are underdetermined, that is we do not have enough data to uniquely determine a solution. In mathematical tomogra- phy a \\perfect reconstruction\" requires an in\fnite number of exact measurements. These are, of course never available. In a linear algebra problem, this is the case where m< n: When the measurements y do not uniquely determine the state x; additional criteria are needed to determine which solution to actually use, for example one might use the solution to ax = y which is of smallest norm. Another approach is to assume that x belongs to a subspace whose dimension is equal to the number of independent measurements. Both of these approaches are used in medical imaging. Example 1.3.2. In the refraction problem considered in example 1.1.4 we remarked that the refractive index of the lower ﬂuid n2 could be determined by an additional measurement. Suppose that we shine a beam of light in at a di\u000berent angle, so that the upper angle is ˚1 and the lower angle is ˚2: This light beam is displaced by l2 as it passes through the ﬂuid. We now have 3 equations for the two unknowns: 0 @ 11 tan(\u00121)tan(\u00122) tan(˚1)tan(˚2) 1 A \u0012h1 h2 \u0013 = 0 @h l1 l2 1 A : (1.17) In order for this equation to have a solution the measurements (h; l1;l2) must satisfy the condition 0 @ 1 tan(\u00121) tan(˚1) 1 A \u0002 0 @ 1 tan(\u00122) tan(˚2) 1 A \u0001 0 @h l1 l2 1 A =0: Here \u0002 is the vector cross product. Since sin(\u00121) 1.3. LINEARITY 27 measurement process. If x1 and x2 are two states of our system then, because the model is linear the di\u000berence in the measurements can easily be computed y1 − y2 = ax1 − ax2 = a(x1 − x2): From this formula we see that nearby states result in nearby measurements. However the reverse is often not true. There may exist states x1 and x2 which are not nearby, in the sense that kx1 − x2k is large but ka(x1 − x2)k is small. Physically, the measurements performed are not su\u000eciently independent to distinguish certain pairs of states, which are not, in fact very close together. In numerical analysis this is known as an ill-conditioned equation. Brieﬂy, a small error in the measurement process can be magni\fed by applying a−1 to the measurement vector. For an ill-conditioned problem even a good algorithm for solving linear equations can produce meaningless results. Example 1.3.3. For example, consider the system with m = n =2 and a = \u001210 110−5 \u0013 : Then x is given by a−1y where a = \u0012 10 −105 105 \u0013 : If the actual data is y =(1; 1) but we make an error in measurement and measure, ym = (1; 1+ \u000f) then the relative error is jym − yj 28 CHAPTER 1. MEASUREMENTS AND MODELING 1.3.2 Inﬁnite dimensional linear algebra The state of a `system' in medical imaging is described by a function of continuous variables. In this introductory section we consider real valued functions de\fned on the real line. Let f (x) describe the state of the system. A linear measurement of the state is usually described as an integral M(f )(x)= 1Z −1 m(x; y)f (y)dy: Here m(x; y) is a function onR\u0002Rwhich provides a model for the measurement process. It can be thought of as an in\fnite `matrix' with indices x and y: A linear transformation of an in\fnite dimensional space is called a linear operator. A linear transformation which can be expressed as an integral is called an integral operator. Suppose that the function g(x) is the output of the measurement process, to reconstruct f means solving the linear equation Mf = g: This is a concise way to write a system of in\fnitely many equations in in\fnitely many unknowns. Theorems 1.3.1 and 1.3.2 contain the complete theory for the existence and uniqueness of solutions to linear equations in \fnitely many variables. These theorems are entirely algebraic in character. No such theory exists for equations in in\fnitely many variables. It is usually a very complicated problem to describe both the domain and range of such a transformation. We close this section with a few illustrative examples. Example 1.3.4. Perhaps the simplest linear operator is the inde\fnite integral I(f )(x)= xZ 0 f (y)dy: If we use the continuous functions onRas the domain of I then every function in the range is continuously di\u000berentiable. Moreover the null-space of I is the zero function. Observe that the domain and range of I are fundamentally di\u000berent spaces. Because I(f )(0) = 0 not every continuously di\u000berentiable function is in the range of I: The derivative is a left inverse to I as the Fundamental Theorem of Calculus states that if f is continuous then d 1.3. LINEARITY 29 for every x 2R: Enlarging the domain also enlarges the range. For example the function jxj lies in the enlarged range of I; jxj = xZ 0 sign(y)dy; where sign(y)= 1 if y \u0015 0and −1if y< 0: Even though jxj is not di\u000berentiable at x =0 it is still the inde\fnite integral of a locally integrable function, however the formula djxj 30 CHAPTER 1. MEASUREMENTS AND MODELING Example 1.3.6. A real physical measurement is always some sort of an average. If the state of the system is described by a function f of a single variable x then the average of f over an interval of length 2\u000e is M\u000e(f )(x)= 1 1.4. CONCLUSION 31 The integral in (1.22) de\fnes a measure for the size of f called the Lp-norm. It is a generalization of the notion of a norm on a \fnite dimensional vector space and satis\fes the familiar conditions for a norm: kaf kp = jajkf kp and kf + gkp \u0014kf kp + kgkp: The \frst step in analyzing linear transformations of in\fnite dimensional spaces is the in- troduction of norms on the domain and range. This was not necessary in \fnite dimensions but is absolutely essential in the in\fnite dimensional case. In medical image reconstruction there is a small list of linear transformations that are very important, the Fourier transform, Radon transform and Abel transform. A large part of this text is devoted to the analysis of these operators. Exercise 1.3.3. Prove that the null-space of I acting on C0(R) is the zero function. 1.4 Conclusion By examining a large collection of examples we have seen how physical systems can be described using mathematical models. The models suggest measurements which one can make to determine the state of the system. It is important to keep in mind that mathe- matical models are just that, models, often toy models. A good model must satisfy two opposing requirements: the model should accurately depict the system under study while at the same time being simple enough to be usable. Which models are \\simple enough to be useful\" depends on what you know, one of our goals, in the succeeding chapters is to develop some sophisticated mathematical tools to work with models. The workhorse throughout this book and in most applications of mathematics to problems in measurement and signal processing is the Fourier transform. The models used in medical imaging usually involve in\fnitely many degrees of freedom. The state of the system is described by a function of continuous variables. Ultimately of course only a \fnite number of measurements can be made and only a \fnite amount of time is available to process them. Our analysis of the reconstruction process in X-ray CT passes through several stages, beginning with a description of the complete, perfect data situation and concluding with an analysis of the e\u000bects of noise on the quality of an approximate image, reconstructed from \fnitely many measurements. In mathematics, problems of determining the state of a physical system from feasible measurements are gathered under the rubric of inverse problems. The division of problems into inverse problems and direct problems is often a matter of history. Usually a physical theory which models how the state of the system determines feasible measurements preceded a description of the inverse process: how the state can be determined from measurements. Example 1.1.6 is typical though very simple example. Formula (1.22) describes the solution to the direct problem: the determination of the transit time from a knowledge of the sound speed and the depth. The inverse problem asks for a determination of the depth from a knowledge of the sound speed and the transit time. While many of the problems which arise in medical imaging are considered to be inverse problems, we do not give any systematic development of this subject. The curious reader is referred to the very nice article by Joe Keller which contains analyses of many classical inverse problems, see [41]. 32 CHAPTER 1. MEASUREMENTS AND MODELING Chapter 2 A basic model for tomography We begin our study of medical imaging with a purely mathematical model of the image reconstruction process used in transmission CT. The model begins with a very simpli\fed description of the interaction of X-rays with matter. The physical properties of an object are encoded in a function \u0016; called the absorption coe\u000ecient which quanti\fes the tendency of an object to absorb X-rays. The mathematical model describes idealized measurements that can be made of certain averages of \u0016: In mathematical terms, these measurements are described as an integral transform. Assuming that a complete, error free set of measure- ments can be made, the function \u0016 can be reconstructed. In reality, the data collected is a very limited part of the mathematically \\necessary\" data and the measurements are subject to a variety of errors. In later chapters we re\fne the measurement model and reconstruction algorithm to reﬂect more realistic models of the physical properties of X-rays and the data which is actually collected. 2.1 Tomography Literally, tomography means slice imaging. It is collection of methods for reconstruct- ing a three dimensional object from its two dimensional slices. The objects of interest in medical imaging are described by functions de\fned onR3 : The function of interest in X-ray tomography is called the absorption coe\u000ecient; it quanti\fes the tendency of an ob- ject to absorb X-rays. This function varies from point-to-point within the object and is usually taken to vanish outside it. The absorption coe\u000ecient is like density, in that it is non-negative. It is useful for medical imaging because di\u000berent anatomical structures have di\u000berent absorption coe\u000ecients. Bone has a much higher absorption coe\u000ecient than soft tissue and di\u000berent soft tissues have slightly di\u000berent coe\u000ecients. For medical applications it is crucial that normal and cancerous tissues also have slightly di\u000berent absorption coef- \fcients. However the absorption coe\u000ecients of di\u000berent soft tissues vary over a very small range. Table 2.1 lists typical absorption coe\u000ecients for di\u000berent parts of the body, these are given in Houns\feld units. This is a dimensionless quantity de\fned by comparison with the absorption coe\u000ecient of water, Htissue = \u0016tissue − \u0016water 34 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY2.1. TOMOGRAPHY 35 This models an object with constant absorption coe\u000ecient. In this case the object is determined by its intersection with the planes Hc = f(x1;x2;c): x1;x2 2Rg: For each c we let Dc = D \\ Hc: Figure 2.1 shows sample slices of a 3-dimensional object.? Figure 2.1: Parallel slices of an object. Example 2.1.2. Suppose that the object is contained in the ball of radius 1 and its absorption coe\u000ecient is \u0016(x)= ( 1 −kxk if kxk\u0014 1; 0if kxk > 1: The slices of \u0016 are the functions fc(x1;x2)= ( 1 − p 36 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY Example 2.1.3. The support of the function f (x)= x is the whole real line, even though f (0) = 0: The support of the function f (x; y)= xy is the whole plane, even though f (0;y)= f (x; 0) = 0: The support of the function ˜(0;1)(x)is[0; 1]: De\fnition 2.1.2. A function f de\fned inRn is said to have bounded support if there is an R so that f (x)= 0 if kxk >R: In this case one says that the support of f is contained in the ball of radius R: Most functions in medical imaging have bounded support. 2.1.1 Beer’s law and X-ray tomography We now turn our attention to a simple model of X-ray tomography. This refers to the usage of X-rays to reconstruct a 3-dimensional object by reconstructing its two dimensional slices. X-rays are a very high energy form of electro-magnetic radiation. An X-ray beam can be modeled as a continuous ﬂux of energy. In truth, an X-ray beam is composed of a large number of discrete particles called photons. Each photon has a well de\fned energy which is often quoted in units of electron-volts. In the last chapter of this book we consider the implications of this fact. For the present we use a continuum model and a simpli\fed, though adequate description of the interaction of the X-ray beam with matter. Three physical assumptions are used in the construction of this model: No refraction or diﬀraction: The X-rays beam travel along straight lines which are not \\bent\" by the objects they pass through. This is a good approximation to the truth, because X-ray photons have very high energy. The X-rays used are monochromatic: All the photons making up the X-ray beam have the same energy. Beer’s law: Each material encountered has a characteristic linear absorption coe\u000ecient \u0016(x) forX-raysofthe given energy. The intensity, I(x) of the X-ray beam satis\fes Beer's law dI 2.1. TOMOGRAPHY 37 of electron-volts/sec. Beer's law predicts the change in the ﬂux due to the material lying between s and s +\u0001s : I(s +\u0001s) − I(s) ˇ−\u0016(s)I(s)\u0001s: Think, now of the ﬂux as being composed of a large number, N (s) photons/second, each of the given energy, Beer's law can be rewritten in these terms as N (s +\u0001s) − N (s) ˇ−\u0016(s)N (s)\u0001s: In this formulation it is clear that \u0016(s)\u0001s can be regarded as giving the probability that a photon incident on the material at coordinate s is absorbed. Implicit in Beer's Law is the assumption that X-rays travel along straight lines, it is an essentially 1-dimensional relation. It implicitly asserts that the absorption of X-rays is an isotropic process: it does not depend on the direction of the line along which the X-ray travels. Because of its one dimensional character, Beer's law is easily applied to 2 and 3- dimensional problems. The 3-dimensional X-ray beam is modeled as a collection of 1- dimensional beams. Beer's law describes how the material attenuates each of these 1- dimensional beams. Suppose that one of the X-ray beams is traveling along the line lt;! sitting in the plane. The function i(s)= I(t! + s^!) gives the intensity of the beam at points along this line and m(s)= \u0016(t! + s^!) gives the absorption coe\u000ecient. Beer's law state that di 38 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY o (x,y) r (a) A point source of X-rays source S (b) The ﬂux through a curve not enclosing the source is zero. Figure 2.2: Analysis of an isotropic point source. Because the source is isotropic, the intensity of the beam is only a function of the distance to the source. Let I(r) denote the intensity of the ﬂux at distance r from the source, by the conservation of energy, I0 = Z x2+y2=r2 I(r)ds =2ˇrI(r): (2.2) The intensity of the beam at distance r from the source is therefore I(r)= I0 2.1. TOMOGRAPHY 39 For a point source the intensity of the rays diminish as you move away from the source; this is called \\beam spreading.\" Beer's law can be used to model this e\u000bect. Let \u0016s(x; y) denote the attenuation coe\u000ecient which accounts for the spreading of the beam. As a guess we let \u0016s =1=r and see that dI 40 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY X−ray source object detector (a) One object. X−ray source detector object object (b) Two objects. Figure 2.3: The failure of ordinary X-rays to distinguish objects. X−ray source detector object object Figure 2.4: A di\u000berent projection. The principle is clear: the more directions you make measurements from, the more arrangements of objects you can distinguish. The goal of X-ray tomography is much more ambitious, we would like to use these projections to reconstruct a picture of the slice. This problem is similar to that considered in example 1.1.5. However, it is much more challenging to reconstruct a density function from its averages along lines than to reconstruct the outline of an object from its projections. To accomplish this in principle and in practice requires a great deal more mathematics. Exercise 2.1.1. Suppose that we have an isotropic point source of X-rays in 3-dimensions of intensity I0: Find the formula for the intensity of the beams at a distance r from the source. What are the units of I(r)? 2.2. ANALYSIS OF A POINT SOURCE DEVICE 41 Exercise 2.1.2. Verify (2.5) by direct computation. Exercise 2.1.3. Describe an apparatus that would produce a uniform, non-divergent source of X-rays. 2.2 Analysis of a point source device In this section we use Beer's law to study a simple 2-dimensional apparatus and analyze what it measures. Figure 2.5 shows an apparatus with a point source of X-rays, an absorbing body and a photographic plate. We now derive an expression for the ﬂux at a point P on the photographic plate in terms of the attenuation of the beam caused by absorption as well as beam spreading. The \fnal expression involves the line integral of the attenuation coe\u000ecient. ϕ l L L+h a b P source ϕ ϕ film Figure 2.5: A point source device for measuring line integrals of the absorption coe\u000ecient. The e\u000bect of beam spreading on the intensity of the ﬂux is analyzed in example 2.1.4. The geometry of our apparatus suggests the use of polar coordinates to label points in the plane. Let r denote the distance from the source and ˚ the angle indicated in the diagram. The attenuation coe\u000ecient for the absorbing body in \fgure 2.5 is then a function of (r; ˚); denoted by \u0016a(r; ˚): The total attenuation coe\u000ecient is obtained by adding \u0016s(r)= r−1 to \u0016a: For the beam of X-rays traveling along the line through the source, at angle ˚; the di\u000berential equation describing the attenuation of the X-ray beam is dI 42 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY by h; L; and L + h respectively. Integrating equation (2.8) from r = r0 to the \flm plane r = r˚ gives log I(r˚;˚) 2.2. ANALYSIS OF A POINT SOURCE DEVICE 43 According to (2.9) the density of the \flm at P˚ is therefore γ log dF 44 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY 2.3 Some physical considerations Before proceeding with the mathematical development we brieﬂy revisit the assumptions underlying our model for the absorption of X-rays. This discussion previews topics consid- ered in later chapters and is not essential to the remainder of this chapter. The X-ray source is assumed to be monochromatic. In fact the beam of X-rays is made up of photons having a wide range of energies. The distribution of photons according to energy is described by its spectral function, S(E): If E1 and E2 are nearby energies then the energy in the beam due to photons with energies lying in the interval [E1; E2]isabout S(E1)(E2 −E1); or more precisely E2Z E1 S(E)dE: The graph of a typical spectral function is shown in \fgure 2.7. The total energy output of the source is given by Ψi = 1Z 0 S(E)dE: Figure 2.7: A typical X-ray source spectral function, courtesy Dr. Andrew Kavellas. The attenuation coe\u000ecient of a given material is a complicated function of the energy, monotonely decreasing as the energy increases. The absorption coe\u000ecient is the sum total of the results of several physical processes that X-rays undergo. A discussion of the physics behind the absorption coe\u000ecient can be found in [4]. Let \u0016(x; E) denote the absorption coe\u000ecient of the object for photons of energy E: Beer's law, applied to the photons of energy E; traveling along a line l states that the ratio Io(E)=Ii(E) of emitted ﬂux to incident ﬂux at this energy is Io(E) 2.3. SOME PHYSICAL CONSIDERATIONS 45 The incident ﬂux at energy E is S(E)dE and therefore Io(E)= S(E)dE exp 2 4− Z l \u0016(x; E)ds 3 5 : Because low energy (or soft) X-rays are absorbed more e\u000eciently than high energy (or hard) X-rays, the distribution of energies in the output beam is skewed towards higher energies. Along a given line the spectral function at energy E of the output beam is Sout(E)= S(E)exp \u0014− Z \u0016(x; E)ds\u0015 : In medical imaging, this is called beam hardening. Integrating the output over the energy gives the measured output Ψo = 1Z 0 S(E)exp 2 4− Z l \u0016(x; E)ds 3 5 dE: As before we would like to reconstruct \u0016(x; E) or perhaps some average of this function over energies. Mathematically this is a very di\u000ecult problem as the measurement, Ψo is a non- linear function of \u0016(x; E): We have avoided this problem by assuming that the X-ray beam used to make the measurements is monochromatic. This provides the much simpler linear relationship (2.7) between the measurements and the absorption coe\u000ecient. In Chapter 8 we brieﬂy consider the artifacts which result from using polychromatic X-rays and methods used to ameliorate them. The fact that the X-ray \\beam\" is not a continuous ﬂux, but is composed of discrete particles produces random errors in the measurements. This type of error is called Poisson noise, quantum noise or photon noise. In Chapter 12 we analyze this e\u000bect, showing that the available information in the data is proportional to the square root of the number of photons used to make the measurement. The accuracy of the measurements is the ultimate limitation on the number of signi\fcant digits in the reconstructed absorption coe\u000ecient. Table 2.1 lists the absorption coe\u000ecients of di\u000berent structures encountered in medical CT. The absorption coe\u000ecients of air (-1000) and bone (990) de\fne the range of values present in a typical clinical situation. The dynamic range of a clinical CT-measurement is about 2000 Houns\feld units. From the table it is apparent that the variation in the absorption coe\u000ecients of soft tissues is about 2% of this range. For X-ray CT to be clinically useful this means that the reconstruction of the absorption coe\u000ecient needs to be accurate to less than a half a percent or about 10 Houns\feld units. An obvious solution to this problem would be to increase the number of photons. Since each X-ray photons carries a very large amount of energy, considerations of patient safety preclude this solution. The number of X-ray photons involved in forming a CT image is approximately 107= cm2 : This should be compared with the 1011 to 1012= cm2 photons, needed to make a usable photographic image. In ordinary photography, quantum noise is not a serious problem because the number of photons involved is very large. In X-ray tomography, quantum noise places a de\fnite limit on the distinctions which can be made in a CT image. 46 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY In Chapter 4 we give a formula for determining a function from its integrals on all lines in the plane. Of course it is not possible to make an in\fnite number of measurements. This means that we need a method for reconstructing an approximation to a function from a \fnite collection of line integrals. In Chapter 8, beginning with the exact reconstruction formula, we derive algorithms for use with \fnitely many measurements. 2.4 The deﬁnition of the Radon transform 2.4. THE DEFINITION OF THE RADON TRANSFORM 47 De\fnition 2.4.1. The Radon transform is the integral transform de\fned in (2.13) mapping functions de\fned inR2 to functions de\fned onR\u0002 S1: The parametric representation of the line gives a more explicit formula Rf (t; !)= Z f (s^! + t!)ds: (2.14) In terms of Cartesian coordinates inR2 and (t; \u0012)-coordinates for the set of oriented lines this can be expressed as Rf (t; \u0012)= 1Z −1 f (t cos \u0012 − s sin \u0012; t sin \u0012 + s cos \u0012)ds: (2.15) It is not necessary for f to be either continuous or of bounded support. The Radon transform can be de\fned, apriori for functions whose restriction to each line can be inte- grated, 1Z −1 jf (t cos \u0012 − s sin \u0012; t sin \u0012 + s cos \u0012)dsj < 1 for all (t; \u0012) 2R\u0002S1: (2.16) With these conditions the improper integrals in (2.15) are unambiguously de\fned. We say that functions which satisfy (2.16) are in the natural domain of the Radon transform. This is really two di\u000berent conditions: (1). The function is regular enough so that restricting it to any line gives a locally integrable function. (2). The function goes to zero rapidly enough for the improper integrals to converge. The function f (x; y) = 1 is not in the natural domain of the Radon transform because it does not decay at in\fnity. The function f (x; y)= (x2 +y2)−1 is not in the natural domain of R because the integrals in (2.16) diverge if t =0: An understanding of the domain of Radon transform is a very important part of its mathematical analysis. Though in applications to medical imaging, functions of interest are usually piecewise continuous and zero outside of some disk and therefore belong to the natural domain of R:. We compute the Radon transforms for a several simple classes functions. Example 2.4.1. For E ˆR2 recall that ˜E(x; y)= ( 1if (x; y) 2 E; 0if (x; y) =2 E: For functions of this type the Radon transform has a very simple geometric description. R˜E(t; !) = the length of the intersection lt;! \\ E: If E is a closed, bounded subset then ˜E belongs to the natural domain of the R: These functions model objects with constant absorption coe\u000ecient 48 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY De\fnition 2.4.2. The disk of radius r centered at (a; b) is denoted Dr(a; b)= f(x; y) j (x − a) 2 +(y − b) 2 <r2g: Often Dr(0; 0) is denoted by Dr: Example 2.4.2. The function ˜D1(x; y) is a special case of the general class considered in the previous example. The formula for the Radon transform of ˜D1 is R˜D1(t; !)= (2 p 2.4. THE DEFINITION OF THE RADON TRANSFORM 49 Such a function is called an even function on the space of oriented lines. Our goal is the recovery of a function, f (x; y) from a knowledge of its Radon transform, Rf (t; !): Since R is a linear map one might hope that there is a linear map R−1 from functions onR\u0002 S1 to functions onR2 satisfying R−1 \u000e Rf = f: Ideally the inverse map should also be given by an integral formula. This turns out to be the case, but the derivation and analysis of this formula are rather involved. Because these spaces of functions are in\fnite dimensional, \fnding the inverse is not just a problem in linear algebra. The domain of R−1 is the range of R and neither the domain of R or R−1 is easy to describe explicitly. These issues are studied in Chapter 4. The remainder of this section is devoted to further properties of the Radon transform and its inverse. Naively one would expect that in order for R−1 to exist it would be necessary that Rf (t; !) = 0 for all pairs (t; !)only if f \u0011 0: In light of its de\fnition it is easy to construct examples of functions which are not zero, but have zero Radon transform. Example 2.4.4. De\fne the function f (x; y)= ( 1if (x; y)= (0; 0); 0if (x; y) 6=(0; 0): Clearly Rf (t; !) = 0 for all (t; !): From the point of view of measurement, this is a very trivial example. A somewhat more interesting example arises as follows. Example 2.4.5. De\fne a function f by setting f (x; y)= 1 if x 2 [−1; 1] and y = 0 and zero otherwise. Then Rf (t; !)= 0 if ! 6=(0; \u00061) and Rf (0; (0; \u00061)) = 2: In this case the Radon transform is usually zero, but for certain special lines it is not. Observe that if we replace f by a function ~f which is 1 on some other subset ofR\u0002 0 of total length 2 then Rf =R ~f: This gives examples, which are not entirely trivial. where the Radon transform does not contain enough information to distinguish between two functions. The concept that underlies these examples is that of a set of measure zero. De\fnition 2.4.3. A subset E ˆRn is said to be of measure zero if for any \u000f> 0there is a collection of balls B(xi;ri)so that E ˆ 1[ i=1 B(xi;ri) and 1X i=1 rn i <\u000f: A set of measure zero carries no n-dimensional mass. For example, a point is set of measure in the line, a line is a set of measure zero in the plane, a plane is a set of measure zero inR3 ; etc. 50 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY A basic fact about sets of measure zero is the following: if f is a function de\fned inRn and the set of points where f 6= 0 is a set of measure zero then ZRn jf (x)jdx =0: With this concept we can state a basic result about the Radon transform Proposition 2.4.1. If f is a function de\fned in the plane such that ZR2 jf (x)jdx =0 then the set of values (t; !) 2R\u0002 S1 for which Rf (t; !) 6=0 is itself a set of measure zero. As example 2.4.5 shows, a function supported on a set of measure zero cannot, in general be reconstructed from its Radon transform. Since the Radon transform is linear, it cannot distinguish functions which di\u000ber only on a set of measure zero. This is a feature common to any set of measurements de\fned by integrals. While it is important to keep in mind, it does not lead to serious di\u000eculties in medical imaging. The support properties of f are reﬂected in the support properties of Rf: Proposition 2.4.2. Suppose that f (x; y) is a function de\fned in the plane with f (x; y)= 0 if x2 + y2 >R2 then Rf (t; !)=0 if jtj >R: (2.20) Proof. Any line lt,ω with jtj >R lies entirely outside of the support of f: From the de\fnition it follows that Rf (t; !)= 0 if jtj >R: 2.4. THE DEFINITION OF THE RADON TRANSFORM 51 The proof of the lemma is at the end of this section. It already indicates the di\u000eculty of inverting the Radon transform. These functions are not in the natural domain of the Radon transform because 1Z −1 jfn(−s sin \u0012; s cos \u0012)jds = 1 for any value of \u0012: On the other hand Rfn(t; !) = 0 for all t 6=0: So in some sense, Rfn is supported on the set of measure zero f0g\u0002 S1: For each n we modify fn to obtain a function Fn in the natural domain of R such that RFn(t; !) = 0 for all (t; !); with jtj > 1: On the hand, the functions Fn do not vanish outside the disk or radius 1. The modi\fed functions are de\fned by Fn(r; \u0012)= (fn(r; \u0012)for r> 1; 0for r \u0014 1: A line lt;! with jtj > 1 lies entirely outside the unit disk. On such a line, the lemma applies to show that RFn(t; !)= Z lt,ω fnds =0: On the other hand Fn is bounded in a neighborhood of (0; 0) and therefore RFn(t; !)is de\fned for all (t; !) 2R\u0002S1: This shows that the Radon transform of a function may vanish for all t with jtj >r without the function being zero outside disk of radius r. Exercise 2.4.1. Let f (x; y)=1 if x2 + y2 = 1 and zero otherwise. Show that Rf (t; !)= 0 for all (t; !) 2R\u0002 S1: Exercise 2.4.2. Suppose that fx1;::: ; xng are n-distinct points on the unit circle. For i 6= j; let lij denote the line segment joining xi to xj and rij denote a real number. Show that if rij = rji for all i 6= j then there is function f supported on the line segments flijg such that Z lij fds = rij for all i 6= j: Exercise 2.4.3. Show that a line segment has measure zero as a subset of the plane. Exercise 2.4.4. Show that the x-axis has measure zero as a subset of the plane. Exercise 2.4.5. Show that the set f(0;!): ! 2 S1g has measure zero as a subset ofR\u0002 S1: 2.4.1 Appendix: Proof of Lemma 2.4.1* The proof of the theorem makes use of the elementary theory of complex variables and is not needed for the subsequent development of the book. 52 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY Proof. Let z = x + iy and observe that by Euler's formula it follows that fn =Re z−n: This means that for t 6=0 Rfn(t; !)= Re Z lt,ω z−nds; where ds is the arc element along the line. The line (t; !(\u0012)) can be represented as z =(t + is)eiθ;t 2R: Using this complex parameterization, the Radon transform of fn can be re-expressed as a complex contour integral: Rfn(t; \u0012)= 1Z −1 fn((t + is)eiθ)ds =Re 2 6 4−ie−iθ Z Re(e−iθ z)=t z−ndz 3 7 5 ; (2.21) where the arc length element, along the line is written in complex notation as ds = −ie−iθdz: As R z−n =(1 − n) −1z1−n the theorem follows from (2.21). 2.4. THE DEFINITION OF THE RADON TRANSFORM 53 as indicated f˝ (t; !)g are vectors inR2 describing the position of the patient as a function of (t; !): How sensitive are the measurements to errors? This is a question about the continuity properties of the map f 7! Rf: It it important to know the answer to this question, but in the \fnal analysis is also not quite the correct question. What we really want to know is how sensitive the reconstruction method is to such errors. In other words, we want to understand the continuity properties of R−1: Since we have not yet constructed R−1 we consider the somewhat easier question of the continuity of R: That is, how sensitive are the measurements to the data. We need to choose a way to measure the size of the errors in both the data and the measurements. For the present we make the following simple choices: For the measurements we use the maximum norm: k Rf (t; !) − Rg(t; !)k1 =max (t;!) j Rf (t; !) − Rg(t; !)j: As a norm on the data we use kf k1;1 =max (t;!) Z lt,ω jfdsj: With these de\fnitions it is not di\u000ecult to see that k Rf (t; !) − Rg(t; !)k1 \u0014kf − gk1;1 (2.23) For the problem of patient motion (2.23) implies that k Rf − Rf˝ k1 \u0014 max ˝ (t;!) kf − f˝ k1;1: If on average f does not vary too quickly and the motions which arise are not too large then this estimate shows that the \\actual\" measurements fRf˝ (t;!)(t; !)g are close to the model measurements fRf (t; !)g: Since the functions which arise in imaging are not continuous it is important that only the average variation needs to be controlled and not pointwise variation. This point is illustrated by a one dimensional example. Example 2.4.7. If ˝ 6= 0 then max x j˜[0;1](x) − ˜[0;1](x − ˝ )j =1 on the other hand for j˝ j < 1it is also truethat 1Z −1 j˜[0;1](x) − ˜[0;1](x − ˝ )jdx =2˝: If f (x; y)= ˜[0;1](x)˜[0;1](y) then a small change in the position of the patient can lead to a large error in Rf; when measured in the maximum-norm. If ˝ =(\u000f; 0) for any \u000f< 0 then Rf (1; (1; 0)) − Rf˝ (1; (1; 0)) = 1: 54 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY Choosing di\u000berent norms leads to di\u000berent estimates for the continuity of the map f 7! Rf: For a function h de\fned onR\u0002 S1 de\fne khk1;1 =max !2S1 1Z −1 jh(t; !)jdt: (2.24) Proposition 2.4.3. Suppose that f is an absolutely integrable function in the natural domain of the Radon transform then k Rf k1;1 \u0014 ZR2 jf (x; y)jdxdy: (2.25) Proof. The proof of this proposition is simply the change of variables formula and the Fubini theo- rem. For each ! 2 S1 1Z −1 j Rf (t; !)jdt \u0014 1Z −1 1Z −1 jf (t! + s^!)jdsdt = ZR2 jf (x; y)jdxdy: (2.26) In the second line we use the fact that (s; t) 7! t! + s^! is an orthogonal change of variables. Since the last line is independent of ! this proves the proposition. 2.4. THE DEFINITION OF THE RADON TRANSFORM 55 Because the measurement is linear we see that if f1 and f2 are two functions then jM\u000ff1(t) − M\u000ff2(t)j\u0014 1 56 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY This is called the back-projection formula. While it is a reasonable guess, it does not give the correct answer. Figure 2.8 shows the result of using backprojection to reconstruct a simple black and white image. The object is recognizable but very blurry. (a) A simple object. (b) The result of back- projecting its Radon transform. Figure 2.8: Back-projection does not work! To \fnd the true inverse of the Radon transform requires an indirect approach passing through the Fourier transform. The Fourier transform, while perhaps more familiar, is a less transparent integral transformation than the Radon transform. On the other hand, the inverse of the Fourier transform is easier to obtain. In the next chapter we consider the Fourier transform in some detail as it is of fundamental importance in the theory of image reconstruction and signal processing. To close this chapter we study the Radon transform acting on radial functions. Inverting the transform in this case is simpler, reducing to a special case of the Abel transform. 2.5 The Radon transform of a radially symmetric function Recall that a function f de\fned onR2 is radial or radially symmetric if f (x; y)= F (x2 +y2): The Radon transform of a radial function does not depend on !: It is given in (2.18) as an integral transform of a function of one variable. After changing variables we see that this is a special case of an Abel transform. For 0 <\u000b \u0014 1, the \u000b-Abel transform of g is de\fned by A\u000b g(t)= 1 2.5. THE RADON TRANSFORM OF A RADIALLY SYMMETRIC FUNCTION 57 Using the formula for the inverse of the Abel transform (2.37), which is derived below, and a change of variables, we can solve equation (2.18) to obtain F (r2)= − 1 58 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY As both integrals have the same limit as r ! 0 this identity holds for r \u0015 0: It is not di\u000ecult to prove that this function is di\u000berentiable, set G(r2)= − 1 2.5. THE RADON TRANSFORM OF A RADIALLY SYMMETRIC FUNCTION 59 2.5.2 The Abel transform \u0003 The Abel transform is a familiar feature of many problems involving measurement. It is also an important example of a non-trivial integral transform which nonetheless admits a fairly explicit analysis. Formally the inverse of the \u000b-Abel transform is A−1 \u000b = −@x A1−\u000b : (2.37) This formula is a consequence of the identity sZ x dt 60 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY If f is also continuous then this inde\fnite integral is di\u000berentiable and therefore f = @x A1−\u000b \u000e A\u000b f: Data of interest in medical imaging are usually not continuous. Instead, the data is usually piecewise continuous and so can be represented as a sum f (x)= fc(x)+ NX j=1 \u000bj˜[aj ;bj](x); where fc(x) belongs to C0(R); and the other term collects the jumps in f: As noted A1−\u000b \u000e A\u000b ˜[a;b](x)= 1Z x ˜[a;b](s)ds: If x 6= a or b then this function is di\u000berentiable with derivative 0 or 1: In order to interpret the formula at the exceptional points we need to extend our notion of di\u000berentiability. De\fnition 2.5.1. A locally integrable function f has a weak derivative,if there is a lo- cally integrable function f1 such that, for every continuously di\u000berentiable function g; the following identity holds 1Z −1 f (x)g0(x)dx = − 1Z −1 f1(x)g(x)dx: (2.42) In this case f1 is called the weak derivative of f: If f is a di\u000berentiable function then formula (2.42), with f1 = f 0 is just the usual integration by parts formula. The weak derivative of the inde\fnite integral of a piecewise continuous function is the function itself. This shows that the inversion formula, properly understood, is also valid for the sort of data which arises in imaging applications. Exercise 2.5.7. Prove (2.38) by using the change of variables t = \u0015x +(1 − \u0015)s and the classical formula 1Z 0 d\u0015 2.5. THE RADON TRANSFORM OF A RADIALLY SYMMETRIC FUNCTION 61 Exercise 2.5.9. Use exercise 2.5.8 to prove the following uniqueness result for the Radon transform. If f is a piecewise continuous, radial function in the natural domain of the Radon transform and Rf (t)= 0 for jtj >R then f (r)= 0 if r> R: Exercise 2.5.10. Generalize the argument given above to prove that A\u000b \u000e A\f =A\u000b+\f : For what range of \u000b and \f does this formula make sense? Exercise 2.5.11. For 0 <a<b compute ga;b =A\u000b(˜[a;b]) and verify by explicit calculation that ˜[a;b] is the weak derivative of − A1−\u000b(ga;b): Exercise 2.5.12. Provide the detailed justi\fcation for the derivation of (2.41) for f a continuous, absolutely integrable function. Exercise 2.5.13. Suppose that f 2C1(R)and that f and f 0 are absolutely integrable. Show that A\u000b [−@x A1−\u000b] f = f: Exercise 2.5.14. Suppose that f is a piecewise continuous, absolutely integrable function. Show that f is the weak derivative of F (x)= − 1Z x f (s)ds: 2.5.3 Fractional derivatives \u0003 To evaluate the inverse of the Abel transform for a function with a jump discontinuity required an extension of the classical notion of a derivative. The Abel transform A\u000b is sometimes called an \u000bth-order anti-derivative or fractional integral. For example if \u000b =1 then A1 f is just the usual inde\fnite integral of f: This interpretation of the Abel transforms motivates the following de\fnition for a function with a fractional order of di\u000berentiability. De\fnition 2.5.2. Let 0 <\f < 1; a continuous function f has a \fth-derivative if sup x2Rsup h jf (x + h) − f (x)j 62 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY As with the usual notion of di\u000berentiability, \f-di\u000berentiability is a local property of a function. There is an analogous de\fnition for \f-H¨older continuous functions de\fned on an interval [a; b]: The space of such functions is denoted by C \f([a; b]): It is reasonable to enquire as to the relationship between 1-H¨older continuity and dif- ferentiability. If f is a di\u000berentiable function then the mean value theorem states that for each 0 <h there is a 0 \u0014 k< h so that f (x + h) − f (x) 2.5. THE RADON TRANSFORM OF A RADIALLY SYMMETRIC FUNCTION 63 This di\u000bers a little from the form of the Abel transform as the integral there extends from x to in\fnity, rather than 0 to x: The function k(x; y) is called the kernel function of the integral operator K: The kernel functions for the Abel transforms are singular where x = y; : In this section we restrict ourselves to kernel functions which satisfy an estimate of the form jk(x; y)j\u0014 M; and analyze Volterra operators acting on functions de\fned on the interval [0; 1]: Volterra operators often appear in applications where one is required to solve an equa- tion of the form g = f + Kf =(Id +K)f: Such equations turn out to be very easy to solve. Formally we would write f =(Id +K) −1g: Still proceeding formally, we can express (Id +K)−1 as an in\fnite series: (Id +K) −1f = 1X j=0(−1) jK jf: (2.43) This is called the Neumann series for (Id +K)−1; it is obtained from the Taylor expansion of the function (1 + x)−1 about x =0: (1 + x) −1 = 1X j=0(−1) jxj: Here K jf means the j-fold composition of K with itself. The sum on the right hand side of (2.43) is an in\fnite sum of functions and we need to understand in what sense it converges. That is, we need to choose a norm to measure the sizes of the terms in this sum. A useful property of Volterra operators is that this series converges for almost any reasonable choice of norm. The basic estimates are summarized in the proposition. Proposition 2.5.3. Let 1 \u0014 p \u00141; suppose that jk(x; y)j\u0014 M and f 2 Lp([0; 1]) then for x 2 [0; 1] and j \u0015 1 jK jf (x)j\u0014 M jxj−1 64 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY This veri\fes (2.44) for j = 1; assume it has been proved for j; then jK j+1f (x)j = \f \f \f \f \f \f xZ 0 k(x; y)K jf (y)dy \f \f \f \f \f \f \u0014 xZ 0 M M jyj−1 2.5. THE RADON TRANSFORM OF A RADIALLY SYMMETRIC FUNCTION 65 In applications K describes a measurement process and f represents measurements. In this context it can be quite di\u000ecult to accurately approximate f 0: As a result, it is often stated that a problem which involves solving an equation of the form (2.48) is ill-posed. Small errors in measurement can lead to substantial errors in the solution of this type of equation. While it is reasonable to expect that we can control measurement errors in the sup-norm, it is usually not possible to control errors in the derivatives of measurements, even in an Lp-norm. The inverse problem is ill-posed because K −1 is not continuous as a map from C0([0; 1]) to itself. Remark 2.5.2. The material in this section is a small sample from the very highly developed \feld of integral equations. A good introductory treatment can be found in [83] or [61]. Exercise 2.5.16. Suppose that instead of assuming that k(x; y) is uniformly bounded we assume that xZ 0 jk(x; y)j qdy \u0014 M for a 1 <q < 1 and all x 2 [0; 1]: Show that estimates analogous to (2.44) hold for f 2 Lp([0; 1]) where p = q(q − 1)−1: Exercise 2.5.17. Using the previous exercise, show that the equation g =(Id +K)f is solvable for g 2 Lp([0; 1]): Exercise 2.5.18. Volterra operators of the \frst kind are in\fnite dimensional generaliza- tions of strictly upper triangular matrices. These are matrices aij such that aij =0if i \u0014 j: Suppose that A is an n \u0002 n strictly upper triangular matrix. Show that An =0: Prove that I + A is always invertible and give a formula for its inverse. 66 CHAPTER 2. A BASIC MODEL FOR TOMOGRAPHY Chapter 3 Introduction to the Fourier transform In this chapter we introduce the Fourier transform and review some of its basic properties. The Fourier transform is the \\swiss army knife\" of mathematical analysis, it is a powerful general purpose tool with many useful special features. In particular the theory of the Fourier transform is largely independent of the dimension: the theory of the Fourier trans- form for functions of one variable is formally the same as the theory for functions of 2, 3 or n variables. This is in marked contrast to the Radon, or X-ray transforms. For simplicity we begin with a discussion of the basic concepts for functions of a single variable. 3.1 The complex exponential function. 68 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM The logarithm is then extended to non-zero complex numbers by setting log z = s + i\u0012 =log jzj + i tan−1 \u0012 Im z 3.2. FUNCTIONS OF A SINGLE VARIABLE 69 3.2.1 Absolutely integrable functions 70 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Interchange the integrations in the last formula and use example 3.2.5 to get lim ϵ!0+ 1 3.2. FUNCTIONS OF A SINGLE VARIABLE 71 Example 3.2.2. Recall that ˜[a;b)(x) equals 1 for a \u0014 x< b and zero otherwise. Its Fourier transform is given by ^˜[a;b)(˘)= e−ib˘ − e−ia˘ 72 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Exercise 3.2.4. Prove formula (3.10). Show that for any numbers a<b there is a constant M so that j ^˜[a;b)(˘)j\u0014 M 3.2. FUNCTIONS OF A SINGLE VARIABLE 73 3.2.3 Regularity and decay 74 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM De\fnition 3.2.2. A function f 2Cj(R)if it has j-continuous derivatives. Since the Fourier transform involves integration over the whole real line it is often important to assume that these derivatives are also integrable. To quantify rates of decay we compare a function f (x) to a simpler function such as a power of jxj: De\fnition 3.2.3. A function f (x) decays like jxj−\u000b if there are constants C and R so that jf (x)j\u0014 C 3.2. FUNCTIONS OF A SINGLE VARIABLE 75 exactly like j˘j−1: This is a reﬂection of the fact that r1(x)is not everywhere di\u000berentiable, having jump discontinuities at \u00061: If f has j integrable derivatives then, by repeatedly integrating by parts, we get a formula for ^f (˘) ^f (˘)= \u0014 1 76 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1x (a) 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1x (b) 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1x (c) Figure 3.1: Furry functions 0.88 0.9 0.92 0.94 0.96 0.98 1 1.02 1.04 0.2 0.22 0.24 0.26 0.28 0.3x (a) 0.9 0.92 0.94 0.96 0.98 1 1.02 1.04 0.25 0.252 0.254 0.256 0.258 0.26x (b) 0.9 0.92 0.94 0.96 0.98 1 0.25850.2586 0.2587 0.2588 0.2589 0.259x (c) Figure 3.2: A furry function at smaller scales Example 3.2.5. Let '(x) be a smooth, rapidly decaying function with Fourier transform ^'(˘) which satis\fes the following conditions (1). 0 \u0014 ^'(˘) \u0014 1 for all ˘; (2). ^'(0) = 1; (3). ^'(˘)=0 if j˘j > 1: For example we could take ^'(˘)= ( e − 1 3.2. FUNCTIONS OF A SINGLE VARIABLE 77 For a given ˘ at most one term in the sum is non-zero. If k> 1then ^fk(˘) is zero \\most of the time.\" On the other hand the best rate of decay that is true for all ˘ is j ^fk(˘)j\u0014 C 78 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Of course (3.22) gives a formula for the Fourier transform of xkf (x) in terms of the Fourier transform of f : dxkf (˘)= i k@k ˘ ^f (˘): (3.23) A special case of this proposition arises if f vanishes outside a bounded interval. In this case xkf (x) is absolutely integrable for any positive integer k and therefore ^f (˘) is a function with in\fnitely many derivatives. The derivatives tend to zero as j˘j tends to in\fnity but the rate of decay may be the same for all the derivatives, for example ^r1(˘)= 2sin ˘ 3.2. FUNCTIONS OF A SINGLE VARIABLE 79 Exercise 3.2.6. Suppose that f 0g and fg0 are absolutely integrable. Show that the limits lim t!1 fg(x) and lim t!−1 fg(x) both exist. Exercise 3.2.7. Prove that for any number j the jth-derivative @j ˘ ^r1(˘) has a term which decays exactly like j˘j−1: Exercise 3.2.8. Show that fP (x); de\fned in example 3.25 and its \frst n derivatives tend to zero as jxj tends to in\fnity. Exercise 3.2.9. Show that the function, '(x) de\fned in example 3.2.5 is in\fnitely di\u000ber- entiable. 3.2.4 Fourier transform on L 2(R) 80 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM The norm on L2(R) is de\fned by an inner product, hf; giL2 = ZRn f (x) 3.2. FUNCTIONS OF A SINGLE VARIABLE 81 The proof of the Parseval formula is given in exercise 3.2.13. In many physical applications the square integral of a function is interpreted as a total energy. Up to the factor of 2ˇ; Parseval's formula says that the total energy in f is the same as that in ^f: Often the variable ˘=2ˇ is thought of as a frequency, following the quantum mechanical practice, higher frequencies corresponding to higher energies. In this context j ^f (˘)j2 is interpreted as the energy density of f at frequency ˘=2ˇ: As we shall see \\noise\" is essentially a high frequency phenomenon, a noisy signal has a lot of energy at high frequencies. The Parseval formula shows that the L2-norm is very intimately connected to the Fourier transform. However there is a price to pay. For a function like f (x) in example 3.2.7 the integral de\fning ^f (˘) is not absolutely convergent. Parseval's formula says that F is a continuous linear transformation, if the L2-norm is used in both the domain and range. This indicates that it should be possible to extend the Fourier transform to all functions in L2(R) and this is, indeed the case. Let f 2 L2(R); for each R> 0 de\fne ^fR(˘)= RZ −R f (x)e −ix˘dx: (3.28) From Parseval's formula it follows that if R1 <R2; then k ^fR1 − ^fR2k 2 L2 =2ˇ Z R1\u0014jxj\u0014R2 jf (x)j 2dx Because f is square integrable the right hand side of this formula goes to zero as R1;R2 tend to in\fnity. This says that, if we measure the distance in the L2-norm, then the functions < ^fR > are clustering, closer and closer together as R !1: Otherwise put, < ^fR > is an L2-Cauchy sequence. Because L2(R) is a complete, normed vector space, this implies that f ^fRg converges to a limit as R !1; this limit de\fnes ^f: The limit of a sequence in the L2-norm is called a limit in the mean; it is denoted by the symbol LIM : De\fnition 3.2.5. If f is a function in L2(R) then its Fourier transform is de\fned by ^f = LIM R!1 ^fR; where ^fR is de\fned in (3.28). Example 3.2.8. The function f (x)= x−1˜[1;1](x) is square integrable but not absolutely integrable. We use integration by parts to compute ^fR(˘): ^fR(˘)= e−i˘ 82 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Note that each term diverges at ˘ =0; but the divergences cancel, giving a function which is actually continuous at ˘ =0: A consequence of Parseval's formula is the identity. 1Z −1 f (x) 3.2. FUNCTIONS OF A SINGLE VARIABLE 83 3.2.5 Basic properties of the Fourier Transform onR The following properties hold for integrable or square integrable functions. 1. Linearity: The Fourier transform is a linear operation:[f + g = ^f +^g; c\u000bf = \u000b ^f; \u000b 2C: 2. Scaling: The Fourier transform of f (ax), a function dilated by a 2Ris given by 1Z −1 f (ax)e −i˘xdx = 1Z −1 f (y)e − iξy 84 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM 3.2.6 Convolution 3.2. FUNCTIONS OF A SINGLE VARIABLE 85 Proposition 3.2.6. Suppose that f and g are absolutely integrable then[f \u0003 g(˘)= ^f (˘)^g(˘): (3.36) Proof. Because f \u0003 g(x) is absolutely integrable it has a Fourier transform. Since f (y)g(x − y)is an absolutely integrable function of (x; y) the following manipulations are easily justi\fed:[f \u0003 g(˘)= 1Z −1 (f \u0003 g)(x)e−iξxdx = 1Z −1 1Z −1 f (y)g(x − y)e−iξxdydx = 1Z −1 1Z −1 f (y)g(t)e−iξ(y+t)dtdy = ^f (˘)^g(˘): (3.37) 86 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Formula (3.37) also suggests that the convolution of an L1-function and an L2-function should be de\fned. For if f 2 L1(R)then ^f (˘) is a bounded function and so if g 2 L2(R) then ^f ^g 2 L2(R) as well. This is indeed the case, even though the integral de\fning f \u0003 g need not be absolutely convergent, see exercise (3.2.17). Using the linearity of the integral, simple changes of variable and interchanges in the order of integration it can easily be shown that the convolution product is commutative, associative, and distributive. Proposition 3.2.7. If f 2 L1(R) and g; h belong to L1(R) or L2(R) then g \u0003 f = f \u0003 g; (f \u0003 g) \u0003 h = f \u0003 (g \u0003 h);f \u0003 (g + h)= f \u0003 g + f \u0003 h: If either ^f or ^g decreases rapidly then so does ^f (˘)^g(˘): If ^g decreases rapidly then by Proposition 3.2.3 this implies that g is a smooth function with integrable derivatives. Applying this proposition again we see that f \u0003 g is also a smooth function with integrable derivatives. Theorem 3.2.3. If R jf (x)jdx < 1 and g has k continuous derivatives for which there is a constant M so that j@j xg(x)j <M for all j \u0014 k and all x: Then f \u0003 g has k continuous derivatives with @j x(f \u0003 g)(x)= f \u0003 (@j xg)(x): Proof. In light of the hypotheses on f and g; for any j \u0014 k we can interchange the order of di\u000berentiation and integration to obtain @j x(f \u0003 g)(x)= @j x 1Z −1 f (y)g(x − y)dy = 1Z −1 f (y)@j xg(x − y)dy = f \u0003 (@j xg)(x): 3.2. FUNCTIONS OF A SINGLE VARIABLE 87 0 0.5 1 1.5 2 2.5 3 –2 –1 1 2 x Figure 3.3: Graphs of '\u000f; with \u000f = :5; 2; 8. For \u000f> 0let '\u000f(x)= \u000f−1'(x=\u000f); see \fgure 3.3. Observe that '\u000f is supported in [−\u000f; \u000f]and Z '\u000f(x)dx = Z 1 88 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Proof. As f is continuous at x; given \u0011> 0 there is a \u000e> 0so that jx − yj <\u000e )jf (x) − f (y)j <\u0011: (3.42) As ' has bounded support there is an \u000f0 such that if \u000f< \u000f0 then the support of 'ϵ is contained in the interval (−\u000e; \u000e): Finally since the total integral of 'ϵ is 1wehave, foran \u000f< \u000f0 that j'ϵ \u0003 f (x) − f (x)j = \f \f \f \f \f \f δZ −δ 'ϵ(x − y)(f (y) − f (x))dx \f \f \f \f \f \f \u0014 δZ −δ 'ϵ(x − y)jf (y) − f (x)jdx \u0014 δZ −δ 'ϵ(x − y)\u0011dx \u0014 \u0011: (3.43) In the second line we use that fact that 'ϵ is non-negative and, in the third line, estimate (3.42). 3.2. FUNCTIONS OF A SINGLE VARIABLE 89 Proof of the Fourier inversion formula, completed. Suppose that f and ^f are absolutely integrable and 'ϵ is as above. Note that ^f (˘) is a continuous function. For each \u000f> 0 the function 'ϵ \u0003 f is absolutely integrable and continuous. Its Fourier transform is ^'(\u000f˘) ^f (˘); which is absolutely integrable. By Proposition 3.2.8 it converges locally uniformly to ^f (˘): Since these functions are continuous we can use the Fourier inversion formula to conclude that 'ϵ \u0003 f (x)= 1 90 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Exercise 3.2.21. Use Corollary A.6.1 and Proposition 3.2.8 to prove that if f 2 Lp(R)for a1 \u0014 p< 1 then '\u000f \u0003 f converges to f in the Lp-norm. Exercise 3.2.22. For the functions k; de\fned in (3.44), \fnd the constants ck so that 1Z −1 k(x)dx =1: Exercise 3.2.23. Use the Fourier inversion formula to prove that cfg(˘)= 1 3.2. FUNCTIONS OF A SINGLE VARIABLE 91 Example 3.2.13. Suppose that is a non-negative function which vanishes outside the interval [−\u000f; \u000f] and has total integral 1, 1Z −1 (x)dx =1: If f is a locally integrable function then f \u0003 (x) is the weighted average of the values of f over the interval [x − \u000f; x + \u000f]: Note that \u0003 also has total integral 1 1Z −1 \u0003 (x)dx = 1Z −1 1Z −1 (y) (x − y)dydx = 1Z −1 1Z −1 (y) (t)dtdy =1 \u0001 1=1: (3.46) In the second to last line we reversed the order of the integrations and set t = x − y: Thus f \u0003 ( \u0003 ) is again an average of f: Note that \u0003 (x) is generally non-zero for x 2 [−2\u000f; 2\u000f]; so convolving with \u0003 produces more blurring than convolution with alone. Indeed we know from the associativity of the convolution product that f \u0003 ( \u0003 )=(f \u0003 ) \u0003 ; so we are averaging the averages, f \u0003 : This can be repeated as many times as one likes, the j-fold convolution \u0003j has total integral 1 and vanishes outside the interval [−j\u000f; j\u000f]: Of course the Fourier transform of \u0003j is [ ^ (˘)]j which therefore decays j timesasfast as ^ (˘): We could also use the scaled j-fold convolution \u000e−1 \u0003j (\u000e−1x) to average our data. This function vanishes outside the interval [−j\u000e\u000f; j\u000e\u000f] and has Fourier transform [ ^ (\u000e˘)]j : If we choose \u000e = j−1 then convolving with this function will not blur details any more than convolving with itself but better suppresses high frequency noise. By choosing j and \u000e we can control, to some extent, the trade o\u000b between blurring and noise suppression. Exercise 3.2.25. If a and b are positive numbers then de\fne wa;b(x)= 1 92 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM 3.2.8 The δ-function 3.2. FUNCTIONS OF A SINGLE VARIABLE 93 One would like this number to be small. This is accomplished by putting more of the mass of ' near to x =0: On the other hand the rate at which ^'(˘) decays as j˘j!1 is determined by the smoothness of '(x): Using the simplest choice, '(x)= 1 94 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM phenomenon.\" It results from using a discontinuous cuto\u000b function in the Fourier domain. This e\u000bect is analyzed in detail, for the case of Fourier series in section 5.5. 0.2 0.4 0.6 0.8 1 –2 –1 1 2 x (a) sinc \u0003˜[−1,1]: 0.02 0.04 0.06 0.08 0.1 0.12 0.14 –2 –1 1 2 x (b) sinc2 \u0003˜[−1,1]: Figure 3.5: Approximate \u000e-functions convolved with ˜[−1;1]: 3.2.9 Windowing and resolution In this section we give a standard de\fnition for the resolution present in a measurement of the form h \u0003 f: Resolution is a subtle and, in some senses, subjective concept. Suppose that h(x) is a non-negative function with a single hump similar to those shown in \fgure 3.3. The important features of this function are 1: It is non-negative, 2: It has a single maximum value, which it attains at 0; 3: It is monotone increasing to the left of the maximum and monotone decreasing to the right. (3.47) De\fnition 3.2.7. Let h satisfy these conditions and let M be the maximum value it attains. Let x1 < 0 <x2 be respectively the smallest and largest numbers so that h(x1)= h(x2)= M 3.2. FUNCTIONS OF A SINGLE VARIABLE 95 Convolving h(x)with f produces two copies of h; h \u0003 f (x)= h(x)+ h(x − d): If d> FWHM(h)then h\u0003f has two distinct maxima separated by a valley. If d \u0014 FWHM(h) then the distinct maxima disappear. If the distance between the impulses is greater than the FWHM(h) then we can \\resolve\" them in the \fltered output. In \fgure 3.6 we use a triangle function for h: The FWHM of this function is 1; the graphs show h and the results of convolving h with a pair of unit impulses separated, respectively by 1:2 > 1and :8 < 1: 0 0.2 0.4 0.6 0.8 1 –2 –1 1 2 x (a) h(x) 0 0.2 0.4 0.6 0.8 1 –2 –1 1 2 x (b) h(x + :6) + h(x − :6) 0 0.2 0.4 0.6 0.8 1 1.2 –2 –1 1 2 x (c) h(x + :4) + h(x − :4) Figure 3.6: Illustration of the FWHM de\fnition of resolution This de\fnition is often extended to functions which do not satisfy all the conditions in (3.47) but are qualitatively similar. For example the characteristic function of an interval ˜[−B;B](x) has a unique maximum value and is monotone to the right and left of the maximum. The FWHM(˜[−B;B]) is therefore 2B: Another important example is the sinc- function, sinc(x)= sin(x) 96 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM h \u0003 f one actually computes F −1(^h ^f ): For practical computations we are only able to use the information in Fourier transform over a \fnite range of frequencies. So the \frst step in applying a \flter is cutting o\u000b ^f (˘) outside a \fnite interval. This is called windowing the Fourier transform of f: The simplest way to window ^f is to replace it with ˜[−B;B](˘) ^f (˘): Windowing in the ˘-variable becomes convolution in the x-variable, f is replaced with F −1(˜[−B;B] ^f )(x)= B sinc(Bx) 3.2. FUNCTIONS OF A SINGLE VARIABLE 97 Using the Taylor expansion for sine function show that, as j gets large, FWHM(hj) ' s 98 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM does not have a classical derivative at x = −1; 0and 1: However the function g(x)= 8 >< >: 0if jxj > 1; 1if − 1 \u0014 x \u0014 0; −1if 0 \u0014 x \u0014 1 is the weak derivative of f: A very useful condition is for the weak derivative to be in L2: De\fnition 3.2.8. Let f 2 L2(R)we say that f has an L2-derivative if there is a function f1 2 L2(R)so that for every once, di\u000berentiable function, ' vanishing outside a \fnite interval the formula hf; ' 0i = −hf1;'i holds. In this case we say that f1 is the L2-derivative of f: We use the usual notations for the L2-derivative, i.e. f 0 or @xf; etc. A function f 2 L2 which is di\u000berentiable in the ordinary sense and whose derivative is f 0 2 L2 is also di\u000berentiable in the L2-sense. Its L2-derivative is just f 0: An important fact about L2-derivatives is that they satisfy the fundamental theorem of calculus. Above it was shown that an L2-function is locally integrable. If f1 is the L2-derivative of f then for any a<b we have that f (b) − f (a)= bZ a f1(x)dx: In particular a function with an L2-derivative is continuous. Using H¨older's inequality we see that jf (b) − f (a)j\u0014 p 3.2. FUNCTIONS OF A SINGLE VARIABLE 99 De\fnition 3.2.9. The higher L2-derivatives are de\fned exactly as in the classical case. If f 2 L2(R) has an L2-derivative, and f 0 2 L2 also has an L2-derivative, then we say that f has two L2-derivatives. This can be repeated to de\fne all higher derivatives. A simple condition for a function f 2 L2(R)to have jL2-derivatives, is that there are functions ff1;::: ;fjgˆ L2(R) so that for every j-times di\u000berentiable function '; vanishing outside a bounded interval and 1 \u0014 l \u0014 j we have that hf; ' [l]iL2 =(−1) lhfl;'iL2 : The function fl is then the lth L2-derivative of f: Standard notations are also used for the higher L2-derivatives, e.g. f [l];@l xf; etc. The basic result about L2-derivatives is. Theorem 3.2.4. A function f 2 L2(R) has jL2-derivatives if and only if ˘j ^f (˘) is in L2(R): In this case cf [l] =(i˘) l ^f (˘); (3.49) moreover 1Z −1 jf [l](x)j 2dx = 1 100 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM have a limit as h ! 0: In order for this limit to exist it is clearly necessary that the ratios jf (x + h) − f (x)j 3.2. FUNCTIONS OF A SINGLE VARIABLE 101 3.2.12 Some reﬁned properties of the Fourier transform \u0003 102 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM limR!1 gR(x0)= g(x0)and f (x)= g(x)for x in an interval containing x0: Then, it is also true that f (x) − g(x) = 0 in an interval containing x0 and therefore f (x0) = lim R!1 fR(x0) = lim R!1 gR(x0) + lim R!1(fR(x0) − gR(x0)): The Fourier inversion process is sensitive to the local behavior of f: It is important to note that this result is special to one dimension. The analogous result is false for the Fourier transform inRn if n \u0015 2: This phenomenon is carefully analyzed in [57], see also section 3.3.8. The next result states that if a function f has bounded support then its Fourier trans- form cannot. Proposition 3.2.10. Suppose supp f ˆ (−R; R) if ^f also has bounded support then f \u0011 0: Proof. The radius of convergence of the series P1 0 (−ix˘) j =j! is in\fnity, and it converges to e−ixξ; uniformly on bounded intervals. Combining this with the fact that f has bounded support, we conclude that we may interchange the integration with the summation to obtain ^f (˘)= 1Z −1 f (x)e−iξxdx = RZ −R 1X j=0 f (x) (−ix˘) j 3.2. FUNCTIONS OF A SINGLE VARIABLE 103 can be repeated to obtain the Taylor expansion of ^f (˘) about an arbitrary ˘0 : ^f (˘)= RZ −R e−i(ξ−ξ0)xf (x)eiξ0xdx = RZ −R 1X j=0 [−i(˘ − ˘0)x] j 104 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM By replacing f by ei˘0xf for an appropriate choice of ˘0 we can also make E(˘)= 0: With these normalizations, the variance of the position and the momentum, (\u0001x)2 and (\u0001˘)2, are given by (\u0001x) 2 = 1Z −1 x2jf (x)j 2dx; (\u0001˘) 2 = 1Z −1 ˘2j ^f (˘)j 2 d˘ 3.2. FUNCTIONS OF A SINGLE VARIABLE 105 With the expected position and momentum normalized to be zero, the variance in the position and momentum are given by \u0001x = 0 @ 1Z −1 x2f 2 1 A 1=2 and \u0001˘ = 0 @ 1Z −1 f 2 x 1 A 1=2 : The estimate (3.58) is equivalent to \u0001x \u0001 \u0001˘ \u0015 1 106 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM accomplish this extension we need to revisit the de\fnition of a generalized function. In section A.4.6 we gave the following de\fnition: Let C1 c (R) denote in\fnitely di\u000berentiable functions de\fned onRwhich vanish outside of bounded sets. These are called test functions. De\fnition 3.2.12. A generalized function onRis a linear function, l de\fned on the set of test functions such that there is a constant C and an integer k so that, for every f 2C1 c (R)we have the estimate jl(f )j\u0014 C sup x2R 2 4(1 + jxj) k kX j=0 j@j xf (x)j 3 5 (3.60) These are linear functions on C1 c (R) which are, in a certain sense continuous. The constants C and k in (3.60) depend on l but do not depend on f: The expression on the right hand side de\fnes a norm on C1 c (R); for convenience we let kf kk =sup x2R 2 4(1 + jxj) k kX j=0 j@j xf (x)j 3 5 : The observation that we make is the following: if a generalized function satis\fes the estimate jl(f )j\u0014 Ckf kk then it can be extended, by continuity, to any function f which is the limit of a sequence <fn >ˆC1 c (R) in the sense that lim n!1 kf − fnkk =0: Clearly f 2Ck(R)and kf kk < 1: This motivates the following de\fnition De\fnition 3.2.13. A function f 2C1(R) belongs to Schwartz class if kf kk < 1 for every k 2N: The set of such functions is a vector space denoted by S(R): From the de\fnition it is clear that C1 c (R) ˆS(R): (3.61) Schwartz class does not have a norm with respect to which it is a complete normed linear space, instead each k\u0001kk de\fnes a semi-norm. A sequence <fn >ˆS(R) converges to f 2S(R) if and only if lim kf − fnkk =0 for every k 2N: With this notion of convergence, Schwartz class becomes a complete metric space, the distance is de\fned by dS (f; g)= 1X j=0 2 −j kf − gkj 3.2. FUNCTIONS OF A SINGLE VARIABLE 107 Let '(x) 2C1 c (R) be a non-negative function with the following properties (1). '(x)= 1 if x 2 [−1; 1]; (2). '(x)= 0 if jxj > 2: De\fne 'n(x)= '(n−1x); it is not di\u000ecult to prove the following proposition. Proposition 3.2.11. If f 2S(R) then fn = 'nf 2C1 c (R) ˆS(R) converges to f in S(R): That is lim n!1 kfn − f kk =0 for every k: (3.62) The proof is left as an exercise. From the discussion above it therefore follows that every generalized function can be extended to S(R): Because (3.62) holds for every k; if l is a generalized function and f 2S(R)then l(f ) is de\fned as l(f ) = lim n!1 l('nf ): To show that this makes sense, it is only necessary to prove that if <gn >ˆC1 c (R)which converges to f in Schwartz class then lim n!1 l(gn − 'nf )= 0: (3.63) This is an immediate consequence of the triangle inequality and the estimate that l satis\fes: there is a C and k so that jl(gn − 'nf )j\u0014 Ckgn − 'nf kk \u0014 C[kgn − f kk + kf − 'nf kk: (3.64) Since both terms on the right hand side of the second line tend to zero as n !1; equa- tion (3.63) is proved. In fact the generalized functions are exactly the set of continuous linear functions on S(R): For this reason the set of generalized functions is usually denoted by S 0(R): Why did we go to all this trouble? How will this help extend the Fourier transform to S 0(R)? The integration by parts formula was the \\trick\" used to extend the notion of derivative to generalized functions. The reason it works is that if f 2S(R)then @xf 2S(R) as well. This implies that l(@xf ) is de\fned, and a generalized function whenever l itself is. Schwartz class has a similar property. Theorem 3.2.7. The Fourier transform is an isomorphism of S(R) onto itself, that is if f 2S(R) then both F(f ) and F −1(f ) also belong to S(R): Moreover, for each k there is an k0 and constant Ck so that kF(f )kk \u0014 Ckkf kk0 for all f 2S(R): (3.65) The proof of this theorem is an easy consequence of results in section 3.2.3. We give the proof for F; the proof for F −1 is essentially identical. 108 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Proof. Since f 2S(R) for any j; k 2N[f0g we have the estimates j@j xf (x)j\u0014 kf kk 3.2. FUNCTIONS OF A SINGLE VARIABLE 109 If f 2S(R) then the identity in (3.68) holds with g =^'; as a simple interchange of integrations shows. Hence, for all f 2S(R) l'( ^f )= 1Z −1 f (x)^'(x)dx = l ^'(f ): This shows that the Fourier transform for generalized functions is indeed an extension of the ordinary transform: if a generalized function l is represented by an integrable function in the sense that l = l' then the de\fnition of the Fourier transform of l is consistent with the earlier de\fnition of the Fourier transform of ': Example 3.2.16. If f 2S(R)then ^f (0) = 1Z −1 f (x)dx: This shows that ^\u000e = l1 which is represented by an ordinary function equal to the constant 1: Example 3.2.17. On the other hand the Fourier inversion formula implies that 1Z −1 ^f (˘)d˘ =2ˇf (0) and therefore bl1 =2ˇ\u000e: This is an example of an ordinary function that does not have a Fourier transform, in the usual sense, and whose Fourier transform, as a generalized function is not an ordinary function. Recall that a sequence <ln >ˆS 0(R) converges to l in S 0(R) provided that l(g) = lim n!1 ln(g) for all g 2S(R): (3.70) This is very useful for computing Fourier transforms because the Fourier transform is con- tinuous with respect to the limit in (3.70). It follows from the de\fnition that: bln(g)= ln(^g) (3.71) and therefore lim n!1 bln(g) = lim n!1 ln(^g)= l(^g)= ^l(g): (3.72) Example 3.2.18. The generalized function l˜[0,1) can be de\fned as a limit by l˜[0,1)(f ) = lim \u000f#0 1Z 0 e −\u000fxf (x)dx: 110 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM The Fourier transform of le−ϵx˜[0,1) is easily computed using example 3.2.15, it is F(le−ϵx˜[0,1))(f )= 1Z −1 f (x)dx 3.2. FUNCTIONS OF A SINGLE VARIABLE 111 Exercise 3.2.42. If l1=x is the Cauchy principal value integral l1=x(f )= P: V: 1Z −1 f (x)dx 112 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM from which the estimate is immediate. The proof of the converse statement is a little more involved; it uses the Fourier inversion formula and a change of contour. We present the outlines of this argument, the complete justi\fcation for the change of contour can be found in [40]. Let x> L> 0; the Fourier inversion formula states that f (x)= 1 3.3. FUNCTIONS OF SEVERAL VARIABLES 113 3.3.1 L 1-case The n-dimensional Euclidean space is the collection of ordered n-tuples of real numbersRn = f(x1;::: ;xn): xj 2Rfor j =1;::: ;ng: We o use lower case, bold Roman letters x; y etc. to denote points inRn ; that is x =(x1;::: ;xn)or y =(y1;::: ;yn): In this case xj is called the jth-coordinate of x: The Fourier transform of a function of n-variables is also a function of n-variables. It is customary to use the lower case, bold Greek letters, ˘˘˘ or \u0011\u0011\u0011 as coordinates on the Fourier transform space with ˘˘˘ =(˘1;::: ;˘n)or \u0011\u0011\u0011 =(\u00111;::: ;\u0011n): De\fnition 3.3.1. If f (x) is an integrable function of n-variables then the Fourier trans- form, ^f of f is de\fned by ^f (˘˘˘)= ZRn f (x)e −i˘˘˘\u0001xdx for ˘˘˘ 2Rn : (3.81) Note that ˘˘˘ \u0001 x is the inner product, if x =(x1;::: ;xn)and ˘˘˘ =(˘1;::: ;˘n)then ˘˘˘ \u0001 x = x \u0001 ˘˘˘ = nX j=1 ˘jxj: This inner product is sometimes denoted by hx;˘˘˘i;the volume form onRn is denoted dx = dx1 ::: dxn: Since f is absolutely integrable overRn the integral can be computed as an iterated integral ZRn f (x)e −i˘˘˘\u0001xdx = 1Z −1 \u0001\u0001\u0001 1Z −1 f (x1;::: ;xn)e −ix1˘1dx1 \u0001\u0001\u0001 e −ixn˘ndxn; (3.82) changing the order of the one dimensional integrals does not change the result. When thought of as a linear transformation, it is customary to use F(f )todenotethe Fourier transform of f: It is useful to have a clear geometric picture of the inner product to have a better understanding of the functions eih˘˘˘;xi: To that end we write ˘˘˘ in polar form as ˘˘˘ = r!: Here r = k˘˘˘k is the length of ˘˘˘ and ! its direction. Write x = x0 + x1! where x0 is orthogonal to !; (i.e. hx0;!i =0). As hx;!i = x1 the function hx;!i depends only x1: Thus e ihx;˘˘˘i = e irx1 is a function which oscillates in the !-direction with wave length 2ˇ 114 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM for several choices of ˘˘˘: In these \fgures white corresponds to +1 and black corresponds to −1: The Fourier transform at ˘˘˘ = r! can be re-expressed as ^f (r!)= 1Z −1 Z L f (x 0 + x1!)e −irx1dx 0dx1: (3.83) Here L is the (n − 1)-dimensional subspace orthogonal to ! : L = fx 0 2Rn : hx 0;!i =0g and dx0 is the (n − 1)-dimensional Euclidean measure on L: –4 –2 0 2 4 y –4 –2 2 4 x (a) –4 –2 0 2 4 y –4 –2 2 4 x (b) Figure 3.8: Real and imaginary parts of exp(ih(x; y); (1; 1)i) –4 –2 0 2 4 y –4 –2 2 4 x (a) –4 –2 0 2 4 y –4 –2 2 4 x (b) Figure 3.9: Real and imaginary parts of exp(ih(x; y); (2; 0)i) The Fourier transform is invertible; under appropriate hypotheses there is an explicit formula for the inverse. 3.3. FUNCTIONS OF SEVERAL VARIABLES 115 Theorem 3.3.1 (Fourier Inversion Formula). Suppose that f is an absolutely integrable function de\fned onRn : If R j ^f (˘˘˘)jd˘˘˘< 1 as well then f (x)= ZRn ^f (˘˘˘)e ix\u0001˘˘˘ d˘˘˘ 116 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM 3.3.2 Regularity and decay Integrable functions onRn are described qualitatively in terms of two general properties: Decay at inﬁnity: How fast does f (x) go to zero as jxj! 1. Regularity: How regular is f ?If f is di\u000berentiable then it is also important to know that these derivatives are integrable. The Riemann-Lebesgue lemma holds in n-variables. Proposition 3.3.1 (Riemann-Lebesgue Lemma). Let f be an absolutely integrable function onRn then ^f is a continuous function and limj˘˘˘j!1 ^f (˘˘˘)= 0: The proof is very similar to the one dimensional case and is left to the reader. The Fourier Transform is sensitive to decay and regularity. In order to understand how decay at in\fnity for f is reﬂected in properties of ^f we \frst suppose that f vanishes outside the ball of radius R. It can be shown without di\u000eculty that ^f (˘˘˘) is a di\u000berentiable function, and its derivatives are given by @˘˘˘j ^f (˘˘˘)= Z BR @˘j [f (x)e −i˘˘˘\u0001x]dx = Z BR f (x)(−ixj)e −i˘˘˘\u0001xdx =\\(−ixjf )(˘˘˘): (3.88) Formulˆ in several variables can rapidly become cumbersome and unreadable. For- tunately there is a compact notation which gives formulˆ in n-variables that have the simplicity and readability of one variable formulˆ . This is called multi-index notation. De\fnition 3.3.2. A multi-index is an ordered n-tuple of non-negative integers usually denoted by a lower case Greek letter. For \u000b\u000b\u000b =(\u000b1;::: ;\u000bn); a multi-index, set \u000b\u000b\u000b!= \u000b1! \u0001\u0001\u0001 \u000bn!and j\u000b\u000b\u000bj = \u000b1 + \u0001\u0001\u0001 + \u000bn: The function j\u000b\u000b\u000bj is called the length of \u000b\u000b\u000b. The following conventions are also useful: x \u000b\u000b\u000b = x\u000b1 1 x\u000b2 2 \u0001\u0001\u0001 x\u000bn n and @\u000b\u000b\u000b x = @\u000b1 x1 @\u000b2 x2 \u0001\u0001\u0001 @\u000bn xn : Example 3.3.1. If f is a k-times di\u000berentiable function onRn then there is a multi- dimensional analogue of Taylor's formula: f (x)= X f\u000b\u000b\u000b : j\u000b\u000b\u000bj\u0014kg @\u000b\u000b\u000b x f (0)x\u000b\u000b\u000b 3.3. FUNCTIONS OF SEVERAL VARIABLES 117 Example 3.3.2. The binomial formula also has a higher dimensional analogue (x1 + \u0001\u0001\u0001 + xn) k = k! X f\u000b\u000b\u000b : j\u000b\u000b\u000bj=kg x\u000b\u000b\u000b 118 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Proof. The proof is a consequence of the Fourier inversion formula. The decay hypothesis implies that f (x)= 1 3.3. FUNCTIONS OF SEVERAL VARIABLES 119 3.3.3 L 2-theory 120 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM The Parseval formula evidently extends to all functions f 2 L2(R2 ): This shows that the Fourier transform is a continuous mapping of L2(Rn ) to itself: if <fn > is a sequence with LIM n!1 fn = f then LIM n!1 ^fn = ^f: The L2-inversion formula is also a consequence of the Parseval formula. Proposition 3.3.4 (L2-inversion formula). Let f 2 L2(Rn ) and de\fne FR(x)= 1 3.3. FUNCTIONS OF SEVERAL VARIABLES 121 3.3.4 Basic properties of the Fourier Transform onRn 122 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Here cn is a constant and J\u0017 (z) is the order \u0017 Bessel function de\fned by the integral J\u0017 (z)= a\u0017z\u0017 ˇZ 0 e iz cos(\u0012) sin2\u0017(\u0012)d\u0012: Here a\u0017 is a constant. Example 3.3.3. The Fourier transform of the characteristic function of the unit ball B1 ˆRn is given by the radial integral d˜B1(˘˘˘)= cn 3.3. FUNCTIONS OF SEVERAL VARIABLES 123 That is kf \u0003 gkL1 \u0014kf kL1kgkL1 : (3.99) Any function g 2 L1(Rn ) is the limit, in the L1-norm of a sequence of functions <gn >; where each gn is a bounded function of bounded support. The estimate (3.99) shows that kf \u0003 gn − f \u0003 gmkL1 = kf \u0003 (gn − gm)kL1 \u0014kf kL1kgn − gmkL1: Since L1(Rn ) is complete this implies that the convolution f \u0003 g can be de\fned as the limit of the L1-Cauchy sequence <f \u0003gn >: Convolution of integrable functions therefore de\fnes a bilinear operation from L1(Rn ) \u0002 L1(Rn )to L1(Rn ): The convolution product has most of the properties one expects of a multiplication. Proposition 3.3.5. The convolution product is commutative, associative and distributive: for f; g 2 L1(Rn ) we have g \u0003 f = f \u0003 g; (f \u0003 g) \u0003 h = f \u0003 (g \u0003 h);f \u0003 (g + h)= f \u0003 g + f \u0003 h: The proofs are left as exercises for the reader. As in the one dimensional case there is no locally integrable function so that, for all integrable functions f; \u0003 f = f: The n-dimensional \u000e-function is a generalized function de\fned by the condition that, for any continuous function f; ZRn \u000e(x)f (x)dx = f (0): (3.100) The argument given in section 3.2.8 shows that no locally integrable function can satisfy this requirement. The Fourier transform of \u000e is computed, formally as before: ^\u000e(˘˘˘)= ZRn \u000e(x)e −ix\u0001˘˘˘d˘˘˘ =1: Applying (3.100) to fx(y)= f (x − y)shows that \u000e \u0003 f (x)= ZRn \u000e(y)fx(y)dy = fx(0) = f (x): The n-dimensional convolution has the same intimate connection with the Fourier trans- form as in the one dimensional case. Proposition 3.3.6. Suppose that f and g are absolutely integrable then[f \u0003 g(˘˘˘)= ^f (˘˘˘)^g(˘˘˘): 124 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM Proof. The proof is a simple change in then order of integrations followed by a change of variable:[f \u0003 g(˘˘˘)= ZRn (f \u0003 g)(x)e−iξξξ\u0001xdx = ZZRn\u0002Rn f (y)g(x − y)e−iξξξ\u0001xdydx = ZZRn\u0002Rn f (y)g(t)e−iξξξ\u0001(y+t)dtdy = ^f (˘˘˘)^g(˘˘˘): (3.101) 3.3. FUNCTIONS OF SEVERAL VARIABLES 125 3.3.6 The support of f \u0003 g. Suppose that f and g have bounded support. For applications to medical imaging it is important to understand how the support of f \u0003 g is related to the supports of f and g: To that end we de\fne the algebraic sum of two subsets ofRn : De\fnition 3.3.4. Suppose A and B are subsets ofRn . The algebraic sum of A and B is de\fned as the set A + B = fa + b 2Rn : a 2 A; and b 2 Bg: Using this concept we can give a quantitative result describing the way in which con- volution \\smears\" out the support of a function. Lemma 3.3.2. The support of f \u0003 g is contained in supp f + supp g. Proof. Suppose that x is not in supp f + supp g. This means that no matter which y is selected either f (y)or g(x − y) is zero. Otherwise x = y +(x − y) would belong to supp f + supp g. This implies that f (y)g(x − y) is zero for all y 2Rn and therefore f \u0003 g(x)= ZRn f (y)g(x − y)dy =0 as well. This proves the lemma. 126 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM ε after convolution original noise Figure 3.10: f is smeared into the \u000f-neighborhood of supp(f ): If f represents an image then the convolution of f with '\u000f smears the image. The value of '\u000f \u0003 f (x) depends on the values of f in the ball of radius \u000f about x: The parameter \u000f is therefore measure of the resolution of the measuring apparatus. At points where the image is slowly varying the measured image is very close to the actual image. Near points where f is rapidly varying this may not be the case. Noise is usually a high frequency phenomenon with \\mean zero;\" the smoothing averages out the noise. At the same time, the image is blurred. The size of \u000f determines the degree of blurring. The Fourier transform of f \u0003 '\u000f is ^f (˘˘˘)^'(\u000f˘˘˘): Because ' has integral 1 it follows that ^'(0) = 1: As the support of ' is a bounded set, its Fourier transform is a smooth function. This shows that ^'(\u000f˘˘˘) ˇ 1if k˘˘˘k << \u000f−1: Thus the low frequency part of f \u0003 '\u000f closely approximates that of f: On the other hand ^'(˘˘˘) tend to zero rapidly as k˘˘˘k! 1: Thus the high frequency content of f is strongly suppressed in f \u0003 '\u000f: Unfortunately both noise and \fne detail are carried by the high frequency components. Noise looks like rapid local variations in the image; convolving f with a smooth function produces a smoother and therefore less noisy image. Exercise 3.3.12. If f is a locally integrable function and ' has bounded support show that, if f is continuous at x then lim \u000f#0 '\u000f \u0003 f (x)= f (x): The functions f'\u000fg are an approximation to the \u000e-function. 3.3.7 L 2-derivatives \u0003 3.3. FUNCTIONS OF SEVERAL VARIABLES 127 The function fj is the weak xj-partial derivative of f: Higher derivatives are de\fned recur- sively as before. For a multi-index \u000b\u000b\u000b we say that f has an \u000b\u000b\u000bth weak derivative if there is a locally integrable function, f\u000b\u000b\u000b so that for every smooth function, ' of bounded support we have ZRn f@\u000b\u000b\u000b x 'dx =(−1) \u000b\u000b\u000b ZRn f\u000b\u000b\u000b'dx: Recall that a smooth function with bounded support is called a test function. The case of principal interest is where f and its weak derivatives are in L2(Rn ): De\fnition 3.3.6. Let f 2 L2(Rn ) and suppose that its weak xj-partial derivative belongs to L2(Rn ); we then say that f has an L2 xj-partial derivative. For a multi-index \u000b\u000b\u000b we say that f has an \u000b\u000b\u000bth L2-derivative if its weak \u000b\u000b\u000bth-derivative belongs to L2: Proposition 3.3.9. A function f 2 L2(Rn ) is k-times L2-di\u000berentiable if and only if ZRn k˘˘˘k 2kj ^f (˘˘˘)j 2d˘˘˘< 1: (3.104) In this case, for each \u000b\u000b\u000b with j\u000b\u000b\u000bj\u0014 k we have the relations d@\u000b\u000b\u000b x f =(i˘˘˘) \u000b\u000b\u000b ^f and ZRn j@\u000b\u000b\u000b x f (x)j 2dx = ZRn j˘˘˘\u000b ^f (˘˘˘)j 2 d˘˘˘ 128 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM There is an important di\u000berence between one and several dimensions in the theory of L2-derivatives. In one dimension we showed that an L2-di\u000berentiable function is continuous (actually H¨older- 1 3.3. FUNCTIONS OF SEVERAL VARIABLES 129 function can have more and more weak derivatives. There is an interplay between the order of the singularity and the size of the set where the function is singular. In these examples the singular set is a point, which constitutes the smallest possible singular set. The Fourier transform of fa can be computed in terms of the (n−2) 130 CHAPTER 3. INTRODUCTION TO THE FOURIER TRANSFORM 3.3.8 The failure of localization in higher dimensions \u0003 The localization principle is a remarkable feature of the 1-dimensional Fourier transform. Suppose that f is an integrable function de\fned onR: According to the localization principle the convergence of the partial inverse fR(x)= 1Chapter 4 The Radon transform In Chapter 2 we introduced the Radon transform and discussed its simpler properties. After reviewing its de\fnition we establish several further properties of this transform. The remainder of the chapter is devoted to a study of its inverse. 4.1 The Radon transform 132 CHAPTER 4. THE RADON TRANSFORM allows an identi\fcation ofR\u0002 S1 withR\u0002 [0; 2ˇ): With this identi\fcation dtd\u0012 can be used as an area element on the space of lines. The integral of a function h(t; \u0012)= h(t; !(\u0012)) is given by 2ˇZ 0 1Z −1 h(t; \u0012)dtd\u0012: We often use the notation dtd! to denote this measure onR\u0002 S1: De\fnition 4.1.1. The set L2(R\u0002 S1) consists of measurable functions for which khk 2 L2(R\u0002S1) = 2ˇZ 0 1Z −1 jh(t; \u0012)j 2dtd\u0012 (4.2) is \fnite. A function h onR\u0002 S1 is continuous if h(t; \u0012)is 2ˇ-periodic in \u0012 and continuous as a function onR\u0002 [0; 2ˇ]: Similarly h is di\u000berentiable if it is 2ˇ-periodic and di\u000berentiable onR\u0002 [0; 2ˇ]and @\u0012h is also 2ˇ-periodic. Higher orders of di\u000berentiability have similar de\fnitions. The Radon transform has several properties analogous to those established for the Fourier transform in the previous chapter. Suppose that f and g are two functions with bounded supported. There is a simple formula relating R(f \u0003 g)toRf and Rg: Proposition 4.1.1. Let f and g be piecewise continuous functions with bounded support then R[f \u0003 g](t; !)= 1Z −1 Rf (s; !)Rg(t − s; !)ds: (4.3) Remark 4.1.1. Colloquially one says that the Radon transform converts convolution in the plane to convolution in the a\u000ene parameter. Proof. The proof is a calculation. Fix a direction !; coordinates (s; t) for the plane are de\fned by the assignment (s; t) 7! s^! + t!: This is an orthogonal change of variables so the area element onR2 is given by dsdt: In these variables the convolution of f and g becomes f \u0003 g(s^! + t!)= 1Z −1 1Z −1 f (a^! + b!)g((s − a)^! +(t − b)!)dadb: 4.1. THE RADON TRANSFORM 133 The Radon transform of f \u0003 g is computing by switching the order of the integrations: Rf \u0003 g(˝; !)= 1Z −1 f \u0003 g(˝! + s^!)ds = 1Z −1 1Z −1 1Z −1 f (a^! + b!)g((s − a)^! +(˝ − b)!)dadbds = 1Z −1 1Z −1 1Z −1 f (a^! + b!)g((s − a)^! +(˝ − b)!)dsdadb = 1Z −1 Rf (b; !)Rg(˝ − b; !)db: (4.4) In the second to last line we interchanged the s-integration with the a and b integrations. 134 CHAPTER 4. THE RADON TRANSFORM Lemma 4.1.1. If f is a function with bounded support and bounded, continuous \frst partial derivatives then Rf (t; !) is di\u000berentiable in t and R@xf (t; !)= −!1@t Rf (t; !); R@yf (t; !)= −!2@t Rf (t; !): (4.6) Proof. We consider only the x-derivative, the y-derivative is identical. From (4.5) and the linearity of the Radon transform we conclude that R \u0014 fhe1 − f 4.2. INVERSION OF THE RADON TRANSFORM 135 for an \u000f> 0; then f belongs to the natural domain of the Radon transform. The results in this section extend to functions in the natural domain of R: The proofs in this case are left to the reader. Using functional analytic methods the domain of the Radon transform can be further extended, allowing functions with both less regularity and slower decay. An example of such an extension was already presented in section 2.4.2. We return to this question in section 4.6. Exercise 4.1.1. Prove formula (4.5). The argument is similar to that used in the proof of (4.3). Exercise 4.1.2. Give the details of the argument in the proof of Lemma 4.1.1 showing that Rf (t; !) is di\u000berentiable in the t-variable. Exercise 4.1.3. Show how to derive formula (4.7) from (4.6). Exercise 4.1.4. The Laplace operator \u0001 is de\fned by \u0001f = −(@2 xf + @2 y f ): Find a formula for R[\u0001f ] in terms of Rf: Exercise 4.1.5. Suppose that A :R2 !R2 is an arbitrary linear transformation how is RfA related to Rf ? Exercise 4.1.6. Let A\u0012 denote the rotation through the angle \u0012: Setting !(˚) = (cos ˚; sin ˚); let Rf (t; ˚)=Rf (t; !(˚)) so that RfAθ (t; ˚)=Rf (t; \u0012 + ˚): Using these formulˆ show that R[(y@x − x@y)f ](t; ˚)= (@\u0012 R)f (t; ˚): 4.2 Inversion of the Radon Transform Now we are ready to use the Fourier transform to invert the Radon transform. 4.2.1 The Central slice theorem The Fourier transform and Radon transform are connected in a very simple way. In medical imaging this relationship is called the Central slice theorem. Theorem 4.2.1 (Central slice theorem). Let f be an absolutely integrable function in the natural domain of R: For any real number r and unit vector ! we have the identity 1Z −1 Rf (!; t)e −itrdt = ^f (r!): (4.10) 136 CHAPTER 4. THE RADON TRANSFORM Proof. From the de\fnition of the Radon transform, the integral on the left is equal to 1Z −1 Rf (!; t)e−itrdt = 1Z −1 1Z −1 f (t! + s^!)e−itrdsdt: (4.11) This integral is absolutely convergent and therefore we may make the change of variables, (x; y)= t! + s^!: Checking that the Jacobian determinant is 1 and noting that t = h(x; y);!i; the above integral therefore becomes 1Z −1 1Z −1 f (t! + s^!)e−itrdsdt = ZZR2 f (x; y)e−ih(x,y),ωirdxdy = ^f (r!) (4.12) This completes the proof of the central slice theorem. 4.2. INVERSION OF THE RADON TRANSFORM 137 If h(t; !) belongs to L2(R) for a \fxed ! then the one dimensional Parseval formula implies that 1Z −1 jh(t; !)j 2dt = 1 138 CHAPTER 4. THE RADON TRANSFORM L2(R\u0002 S1)to L2(R2 ): These assertions follow from Corollary A.5.1 and the observation that khk 2 L2(R\u0002S1) = 1 4.2. INVERSION OF THE RADON TRANSFORM 139 Proof. Because Rf is an even function, it follows that its Fourier transform satis\fes fRf (t; !)= fRf (−t; −!): (4.21) As f and ^f are absolutely integrable it follows from Theorem 3.3.1 that f (x; y)= 1 140 CHAPTER 4. THE RADON TRANSFORM Remark 4.2.3. Formula (4.20) allows the determination of f from its Radon transform. This formula completes a highly idealized, mathematical model for medical image reconstruction. \u000f We consider a two dimensional slice of a three dimensional object, the physical pa- rameter of interest is the attenuation coe\u000ecient f (x; y): According to Beer's law, the intensity I(t;!) of X-rays (of a given energy) traveling along a line, lt;! is attenuated according the di\u000berential equation: dI(t;!) 4.2. INVERSION OF THE RADON TRANSFORM 141 4.2.3 Backprojection \u0003 142 CHAPTER 4. THE RADON TRANSFORM Proposition 4.2.1. Suppose that f is an absolutely integrable and square integrable func- tion in the natural domain of the Radon transform then r 4.2. INVERSION OF THE RADON TRANSFORM 143 4.2.4 Filtered Backprojection We now turn our attention to understanding the inversion formula for R: It can be under- stood as a two step process: (1). The radial integral is interpreted as a \flter applied to the Radon transform. The \flter acts only in the a\u000ene parameter, the output of the \flter is denoted by G Rf (t; !)= 1 144 CHAPTER 4. THE RADON TRANSFORM for all values of r: In other words, the Hilbert transform is not a local operation. Concep- tually, the Hilbert transform is the most di\u000ecult part of the Radon inversion formula. On the other hand, the Hilbert transform has a very simple expression in terms of the Fourier transform and this makes it easy to implement. We now compute a couple of examples of Hilbert transforms. Example 4.2.1. Let f (x)= sin(x) 4.2. INVERSION OF THE RADON TRANSFORM 145 Since sign(r)r = jrj we can identify G Rf as G Rf (t; !)= 1 146 CHAPTER 4. THE RADON TRANSFORM Example 4.2.3. In the \frst example f is the characteristic function on the unit disk. It is de\fned by f (x; y)= ( 1 k(x; y)k\u0014 1 0 k(x; y)k > 1: Using the rotational symmetry, we can check that Rf (t; !)= (2 p 4.2. INVERSION OF THE RADON TRANSFORM 147 This computation is described in greater detail in appendix 4.8. Now we do the backprojection step. If (x; y) is inside the unit disc then jh(x; y);!ij \u0014 1: At such points, the inverse of the Radon transform is quite easy to compute: 1 148 CHAPTER 4. THE RADON TRANSFORM This time @t Rg is a H¨older- 1 4.3. THE HILBERT TRANSFORM 149 With this de\fnition we can re-write (4.42) as 4ˇf (x; y)=\u0001 1 150 CHAPTER 4. THE RADON TRANSFORM The Hilbert transform is de\fned by Hf = F −1( ^f (˘)sign(˘)) ) dHf (˘)=sign(˘) ^f (˘): The Fourier transform of a convolution is the product of their Fourier transforms, that is F −1( ^f ^g)= f \u0003 g: Hence, if there existed a nice function h such that ^h(˘)=sign(˘), then the Hilbert transform would be just h\u0003f: Unfortunately the sign(˘) is not the Fourier transform of a nice function because it does not go to zero as j˘j!1. Approximating sign(˘) by a function which decays at 1 gives approximations to the Hilbert transform expressible as convolutions with nice functions. Modify the signum function by setting bh\u000f(˘):=sign(˘)e −\u000fj˘j for \u000f> 0: The inverse Fourier transform of bh\u000f is h\u000f = i 4.3. THE HILBERT TRANSFORM 151 We can multiply this by f (t) and still get zero: \u0012 −\u000fZ −R + RZ \u000f \u0013f (t) ds 152 CHAPTER 4. THE RADON TRANSFORM So we see that in general lim \u000f#0 i 4.3. THE HILBERT TRANSFORM 153 Exercise 4.3.3. Below are linear operators de\fned in terms of the Fourier transform. Re- express these operators in terms of di\u000berentiations and the Hilbert transform. For example, if Af is de\fned by Af (x)= 1 154 CHAPTER 4. THE RADON TRANSFORM 4.3.1 Mapping properties of the Hilbert transform \u0003 4.4. APPROXIMATE INVERSES FOR THE RADON TRANSFORM 155 Using formula (4.37) we get an approximate inverse for the Radon transform f (x; y) ˇ 1 156 CHAPTER 4. THE RADON TRANSFORM Using the convolution theorem for the Fourier transform we see that^Rf \u0003 g(r; !)= fRf (r; !) fRg(r; !): Suppose now that g is a radial function so that Rg is independent of !: The \fltered backprojection formula for f \u0003 g reads f \u0003 g(x; y)= 1 4.5. THE RANGE OF THE RADON TRANSFORM 157 apply. In this special case the integral de\fning k is a convergent, improper integral, which can be computed exactly. We use the formula 1Z 1 sin(xt)dt 158 CHAPTER 4. THE RADON TRANSFORM 4.5.1 Data with bounded support Suppose that f is a function which vanishes outside the disk of radius R: As observed above this implies that Rf (t; !)=0if jtj >R: For a non-negative integer, n consider the integral, Mn(f )(!)= ZZR2 f (x; y)[h(x; y);!i] ndxdy: (4.59) If f has bounded support, then these integrals are well de\fned for any n 2N[f0g: On the other hand, if f does not vanish outside a disk of \fnite radius then, for su\u000eciently large n; these integral may not make sense. Changing coordinates with (x; y)= t! + s^! we can rewrite this integral in terms of Rf , Mn(f )(\u0012)= ZZR2 f (t! + s^!)tndsdt = Z −−1 1 Rf (t; !)tndt: (4.60) The function Mn(f )(!) is called the nth moment of the Radon transform of f: If Rf (t; !) vanishes for jtj >R then this integral is well de\fned for all n: In example 2.4.6 we showed that there are functions, which do not have bounded support, for which the Radon trans- form is de\fned and vanishes for large enough values of t: If f does have bounded support then Mn(f )(!) depends on ! in a very special way. It is useful to express ! as a function of the angle \u0012; !(\u0012) = (cos(\u0012); sin(\u0012)): Using the binomial theorem we obtain h(x; y);!(\u0012)i n =(x cos \u0012 + y sin \u0012) n = nX j=0 \u0012n j \u0013 (x cos \u0012) j(y sin \u0012) n−j = nX j=0 \u0012n j \u0013 cosj \u0012 sinn−j \u0012xjyn−j: Putting the sum into formula (4.59) we see that this integral de\fnes a trigonometric poly- nomial of degree n. Mn(f )(\u0012)= nX j=0 \u0012n j \u0013 cosj \u0012 sin n−j \u0012 ZZR2 f (x; y)xjyn−jdxdy = nX j=0 anj sinj \u0012 cosn−j \u0012 (4.61) 4.5. THE RANGE OF THE RADON TRANSFORM 159 where anj = \u0012n j \u0013 ZZR2 f (x; y)xjyn−jdxdy: If f has bounded support then Mn(f )(\u0012) is a trigonometric polynomial of degree n: We summarize these computations in a proposition. Proposition 4.5.1. Suppose that f is a function with bounded support then (1). Rf (t; !) has bounded support. (2). For all non-negative integers, n there exist constants fan0;::: ;anng such that 1Z −1 Rf (t; !(\u0012))tndt = nX j=0 anj sinj \u0012 cosn−j \u0012: The proposition suggests the following question: Suppose that h(t; !) is a function onR\u0002 S1 such that (1). h(t; !)= h(−t; −!); (2). h(t; !)= 0 if jtj >R; (3). For each non-negative integer n mn(h)(\u0012)= 1Z −1 h(t; !(\u0012))tndt is a trigonometric polynomial of degree n; (4). h(t; !) is a su\u000eciently smooth function of (t; !): Does there exist a function f (x; y) in the domain of the Radon transform vanishing outside of the disk of radius R such that h(t; !)= Rf (t; !)? In other words: does h belong to the range of the Radon transform, acting on smooth func- tions with bounded support? According to a theorem of Helgason and Ludwig, the answer to this question turns out to be yes, however the proof of this result requires techniques well beyond the scope of this text. For a detailed discussion of this question the reader is referred to [50]. More material can be found in [23], [44], [48] or [14]. We model the data measured in CT-imaging as the Radon transform of a piecewise continuous function with bounded support. If we could measure Rf (t; !) for all (t; !)then it would probably not be the exact the Radon transform of such a function. This is because all measurements are corrupted by various sources of error and noise. In particular the patient's movements, both internal (breathing, heart beat, blood circulation, etc.) and external, a\u000bect the measurements. The measured data would therefore be inconsistent and fail to satisfy the moment conditions prescribed above. 160 CHAPTER 4. THE RADON TRANSFORM 4.5.2 More general data \u0003 In this section we show how to use functional analytic methods to extend the domain of the Radon transform and study its range. The material in this section requires elementary measure theory and is included for completeness. It is not used in the latter parts of the book. The problem of characterizing the range of the Radon transform is intimately connected with that of determining its domain. As remarked in section 2.4 the Radon transform is de\fned for any function f such that the restriction of jf j to any line is integrable. A simple su\u000ecient condition is that f is piecewise continuous and satis\fes an estimate of the form f (x; y) \u0014 C 4.5. THE RANGE OF THE RADON TRANSFORM 161 (2). ^f 2 L1(R2 ): Then f has a generalized Radon transform, denoted by F with the following properties (1). Both F and D 1 162 CHAPTER 4. THE RADON TRANSFORM a limit F (t; !); in L2(R\u0002 S1)for <Fρ : ˆ> 0 > which satis\fes the \frst two conditions in the conclusion of the proposition. To establish the last property we assume that f 2 Lp(R2 )for a 1 <p< 2: Let g be a bounded, continuous function with bounded support. For each ˆ> 0 we have the identity hFρ;giR\u0002S1 = h'ρf; R \u0003giR2: (4.66) In exercise 4.2.3 it is shown that there is a constant C so that j R \u0003g(x; y)j\u0014 C 4.5. THE RANGE OF THE RADON TRANSFORM 163 From our hypotheses it follows f 2 L2(R2 )and ^f (r!)= eh(r; !): Indeed this integral is absolutely convergent and therefore f (x; y) is a bounded, continuous function. To \fnish the proof we need to show that f 2 Lp(R2 )for a p< 2: This follows from the weak di\u000berentiability of eh: We identify eh as the polar coordinate representation of a function H(˘1;˘2) de\fned onR2 by H(r cos \u0012; r sin \u0012)= eh(r; !(\u0012)): (4.70) Our assumptions on eh imply that H is in L2(R2 ) and has weak derivatives Hξ1 ;Hξ2 2 L2(R2 )as well. The proof of this statement is left as an exercise. Proposition 3.3.9 implies that there are functions f1;f2 2 L2(R2 )sothat xf (x; y)= f1(x; y)and yf (x; y)= f2(x; y) and therefore jf (x; y)j = g(x; y) 164 CHAPTER 4. THE RADON TRANSFORM 4.6 Continuity of the Radon transform and its inverse In order for the measurement process in X-ray tomography to be stable the map f 7! Rf should be continuous in a reasonable sense. Estimates for the continuity of this map quantify the sensitivity of the output, Rf of a CT-scanner to changes in the input. The less continuous the map, the more sensitive the measurements are to changes in the input. Estimates for the continuity of inverse, h 7! R−1h quantify the e\u000bect of errors in the measured data on the quality of the reconstructed image. Because we actually measure the Radon transform, estimates for the continuity of R−1 are more important for the problem of image reconstruction. To discuss the continuity properties of either transform we need to select norms for functions in the domain and range. Using the L2-norms on both, the Parseval formula, (4.15) provides a starting point for this discussion. The Parseval formula says that if f 2 L2(R2 )then D 1 4.6. CONTINUITY OF THE RADON TRANSFORM AND ITS INVERSE 165 The proposition shows that, if f vanishes outside a bounded set, then we control not only the overall L2-norm of Rf but the L2-norm in each direction, ! separately. Using the support properties of f more carefully gives a weighted estimate on the L2-norm of Rf: Proposition 4.6.2. Let f 2 L2(R2 ) and suppose that f vanishes outside the ball of radius L then, for each !; we have the estimate 1Z −1 j Rf (t; !)j2dt 166 CHAPTER 4. THE RADON TRANSFORM For bounded functions onR\u0002 S1 vanishing for jtj >L a norm is de\fned by khk 2 2;L =sup !2S1 LZ −L jh(t; !)j 2dt + 1 4.6. CONTINUITY OF THE RADON TRANSFORM AND ITS INVERSE 167 is small. This means that we need to control the high frequency content of Rfm; in practice this is not possible. If this could be done, it would only give an estimate for the L2-error. In order for the reconstructed image to \\look like\" the original it may be important to control the pointwise errors. Even though the L2-error is small, the pointwise errors can be large on small sets. While the mathematical problem of estimating the Radon inverse is quite interesting and important, it has little bearing on the problem of practical image reconstruction. A very nice treatment of the mathematical question is given in [50]. We now turn our attention to understanding the continuity of the approximate inverses de\fned in section 4.4. An approximate inverse is denoted by R−1 ; where (t) is a regularizing function speci\fed in terms of its Fourier transform by the conditions ^ (0) = 1; ^ (r)=0 for jrj >W: (4.76) It is also assumed that the radial function k de\fned in 4.57 is in the domain of the Radon transform and Rk = : In this case R−1 Rf = k \u0003 f: (4.77) Example 4.6.1. Let ^ be the piecewise linear function ^ (r)= 8 >< >: 1for jrj <W − C; W −jrj 168 CHAPTER 4. THE RADON TRANSFORM The reconstructed image is f =R−1 Rfm; therefore we need to estimate the di\u000berence f − f : As k \u0003 f =R−1 Rf we can rewrite this di\u000berence as f − f =(f − k \u0003 f )+ R−1 (Rf − Rfm): (4.78) The \frst term on the right hand side is the error caused by using an approximate inverse. It is present even if we have perfect data. Bounds for this term depend in an essential way on the character of the data. If f is assumed to be a continuous function of bounded support then, by taking W very large, the pointwise error, kf − k \u0003 f k1 =sup (x;y)2R2 jf (x; y) − k \u0003 f (x; y)j can be made as small as desired. It is more realistic to model f as a piecewise continuous function. In this case the di\u000berence, jf (x; y) − k \u0003 f (x; y)j can be made small at points where f is continuous. Near points where f has a jump the approximate reconstruction may display an oscillatory artifact. Figure 4.2 shows the reconstruction of ˜D1(x; y)using the regularizing function graphed in \fgure 4.1. Robust estimates for the second term are less dependent on the precise nature of f: 0 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 1.2 1.4 x Figure 4.2: Radial graph of k \u0003 ˜D1; with W =40;C =5: For h(t; !); a function onR\u0002 S1 with bounded support, the approximate inverse is given by (R−1 h)(x; y)= 1 4.6. CONTINUITY OF THE RADON TRANSFORM AND ITS INVERSE 169 Here g = F −1( ^ (r)jrj)and \u0003t indicates convolution in the t-variable. A simple estimate for the sup-norm of R−1 h follows from the sup-norm estimate for a convolution kl \u0003 kkL1 \u0014klkL1 kkkL1 : Applying this estimate gives k R−1 hkL1 \u0014 kg kL1 170 CHAPTER 4. THE RADON TRANSFORM Exercise 4.6.5. Use the central slice theorem to give a formula for k as a Bessel transform of ^ (r): Exercise 4.6.6. Use H¨older's inequality to show that kl \u0003 kkL1 \u0014klkL2kkkL2 : Use this estimate to prove that k R−1 hkL1 \u0014 kg kL2(R) 4.7. THE HIGHER DIMENSIONAL RADON TRANSFORM\u0003 171 Theorem 4.7.1 (Central slice theorem). If f is an absolutely integrable function onRn then fRf (r; !)= 1Z −1 Rf (t; !)e −irtdt = ^f (r!): (4.84) The central slice theorem and the Fourier inversion formula give the Radon inversion formula. Theorem 4.7.2 (The Radon Inversion Formula). Suppose that f is a smooth function with bounded support onRn then f (x)= 1 172 CHAPTER 4. THE RADON TRANSFORM We close our discussion by explaining how the Radon transform can be applied to solve the wave equation. Let ˝ denote the time variable and c the speed of sound. The \\wave equation\" for a function u(x; ˝ ) de\fned onRn \u0002Ris @2 ˝ u = c 2\u0001Rnu: If u satis\fes this equation then it follows from the proposition that, for each ! 2 Sn−1; Ru(t; !; ˝ ) satis\fes the equation @2 ˝ Ru = c 2@2 t Ru: Here Ru(t; !; ˝ ) is the Radon transform of u in the x-variables with ˝ the time parameter. In other words, the Radon transform translates the problem of solving the wave equation in n-dimensions into the problem of solving a family of wave equations in 1-dimension. The one dimensional wave equation is solved by any function of the form v(t; ˝ )= g(ct + ˝ )+ h(ct − ˝ ): The initial data is usually v(t;0) and v˝ (t; 0); it is related to g and h by g(ct)= 1 4.8. THE HILBERT TRANSFORM AND COMPLEX ANALYSIS\u0003 173 Exercise 4.7.3. Prove formula (4.87). Exercise 4.7.4. Prove Proposition (4.7.1) . Hint: Integrate by parts. Exercise 4.7.5. Use the simpli\fed version of the Radon inversion formula available for n = 3 to derive an explicit formula for the solution of the wave equation in 3 space dimensions in terms of the initial data u0(x)and u1(x): 4.8 The Hilbert transform and complex analysis \u0003 In the earlier part of the chapter we used several explicit Hilbert transforms, here we explain how these computations are done. We restrict to the case of square integrable functions. If f 2 L2(R) with Fourier transform f then f (x)= 1 174 CHAPTER 4. THE RADON TRANSFORM (2). lim y#0 1Z −1 jF (x + iy)j 2dx =0 then F \u0011 0: Proof. By Theorem 3.2.9, a function satisfying the L2-boundedness condition has the following property ^F (\u0001 + iy)= ^f (˘)e−yξ where ^f (˘) is the Fourier transform F (x): Moreover ^f (˘)= 0 if ˘< 0: By the Parseval formula 1Z −1 jF (x + iy)j 2dx = 1Z 0 j ^f (˘)j 2e−2yξd˘: The second condition implies that ^f (˘) = 0 and therefore F \u0011 0: 4.8. THE HILBERT TRANSFORM AND COMPLEX ANALYSIS\u0003 175 Exercise 4.8.1. Compute the Hilbert transform of ˜[−1;1](x): A good place to start is with the formula Hf = lim\u000f#0 h\u000f \u0003 f; see section 4.3. 176 CHAPTER 4. THE RADON TRANSFORM Chapter 5 Introduction to Fourier series In engineering applications data is never collected along the whole real line or on the entire plane. Real data can only be collected from a bounded time interval or planar domain. In order to use Fourier analysis to analyze and \flter this type of data we can either \u000f Extend the data by cutting it o\u000b to equal zero outside of the set over which the data was collected, or \u000f Extend the function periodically. If the data is extended \\by zero\" then the Fourier transform is available. If the data is extended periodically then it does not vanish at in\fnity, in any sense, and hence its Fourier transform is not a function. Fourier series provides an alternate tool for the analysis of functions de\fned on \fnite intervals inRor products of intervals inRn : The goal of Fourier series is to express an \\arbitrary\" periodic function as a linear combination of complex exponentials. This theory runs parallel to that of the Fourier transform presented in Chapter 3. After running through the basic results in the one dimensional case we give a detailed analysis of the Gibbs phenomenon. The chapter concludes with a rapid presentation of Fourier series inRn : 5.1 Fourier series in one dimension 178 CHAPTER 5. INTRODUCTION TO FOURIER SERIES The set of such functions is a complete normed, linear space with norm de\fned by k\u0001 kL1: It is denoted by L1([0; 1]): De\fne the Fourier coe\u000ecients of f 2 L1([0; 1]) by ^f (n)= 1Z 0 f (x)e −2ˇinxdx for n 2Z: (5.1) Example 5.1.1. If f (x)= cos(2ˇmx) then, using the formula cos(y)=2−1(eiy + e−iy)we easily compute that ^f (n)= ( 1 5.1. FOURIER SERIES IN ONE DIMENSION 179 In applications one works with a \fxed partial sum, so it is very important to understand in what sense SN (f ; x) is an approximation to f (x): The best one might hope for is that lim N !1 SN (f ; x)= f (x) at every point x: At discontinuities of f such a statement seems very unlikely to be true. In fact, it can even fail at points where f is continuous. The pointwise convergence of Fourier series is a very subtle problem. For the simplest result we make a strong hypothesis about the rate of decay of the Fourier coe\u000ecients. Proposition 5.1.2 (Fourier inversion formula). If f is a continuous function de\fned on [0; 1] such that the Fourier coe\u000ecients of f satisfy 1X n=−1 j ^f (n)j < 1 (5.5) then f (x) is represented, at every point by its uniformly convergent Fourier series f (x)= 1X n=−1 ^f (n)e 2ˇinx; for all x 2 [0; 1]: (5.6) Proof. For 0 <r < 1; de\fne the absolutely convergent series Pr(x)= 1X n=−1 rjnje2πinx =1 + 2 Re \" 1X n=1 rne2πinx# : Using the second expression and the formula for the sum of a geometric series we see that Pr(x)= 1 − r2 180 CHAPTER 5. INTRODUCTION TO FOURIER SERIES For each r 1Z 0 Pr(x)dx =1: This fact and the positivity of Pr imply that jfr(x) − f (x)j = \f \f \f \f \f \f 1Z 0 Pr(x − y)(f (y) − f (x))dy \f \f \f \f \f \f \u0014 1Z 0 Pr(x − y)jf (y) − f (x)jdy: (5.7) If x 6=0 then lim r\"1 Pr(x)=0: In fact if \u000f> 0 is \fxed then there is a 0 <\u000e so that Pr(x) <\u000f if r> 1 − \u000e and \u000f< x< 1 − \u000f: (5.8) To show that the di\u000berence f (x) − fr(x) becomes small we break the integral into two pieces. One piece is small because f is continuous; the other is small because f is bounded and Pr(x)is small, if x is far enough from 0: Since f is continuous, given \u0011> 0 there is an \u000f> 0sothat jx − yj <\u000f implies that jf (x) − f (y)j <\u0011: There is also an M so that jf (y)j\u0014 M for all y: Let \u000f0 =minf\u000f; \u0011g: Using (5.8), with \u000f0; there is a \u000e> 0sothat r> 1 − \u000e implies that jfr(x) − f (x)j\u0014 1Z 0 Pr(x − y)jf (y) − f (x)jdy = Z jx−yj<ϵ0 Pr(x − y)jf (y) − f (x)jdy + Z jx−yj>ϵ0 Pr(x − y)jf (y) − f (x)jdy \u0014 \u0011 Z jx−yj<ϵ0 Pr(x − y)dy +2M\u000f0 \u0014 (1 + 2M )\u0011: (5.9) This estimate shows that lim r\"1 fr(x)= f (x) and thereby completes the proof of the Theorem. 5.1. FOURIER SERIES IN ONE DIMENSION 181 As was the case with the Fourier integral, the Fourier coe\u000ecients of an absolutely integrable function f may fail to satisfy (5.5). For example, let f (x)= ( 1if x 2 [0; 1 182 CHAPTER 5. INTRODUCTION TO FOURIER SERIES Let f be an arbitrary integrable function. Fix an \u000f> 0; according to Theorem A.6.2 there is a step function g so that kf − gkL1 <\u000f: We need to compare the Fourier coe\u000ecients of f and g : j ^f (n) − ^g(n)j = \f \f \f \f \f \f 1Z 0 (f (x) − g(x))e−2πinxdx \f \f \f \f \f \f \u0014 1Z 0 jf (x) − g(x)jdx \u0014 \u000f: (5.10) The triangle inequality shows that j ^f (n)j\u0014 j ^f (n) − ^g(n)j + j^g(n)j: Taking the lim supn!1 in this estimate we see that lim sup n!1 j ^f (n)j\u0014 \u000f: Since \u000f is an arbitrary positive number this shows that lim n!1 j ^f (n)j =0: 5.2. DECAY OF FOURIER COEFFICIENTS 183 Exercise 5.1.5. Show that Pr(x) > 0 and has total integral 1 for any r: Exercise 5.1.6. Show that if \u000f> 0is\fxedthenthere is a0 <\u000e so that Pr(x) <\u000f if r> 1 − \u000e and \u000f<x< 1 − \u000f: (5.12) Exercise 5.1.7. Explain why (5.11) is a \\continuity\" result for the map f 7!< ^f (n) >: Exercise 5.1.8. Use summation by parts twice to show that f (x)= 1X n=2 cos(2ˇnx) 184 CHAPTER 5. INTRODUCTION TO FOURIER SERIES Example 5.2.1. The function f (x)= x is a continuous function on [0; 1] however its 1- periodic extension is not, see the graphs. 0 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1x (a) f: 0.2 0.4 0.6 0.8 1 –2 –1 1 2 x (b) The 1-periodic extension of f: Figure 5.1: Periodic extension may turn a continuous function into discontinuous function. Suppose that f is continuously di\u000berentiable on [0; 1]; for the moment we do not assume that f (0) = f (1): Integrating by parts gives ^f (n)= 1Z 0 f (x)e −2ˇinxdx = 1 5.2. DECAY OF FOURIER COEFFICIENTS 185 Theorem 5.2.1. If f 2 C k([0; 1]) and f (0) = f (1), f 0(0) = f 0(1);::: ;f (k−1)(0) = f (k−1)(1), then ^f (n)= df (k)(n) 186 CHAPTER 5. INTRODUCTION TO FOURIER SERIES Example 5.2.3. If we set f (x)= x − n for x 2 (n; n +1] then f does not have any periodic derivatives. The Fourier coe\u000ecients of f are given by ^f (n)= ( 1 5.3. L2-THEORY 187 The norm on L2([0; 1]) is de\fned by the inner product hf; giL2 = Z f (x) 188 CHAPTER 5. INTRODUCTION TO FOURIER SERIES \u000f Whenever f; g 2 S then f + g 2 S as well. \u000f If f 2 S and a 2Cthen af 2 S: If S ˆ L2([0; 1]) is a subspace then we say that f is orthogonal to S if hf; giL2 =0 for every g 2 S: De\fnition 5.3.1. Let S be a subspace of L2([0; 1]): The orthogonal complement of S; denoted S? is the subspace of L2([0; 1]) consisting of all functions g 2 L2([0; 1]) orthogonal to S: Connected to a subspace we have orthogonal projections. De\fnition 5.3.2. Let S be a subspace of L2([0; 1]): The orthogonal projection onto S is a linear map PS : L2([0; 1]) ! L2([0; 1]) with the following properties: (1). P 2 S = PS; (a linear map with this property is called a projection), (2). If f 2 S then PS(f )= f; (3). If f 2 S? then PS(f )= 0: It is a basic result in functional analysis that such an orthogonal projection always exists, see [16]. If f is a square summable function on [0; 1] then it is also absolutely integrable, hence its Fourier coe\u000ecients are de\fned. In this case the Fourier coe\u000ecients of f go to zero su\u000eciently fast to make P j ^f (n)j2 converge. Once again we have a Parseval formula. Theorem 5.3.2 (Parseval Formula). If f 2 L2([0; 1]) then 1Z 0 jf (x)j 2dx = 1X n=−1 j ^f (n)j 2: Proof. Again the theorem is simple to prove for a special class of L2-functions, in this instance, the trigonometric polynomials, T = f NX j=−N cje2πijx : cj 2Cg: If f 2T then multiplying out the \fnite sum de\fning f \u0016f gives jf (x)j 2 = NX j,k=−N cj \u0016cke2πi(j−k)x: (5.21) Integrating both sides of (5.21), using (5.20) gives 1Z 0 jf (x)j 2dx = NX j=−N jcjj 2: This is the Parseval formula for f 2T : To complete the proof we need two additional facts. The \frst is that an arbitrary f 2 L2([0; 1]) is well approximated by trigonometric polynomials. 5.3. L2-THEORY 189 Lemma 5.3.1. If f 2 L2([0; 1]) and \u000f> 0 is given then there is a g 2T so that kf − gkL2 <\u000f: The second is Bessel's inequality. It states that among functions of the form gN = NX n=−N cne2πinx the N th-partial sum of the Fourier series of f minimizes the error, kf − gN kL2: The lemma is proved in section 5.5.2 and Bessel's inequality is proved in section 5.3.2. The de\fnition of the Fourier coe\u000ecients implies that 0 \u0014kf − SN (f )k2 L2 = kf k2 L2 −kSN (f )k2 L2 and therefore kSN (f )k2 L2 \u0014kf k2 L2: In particular, using the result for trigonometric polynomials and letting N tend to in\fnity we deduce that 1X n=−1 j ^f (n)j 2 \u0014 1Z 0 jf (x)j 2dx: (5.22) On the other hand the triangle inequality gives the estimate kf kL2 \u0014kf − SN (f )kL2 + kSN (f )k2 L2: Bessel's inequality, Lemma 5.3.1 and the result for trigonometric polynomials now shows that, for any \u000f> 0 1Z 0 jf (x)j 2dx \u0014 1X n=−1 j ^f (n)j 2 + \u000f: Together these inequalities complete the proof of Parseval's formula. 190 CHAPTER 5. INTRODUCTION TO FOURIER SERIES Theorem 5.3.3. The set of exponentials fe2ˇinx jn = −1;::: ; 1g is an orthonormal basis for L2([0; 1]). Remark 5.3.1. The Parseval formula should therefore be regarded as an in\fnite dimensional version of Pythagoras' theorem. Parseval's formula also implies that the Fourier series of an L2-function converges to the function in the L2-norm. Proposition 5.3.1. If f 2 L2([0; 1]) then lim M;N !1 kf − NX j=−M ^f (j)e 2ˇijxkL2 =0: (5.24) Remark 5.3.2. As before it is said that the Fourier series of f converges to f in the mean, this is denoted LIM M;N !1 NX j=−M ^f (j)e 2ˇijx = f (x): Proof. Given the Parseval formula, the proof is a simple computation using the fact that the L2- norm is de\fned by an inner product: kf − NX j=−M ^f (j)e2πijxk2 L2 = kf k2 L2 − NX j=−M j ^f (j)j 2: (5.25) From the Parseval formula it follows that kf k2 L2 − NX j=M j ^f (j)j 2 = −(M+1)X j=−1 j ^f (j)j 2 + 1X j=N +1 j ^f (j)j 2: (5.26) As the sum P j ^f (j)j 2 is \fnite the right hand side in (5.26) tends to zero as M and N tend to in\fnity. 5.3. L2-THEORY 191 these are the Fourier coe\u000ecients of an L2-function. However 1X n=1 n− 3 192 CHAPTER 5. INTRODUCTION TO FOURIER SERIES Given a function f with l(f ) < 1 \fnd the function gf 2TN such that dl(f; gf )=minfdl(f; g) j g 2TN g: That is \fnd the point in TN whose dl-distance to f is as small as possible. The minimum value dl(f; gf ) is called the error in the approximation. The ease with which such a problem is solved depends largely on the choice of l: For most choices of norm this problem is very di\u000ecult to solve, indeed is not solvable in practice. One usually has to settle for \fnding a sequence of approximants <gN > for which the errors <dl(f; gN ) > go to zero at essentially the same rate as the optimal error. The sole exception is L2: The following theorem gives the answer if the error is measured in the L2-norm. Theorem 5.3.4 (Bessel's inequality). Given a function f 2 L2([0; 1]) and constants fa−N ;::: ;aN g the following inequality holds kf − NX n=−N ^f (n)e 2ˇinxk2 \u0014kf − NX n=−N ane 2ˇinxk2 with equality if and only if an = ^f (n) for all n 2f−N;::: ;N g. Proof. Using the following relation, kf + gk 2 2 = hf + g; f + giL2 = kf k 2 2 +2 Rehf; giL2 + kgk 2 2: we have kf − NX n=−N ane 2ˇinxk 2 2 −kf − NX n=−N ^f (n)e 2ˇinxk 2 2 = k NX n=−N(an − ^f (n))e 2ˇinxk 2 2 \u0015 0: The equality holds if and only if an = ^f (n)for −N \u0014 n \u0014 N: 5.3. L2-THEORY 193 5.3.3 L 2-derivatives \u0003 194 CHAPTER 5. INTRODUCTION TO FOURIER SERIES Theorem 5.3.5. A function f 2 L2([0; 1]) has kL2-derivatives if and only if 1X n=−1 (1 + jnj) 2kj ^f (n)j 2 < 1: (5.29) In this case, we have 1Z 0 jf [j](x)j 2dx = 1X n=−1 j2ˇnj 2jj ^f (n)j 2 < 1 and cf [j](n)=[2ˇin] j ^f (n) for j =1;::: ;k: (5.30) Sketch of proof. If (5.29) holds, then Parseval's formula implies that the sequences, < [2ˇin] j ^f (n) >; j =0;:::;k are the Fourier coe\u000ecients of the functions f0;f1;:::;fk in L2([0; 1]): Integrating, formally it is not di\u000ecult to show that these functions are the L2-derivatives of f: On the other hand, if f has k L2-derivatives then, using the alternate de\fnition given in exercise 5.3.8, and test functions de\fned by trigonometric polynomials we deduce that df [j](n)= [2ˇin] j ^f (n)for j =1;::: ;k: (5.31) The estimate (5.29) is a consequence of these formulˆ and Parseval's formula. 5.3. L2-THEORY 195 In (5.4) we de\fned the partial sums, SN (f ) of the Fourier series of a function f: If f has an L2-derivative then it follows from Theorem 5.3.5 and the Cauchy-Schwarz inequality that 1X n6=0 j ^f (n)j\u0014 v u u t 196 CHAPTER 5. INTRODUCTION TO FOURIER SERIES This estimate for the error is an application of (5.19). For a \fxed resolution, we see that, as N !1; the measured di\u000berence between f and SN (f ) goes to zero as N !1: Exercise 5.3.7. Show that the hypothesis, in Proposition 5.1.2, that f is continuous is unnecessary by using the observation that < ^f (n) > is a square summable sequence and therefore f 2 L2([0; 1]): The conclusion needs to be modi\fed to say that f can be modi\fed on a set of measure zero so that (5.6) holds. Exercise 5.3.8. We can give a di\u000berent de\fnition using the integration by parts formula. To wit: a function f de\fned on [0; 1] has an L2-derivative provided that there is a function f1 2 L2([0; 1]) so that 1Z 0 f (x)' 0(x)dx = − 1Z 0 f1(x)'(x)dx for every 1-periodic, once di\u000berentiable function ': Show that this de\fnition is equivalent to the one above. Exercise 5.3.9. Suppose we use the condition in the previous exercise to de\fne L2- derivative but without requiring the test functions ' be 1-periodic. Show that we do not get the same class of functions. What boundary condition must a function satisfy to be di\u000berentiable in this sense? Exercise 5.3.10. Provide the details for the derivations the formulˆ (5.31). Exercise 5.3.11. Given any function g 2 L2([0; 1]); de\fne a measurement by setting lg(f )= hf; giL2 : Show that for any f 2 L2([0; 1]) lim N !1 lg(f − SN (f )) = 0: 5.4 General periodic functions Up to this point we have only considered functions of period 1. Everything can easily be generalized to functions with arbitrary periods. A function, de\fned on the real line is periodic of period L; or L-periodic if f (x + L)= f (x) An L-periodic function is determined by its values on any interval of length L: For an integrable function of period L, de\fne the Fourier coe\u000ecients by ^f (n)= LZ 0 f (x)e − 2πinx 5.4. GENERAL PERIODIC FUNCTIONS 197 1. Inversion Formula: If f is continuous and P1 n=−1 j ^f (n)j < 1 then f (x)= 1 198 CHAPTER 5. INTRODUCTION TO FOURIER SERIES There is also a notion of convolution for sequences. De\fnition 5.4.1. Let A =<an > and B =<bn > be square summable, bi-in\fnite sequences. The convolution of A with B is the sequence de\fned by (A? B)n = 1X j=−1 ajbn−j: H¨older's inequality for l2 implies that A? B is a bounded sequence. This de\fnition is motivated by the result of multiplying trigonometric polynomials, if f = NX j=−N aje 2ˇijx and g = NX j=−N bje 2ˇijx; then f \u0001 g = 2NX l=−2N 2 4 X maxf−N −l;−N g\u0014j\u0014minfN;N +lg ajbl−j 3 5 e 2ˇilx: (5.36) If f and g are square integrable then fg is integrable. Using the notion of convolution of sequences we get obtain a formula for the Fourier coe\u000ecients of the pointwise product fg: Proposition 5.4.2. If f; g are in L2([0;L]) then the Fourier coe\u000ecients of fg are given by cfg(n)= 1 5.4. GENERAL PERIODIC FUNCTIONS 199 and therefore the triangle inequality and another application of the Cauchy-Schwarz inequality give k(f − SN (f ))g + SN (f )(g − SN (g))kL1 \u0014k(f − SN (f ))gkL1 + kSN (f )(g − SN (g))kL1 \u0014k(f − SN (f ))kL2kgkL2 + kSN (f )kL2k(g − SN (g))kL2: (5.40) Which shows that SN (f )SN (g) converges to fg in L1: The proof is completed by using Proposi- tion 5.1.1 to verify (5.39). 200 CHAPTER 5. INTRODUCTION TO FOURIER SERIES It is clear from the de\fnition that bDN = ^dN ; Theorem 5.4.1 shows that for f 2 L1([0;L]) SN (f ; x)= f \u0003 DN (x): (5.42) The zeroth Fourier coe\u000ecient of DN is 1; that is LZ 0 DN (x)dx =1: The Dirichlet kernel is oscillatory and assumes both positive and negative values. It is not di\u000ecult to show that lim N !1 LZ 0 jDN (x)jdx = 1: (5.43) This fact underlies the di\u000eculties in analyzing the pointwise convergence of the partial sums of the Fourier series. Even if f is a continuous function it is not always true that lim N !1 SN (f ; x)= f (x): In the next several sections we explore some of these issues in detail. First we consider what happens to the partial sums of the Fourier series near to a jump discontinuity. Then we \fnd a replacement for the partial sums which has better pointwise convergence properties. Exercise 5.4.7. Prove formula (5.41) by using the formula for the sum of a geometric series. Exercise 5.4.8. Use the explicit formula for DN (x) to prove (5.43). Hint: Compare LZ 0 jDN (x)jdx to the harmonic series. 5.5 The Gibbs Phenomenon 5.5. THE GIBBS PHENOMENON 201 Example 5.5.1. Consider the 2ˇ-periodic function g(x)= ( ˇ−x 202 CHAPTER 5. INTRODUCTION TO FOURIER SERIES We are looking for the maximum of the di\u000berence SN (g; x)−g(x): From elementary calculus we know that, at a point where the maximum occurs, d 5.5. THE GIBBS PHENOMENON 203 lim N !1 SN (g; ˇ 204 CHAPTER 5. INTRODUCTION TO FOURIER SERIES 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 0.2 0.4 0.6 0.8 1 1.2 1.4 x (a) N=24, magni\f- cation=4 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 x (b) N=48, magni\f- cation=8 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 x (c) N=96, magni\f- cation=16 Figure 5.4: Detail showing equi-oscillation property in Gibbs phenomenon 5.5.1 The general Gibbs phenomenon Now suppose that f is piecewise di\u000berentiable function with a jump discontinuity at the point x0: This means that the left and right limits lim x!x− 0 f (x)= L and lim x!x+ 0 f (x)= R; both exist but L 6= R: Suppose that L< R; \fxing any su\u000eciently small \u000f> 0weshow below that lim N !1 max 0<x−x0<\u000f(SN (f ; x) − f (x)) = (G − 1) R − L 5.5. THE GIBBS PHENOMENON 205 For any su\u000eciently small \u000f> 0 we have that lim N !1 max 0<\u0011j (x−xj )<\u000f(SN (f ; x) − f (x)) = (G − 1) jhj j 206 CHAPTER 5. INTRODUCTION TO FOURIER SERIES Assuming this, use partial summation to show that there is a constant M; which depends on x so that jSN (g; x) − g(x)j\u0014 M 5.5. THE GIBBS PHENOMENON 207 5 10 15 20 25 30 35 –0.6 –0.4 –0.2 0.2 0.4 0.6 x Figure 5.5: Graph of the Fejer kernel, F5(x) Theorem 5.5.2 (Fejer's Theorem). If f is an absolutely integrable function which is continuous at x then lim N !1 CN (f ; x)= f (x): Remark 5.5.2. As remarked above, the analogous statement for the partial sums is false. Proof. The proof is very similar to the proof of the Fourier inversion formula. Note that the Fejer kernel shares three properties with Pr(x): (1). The Fejer kernel is non-negative. (2). For every N; R 1 0 FN (x)dx =1: (3). Given \u000f> 0 there is an M so that if N> M then FN (x) <\u000f for \u000f< x< 1 − \u000f: (5.55) The proof of Fejer's Theorem follows from these properties. The \frst two properties imply that jCN (f ; x) − f (x)j = \f \f \f \f Z 1 0 FN (x − y)(f (y) − f (x))dy\f \f \f \f \u0014 Z 1 0 FN (x − y)jf (y) − f (x)jdy: (5.56) As f is continuous at x; given \u000f> 0 there is a \u000e> 0sothat jy − xj <\u000e )jf (x) − f (y)j <\u000f: Using the third property of the Fejer kernel, there is an M so that if N> M then FN (x) <\u000f provided \u000e< x < 1 − \u000e: We split the integral into two parts: jCN (f ; x) − f (x)j\u0014 Z jx−yj<δ FN (x − y)jf (y) − f (x)jdy + Z jx−yj\u0015δ FN (x − y)jf (y) − f (x)jdy \u0014 \u000f Z jx−yj<δ FN (x − y)dy + Z jx−yj\u0015δ \u000f(jf (y)j + jf (x)j)dy \u0014 \u000f(1 + jf (x)j + kf kL1): (5.57) 208 CHAPTER 5. INTRODUCTION TO FOURIER SERIES As \u000f> 0 is arbitrary, this completes the proof of the theorem. 5.5. THE GIBBS PHENOMENON 209210 CHAPTER 5. INTRODUCTION TO FOURIER SERIES –1.2 –1 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 200400600800 1200 1600 2000 pp (a) A function –1.2 –1 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 20 40 60 80100 140 180 220 qq (b) Partial sum –1 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 1.2 20 40 60 80100 140 180 220 qq (c) Fejer mean Figure 5.6: Graphs comparing the partial sums and Fejer means. The graphs in \fgure 5.7 are expanded views of these functions between :1and :3: Here the loss of resolution in the Fejer means, at points away from the jump is quite evident. –1.2 –1 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 300 400 500 600 pp (a) A function –1.2 –1 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 30 40 50 60 70 qq (b) Partial sum –1 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 1.2 30 40 50 60 70 qq (c) Fejer mean Figure 5.7: Expanded view showing the loss of resolution in the Fejer means. Finally we compare the behavior of these approaches near the jump discontinuity. Both the Gibbs ringing and higher resolution are again quite evident in the partial sums. 5.6. THE LOCALIZATION PRINCIPLE\u0003 211 –1.2 –1 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 920940960980 1020 1060 1100pp (a) The jump –1.2 –1 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 114 118 122 126 130 134 138 qq (b) Partial sum –1 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 1.2 114 118 122 126 130 134 138 qq (c) Fejer mean Figure 5.8: Expanded view showing Gibbs phenomenon in the partial sums. Exercise 5.5.8. Derive (5.59) and (5.60). 5.6 The localization principle \u0003 Each Fourier coe\u000ecient is de\fned by an integral of f over its whole domain of de\fnition and therefore depends on the value of the function everywhere. Like the Fourier transform, the Fourier series is very sensitive to the local behavior of a function. This is the content of the following theorem. Theorem 5.6.1 (The localization principle). Let f and g be L-periodic and absolutely integrable over [0;L]: Suppose that for some x; SN (g; x) converges to g(x) as N !1: If f (t)= g(t) in an interval [x − \u000f; x + \u000f] for an \u000f> 0 then lim N !1 SN (f ; x)= f (x) as well. Proof. In the computations below recall that x is a \fxed point. Let sN (x)= DN \u0003 f (x); and tN (x)= DN \u0003 g(x): By linearity, (sN (x) − tN (x)) = DN \u0003 (f − g)(x); and DN \u0003 (f − g)(x) DN \u0003 (f − g)(x)= LZ 0 sin \u0012 ˇ(2N +1)(x − y) 212 CHAPTER 5. INTRODUCTION TO FOURIER SERIES Therefore, the last integral is the N th Fourier coe\u000ecient of an integrable function. By the Riemann- Lebesgue lemma, it goes to zero as N !1. By the hypothesis, we know that limN !1 tN (x)= g(x)= f (x). Rewrite this as lim N !1 sN (x) = lim N !1 (sN (x) − tN (x)) + lim N !1 tN (x): Since sN (x) − tN (x) goes to zero as N !1; we are done. 5.7. HIGHER DIMENSIONAL FOURIER SERIES 213 5.7 Higher dimensional Fourier series The theory of Fourier series extends without di\u000eculty to functions de\fned on the unit cube inRn : For completeness we include statements of the basic results. Proofs can be found in [16] or [72]. The Fourier coe\u000ecients are now labeled by vectors k 2Zn; that is vectors of the form k =(k1;::: ;kn)where kj 2Zfor j =1;::: ;n: If f (x1;::: ;xn) is an absolutely integrable function de\fned on [0; 1] n =[0; 1] \u0002 \u0001\u0001\u0001 \u0002 [0; 1] n−times then its Fourier coe\u000ecients are de\fned by ^f (k)= ZRn f (x)e −2ˇihk;xidx: Many aspects of the theory are quite similar in higher dimensions, however the theory of pointwise convergence is much more involved and has not yet been completely worked out. If the Fourier coe\u000ecients of f tend to zero rapidly enough then we have an inversion formula: Proposition 5.7.1. Suppose that f is an absolutely integrable function on [0; 1]n such that X k2Zn j ^f (k)j < 1 (5.61) then f (x)= X k2Zn ^f (k)e 2ˇihk;xi: In general the Fourier coe\u000ecients of an absolutely integrable function may not sat- isfy (5.61). Indeed as the dimension increases this gets to be a more and more restrictive condition. In order for the in\fnite sum X k2Zn 1 214 CHAPTER 5. INTRODUCTION TO FOURIER SERIES Once again the proof is by approximating L1-functions by the n-dimensional analogue of step functions. In this generality there is, as before, no estimate on the rate at which the Fourier coe\u000ecients go to zero. As in the one dimensional case, when working with Fourier series we need to consider f as a periodic function of period 1 in each variable. That is we extend f to all ofRn by using the condition f (x)= f (x + k) for every k 2Zn: The inversion formula de\fnes a function on all ofRn with this property. As before, in the context of Fourier series, a function is considered continuous if its periodic extension toRn is continuous and di\u000berentiable if its periodic extension toRn is di\u000berentiable, etc. If the Fourier coe\u000ecients do not satisfy (5.61) then the problem of summing the Fourier series can be quite subtle. The question of the pointwise convergence for the partial sums is considerably more complicated in higher dimensions than in one dimension. In the one dimensional case there is, in essence only one reasonable way to de\fne partial sums. In n-dimensions there are many di\u000berent possible choices. The simplest way is to de\fne the N th-partial sum to be SN (f ; x)= NX k1=−N \u0001\u0001\u0001 NX kn=−N ^f (k)e 2ˇihk;xi: Because there is a very fast algorithm to do this calculation (at least for N a power of 2) this is the usual meaning of \\partial sums\" of the Fourier series in applications. However it is by no means the only way to partially invert the higher dimensional Fourier series. We could equally well consider the sum over all vectors k such that kkk\u0014 R: Let \u0006R(f ; x)= X fk : kkk<Rg ^f (k)e 2ˇihk;xi; denote this sum. While not as useful in applications, this form of the partial inverse is easier to analyze. From this analysis, it is known that the localization principle fails in higher dimensions. The convergence of the Fourier series at x is sensitive to the behavior of f at points distant from x: The relationship between SN (f ; x)and \u0006R(f ; x) has, so far, not been completely elucidated. An analysis of \u0006R(f ) is given in [57]. The Gibbs phenomenon also persists in higher dimensions but is, as expected more complicated to analyze. If a piecewise smooth function f has a simple jump along a smooth hypersurface S then the behavior of the partial sums near x 2 S is determined in part by thesizeof the jump at x as well as the curvature of S at x: Asymptotic formulˆ for \u0006R(f ; x) are given in [57]. If S itself is not smooth then even more complicated phenomena arise. As the techniques involved are far beyond the scope of this text we content ourselves with giving as an example a partial sum (of SN -type) for the Fourier series of ˜[−1;1](x)˜[−1;1](y): Note the Gibbs oscillations parallel to the edges of the square and the \\Gibbs shadow\" near the corner. 5.7. HIGHER DIMENSIONAL FOURIER SERIES 215 Figure 5.9: Illustration of the 2d-Gibbs phenomenon There is an obvious generalization of the notion convolution for periodic functions onRn given by f \u0003 g(x)= Z [0;1]n f (x − y)g(y)dy: It is connected to the Fourier series just as in one dimension:[f \u0003 g(k)= ^f (k)^g(k) for all k 2Zn: (5.62) We can also de\fne the convolution of two sequences A =<ak >; B =<bk > indexed byZn by setting (A? B)k = X j2Zn ak−jbj: The Fourier series of a pointwise product is then given by cfg(k)= ^f? ^g(k): (5.63) Exercise 5.7.1. Let r1;::: ;rn be numbers between 0 and 1: Compute the Fourier coe\u000e- cients of f (x1;::: ;xn)= ˜[−r1;r1](x1) \u0001\u0001\u0001 ˜[−rn;rn](xn): Exercise 5.7.2. Show that X k2Zn 1 216 CHAPTER 5. INTRODUCTION TO FOURIER SERIES 5.7.1 L 2-theory 5.7. HIGHER DIMENSIONAL FOURIER SERIES 217 Proposition 5.7.5. A function f 2 L2([0; 1]n) has mL2-derivatives if and only if X k2Zn kkk 2mj ^f (k)j 2 < 1: In this case ZRn j@\u000b\u000b\u000b x f (x)j 2dx = X k2Zn jk \u000b ^f (k)j 2 and d@\u000b x f (k)=(ik) \u000b\u000b\u000b ^f (k); for every multi-index \u000b\u000b\u000b with j\u000b\u000b\u000bj\u0014 m: As noted above, a faster rate of decay is needed in higher dimensions to be able to conclude that the Fourier coe\u000ecients are absolutely summable. In one dimension we showed that a function with one L2-derivative is continuous. In dimension n; slightly more that (n=2) derivatives are required for this conclusion. Functions that are de\fned on products of intervals [a1;b1] \u0002 \u0001\u0001\u0001 \u0002 [an;bn] can be rescaled to be de\fned on [0; 1]n and can therefore be expanded in Fourier series as well. We leave the details of this discussion to the interested reader. While intervals are the only connected subsets of the real line, higher dimensional spaces have a rich array of such subsets. The Fourier series in higher dimensions is, of course only de\fned for functions that are de\fned in products of intervals. The analysis of functions de\fned in other sorts of regions requires more sophisticated mathematical techniques. For example we cannot directly apply Fourier series to study functions de\fned in the unit disk. The interested reader is referred to [17]. Exercise 5.7.5. Prove (5.62) and (5.63) Exercise 5.7.6. Let [n=2] be the largest integer smaller than n=2: Show that a periodic function with [n=2] + 1 L2-derivatives is a continuous function. Hint: Use the Cauchy- Schwarz inequality. 218 CHAPTER 5. INTRODUCTION TO FOURIER SERIES Chapter 6 Poisson summation, Sampling and Nyquist’s theorem 220 CHAPTER 6. SAMPLING 6.1 Sampling and Nyquist’s theorem 6.1. SAMPLING AND NYQUIST’S THEOREM 221 see exercise 3.2.11. If we think of ^f (˘) as a function de\fned on the interval [−L; L] then it follows from (6.1) that the numbers f2ˇf ( πn 222 CHAPTER 6. SAMPLING The exponentials e\u0006iLx have period 2ˇ 6.1. SAMPLING AND NYQUIST’S THEOREM 223 From (6.2) it follows that ^f (˘)= \u0010 ˇ 224 CHAPTER 6. SAMPLING 0 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 1.2 1.4x (a) Window functions in Fourier space. –0.2 0 0.2 0.4 0.6 0.8 1 20 40 60 80 100x (b) Ordinary sinc-function. Figure 6.1: Window functions in Fourier space and the ordinary sinc-pulse. –0.2 0 0.2 0.4 0.6 0.8 20 40 60 80 100x (a) %10-oversampling. –0.2 0 0.2 0.4 0.6 0.8 20 40 60 80 100x (b) %25-oversampling. Figure 6.2: Shannon-Whittaker interpolation functions with second order smoothed win- dows. Exercise 6.1.1. Use the Shannon-Whittaker formula to reconstruct the function f (x)= sin(Lx) 6.2. THE POISSON SUMMATION FORMULA 225 Exercise 6.1.3. The Fourier transform of f (x)= 1 − cos(x) 226 CHAPTER 6. SAMPLING Note that f ^fp(n)g are the Fourier coe\u000ecients of the 1-periodic function fp(x)whereas ^f (˘) is the Fourier transform of the absolutely integrable function f (x) de\fnedonall ofR: To justify these computations it is necessary to assume that the coe\u000ecients ^f (2ˇn) gotozero su\u000eciently rapidly. If f is a su\u000eciently smooth function then this will be true. The Poisson summation formula is a precise formulation of these observations. Theorem 6.2.1 (Poisson summation formula). If f (x) is an absolutely integrable func- tion such that 1X n=−1 j ^f (2ˇn)j < 1 then,atpoints ofcontinuity of fp(x); we have 1X n=−1 f (x + n)= 1X n=−1 ^f (2ˇn)e 2ˇinx: (6.9) Remark 6.2.1. The hypotheses in the theorem are not quite optimal. Some hypotheses are required as there examples of absolutely integrable functions f such that both 1X n=−1 jf (x + n)j and 1X n=−1 j ^f (2ˇn)j converge but (6.9) does not hold. A more detailed discussion can be found in [40]. Using the argument above and rescaling one easily \fnds an Poisson summation formula for 2L-periodic functions: 1X n=−1 f (x +2nL)= 1 6.2. THE POISSON SUMMATION FORMULA 227 f ^f (n\u0001˘)g: On the other hand the function is known, apriori to be supported in a \fxed bounded set [−L; L]: In order to reconstruct f exactly we need to take \u0001˘ \u0014 ˇ 228 CHAPTER 6. SAMPLING Exercise 6.2.1. Explain formula (6.11). Exercise 6.2.2. \u0003 This exercise requires a knowledge of the Fourier transform for gener- alized functions, see section 3.2.13. Suppose that f is a periodic function of period 1: The generalized function lf has a Fourier transform which is a generalized function. Using the dual Poisson summation formula, show that blf =2ˇ 1X n=−1 ^f (n)\u000e(2ˇn − ˘); (6.13) here f ^f (n)g are the Fourier coe\u000ecients de\fned in (5.1). Exercise 6.2.3. \u0003 What is the analogue of formula (6.13) for a 2L-periodic function? 6.2.2 Undersampling and aliasing Using the Poisson summation formula we analyze the errors introduced by undersampling. Whether or not f is an L-bandlimited function, the samples ff ( nˇ 6.2. THE POISSON SUMMATION FORMULA 229 latter type of distortion is called aliasing. The high frequency information in the original signal is not only \\lost\" but resurfaces, corrupting the low frequencies. Hence FL faithfully reproduces neither the high frequency nor the low frequency information in f: Aliasing is familiar in everyday life: If one observes the rotation of the wheels of a fast moving car in movie, it appears that the wheels rotate very slowly. A movie image is actually a sequence of samples (24 frames/second). This sampling rate is below the Nyquist rate needed to accurately reproduce the motion of the rotating wheel. Example 6.2.1. If a car is moving at 60mph and the tires are 3ft in diameter then the angular velocity of the wheels is ! =58 1 230 CHAPTER 6. SAMPLING 0 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 1.2 x (a) Partial Fourier inverse. –0.1 0 0.1 0.2 0.3 0.4 0.5 0.2 0.4 0.6 0.8 1 1.2 x (b) Pure aliasing contribution. Figure 6.4: The two faces of aliasing, d = :05: Example 6.2.2. In \fgure 6.4 the two contributions to f − FL are shown separately, for the rectangle function f (x)= ˜[−1;1](x): Figure 6.4(a) shows the Gibbs contribution, \fg- ure 6.4(b) shows the \\pure aliasing\" part. Figure 6.5 shows the original function, its partial Fourier inverse and its Shannon-Whittaker interpolant. The partial Fourier inverse is the solid line, the dotted line is the Shannon-Whittaker interpolant. In this example, the con- tributions of the Gibbs artifact and the pure aliasing error are of about the same size and have same general character. It is evident that the Shannon-Whittaker interpolant is more distorted than the partial inverse of the Fourier transform, though visually they are quite similar. 0 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 1.2 x Figure 6.5: Partial Fourier inverse and Shannon-Whittaker interpolant. Example 6.2.3. For comparison consider the continuous function g(x)= ˜[−1;1](x)(1 − x2) and its reconstruction using the sample spacing d = :1. In \fgure 6.6(a) it is just barely possible to distinguish the original function from its approximate reconstruction. The worst 6.2. THE POISSON SUMMATION FORMULA 231 errors occur near the points where g is \fnitely di\u000berentiable. Figure 6.6(b) shows the graph of the di\u000berence, g − GL; note the scale along the y-axis. –0.2 0 0.2 0.4 0.6 0.8 1 1.2 y –2 –1 1 2 x (a) The Shannon-Whittaker interpolation. –0.015 –0.01 –0.005 0 0.005 0.01 –2 –1 1 2 x (b) The di\u000berence. Figure 6.6: What aliasing looks like for a smoother function, d = :1: Example 6.2.4. As a \fnal example we consider the e\u000bect of sampling on a \\furry function.\" Here we use a function of the sort introduced in example 3.2.5. These are continuous functions with \\sparse,\" but slowly decaying Fourier transforms. Figure 6.7(a) is the graph of such a function and \fgure 6.7(b) shows the Shannon-Whittaker interpolants with d = :1;:05 and :025: For a function of this sort, Shannon-Whittaker interpolation appears to produce smoothing. –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x (a) A furry function. –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 1.2 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x (b) Shannon-Whittaker interpolants. Figure 6.7: What aliasing looks like for a furry function, d = :1;:05;:025: The functions encountered in imaging applications are usually spatially limited and therefore cannot be bandlimited. However, if the function f is smooth enough then its 232 CHAPTER 6. SAMPLING Fourier transform decays rapidly and therefore, by choosing L su\u000eciently large, the dif- ference, ^f (˘) − ^FL(˘) can be made small. One says that such a function is e\u000bectively bandlimited. Though this concept does not have a precise de\fnition it is very important in imaging. In most applications it is not enough to have ^f (˘) itself small outside on [−L; L]: Examples 6.2.2 and 6.2.3 illustrate what is meant by e\u000bective bandlimiting. As both functions have bounded support, neither is actually bandlimited. Because of the Gibbs phenomenon a Shannon-Whittaker interpolant for f displays large oscillatory arti- facts, not matter how large L is taken. On the other hand, away from the jumps, the Shannon-Whittaker interpolant does a good job reconstructing f: In applications one needs to select the sampling rate to be su\u000eciently large so that the aliasing error, X n6=0 ^f (˘ +2nL) is under control. Whether or not this is possible depends upon whether there exists an L so that f is e\u000bectively L-bandlimited. To diminish the e\u000bects of aliasing, an analogue signal may be passed through a \\low pass \flter\" before it is sampled. An ideal low pass \flter removes the high frequency content in the signal. In this way the sampled data accurately represents the low frequency information present in the original signal without corruption from the high frequencies. An ideal low pass \flter would replace f (x) with the signal fL(x) de\fned by the following properties: ^fL(˘)= ^f (˘)if j˘j\u0014 L; ^fL(˘)=0 if j˘j\u0015 L: (6.16) The samples ffL( nˇ 6.2. THE POISSON SUMMATION FORMULA 233 That is, \\measuring\" the function f at x = nˇ 234 CHAPTER 6. SAMPLING Exercise 6.2.10. The ideal low pass \fltered function, fL(x) can be expressed as a convo- lution fL(x)= f \u0003 kL(x): Find the function kL: If the variable x is \\time\" explain the di\u000eculty in implementing an ideal low pass \flter. Exercise 6.2.11. Suppose that is an even function which assumes its maximum at x =0 then explain why the interval over which[ \u0003 f (˘) − ^f (˘) is small is controlled by 1Z −1 x2 (x)dx: Exercise 6.2.12. Show that if (x)= ' \u0003 ˜[− π 6.2. THE POISSON SUMMATION FORMULA 235 6.2.4 Sampling periodic functions 236 CHAPTER 6. SAMPLING In this case the non-zero Fourier coe\u000ecients are easily obtained from the samples. The formulˆ in (6.19) have a nice geometric interpretation: the set of vectors f(1;e 2πij 6.2. THE POISSON SUMMATION FORMULA 237 Exercise 6.2.15. Prove (6.19), remember to use the Hermitian inner product! Exercise 6.2.16. Suppose that f is an N -bandlimited, L-periodic function. Let fx1;::: ;x2N +1gˆ [0;L) such that xj 6= xk if j 6= k: Show that f can be reconstructed from the samples ff (xj): j =1;::: ; 2N +1g: From the point of view of computation, explain why equally spaced samples are preferable? Exercise 6.2.17. Prove that FN ( jL 238 CHAPTER 6. SAMPLING There exist \fnitely many numbers which are equally close to two such numbers, for these a choice simply has to be made. If x =(−1) b0 Pn−1 j=−1 bj+12j 6.3. HIGHER DIMENSIONAL SAMPLING 239 6.3 Higher dimensional sampling In imaging applications one usually works with functions of two or three variables. Let f be a function de\fned onRn and fxkgˆRn ; a discrete set of points. As before, the values ff (xk)g are the samples of f at the sample points fxkg: Parts of the theory of sampling in higher dimensions exactly parallels the one dimensional theory though the problems of sampling and reconstruction are considerably more complicated. Suppose that f is a function de\fned onRn : As in the one dimensional case samples are usually collected on a uniform grid. In this case it is more convenient to label the sample points using vectors with integer coordinates. To avoid confusion bold face letters are used to denote such vectors, i.e. j =(j1;::: ;jn)where ji 2Z;i =1;::: ;n: De\fnition 6.3.1. The sample spacing for a set of uniformly spaced samples inRn is a vector h =(h1;::: ;hn) with positive entries. The index j corresponds to the sample point xj =(j1h1;::: ;jnhn): A values of a function, ff (xj)g at these points is a uniform sample set. A somewhat more general de\fnition of uniform sampling is sometimes useful: \fx n orthogonal vectors fv1;::: ; vng: For each j =(j1;::: ;jn) 2Zn de\fne the point xj = j1v1 + \u0001\u0001\u0001 jnvn: (6.22) The set of points fxj : j 2Zng de\fnes a uniform sample set. This sample set is the result of applying a rotation to a uniform sample set with sample spacing (kv1k;::: ; kvnk): As in the one dimensional case, the de\fnitions of sample spacing and uniform sampling depend on the choice of coordinate system. A complication in several variables is that there are many di\u000berent coordinate systems that naturally arise. Example 6.3.1. Let (h1;::: ;hn) be a vector with positive coordinates. The set of points, f(j1h1;::: ;jnhn): (j1;::: ;jn 2Zng is a uniform sample set. Example 6.3.2. Let (r; \u0012) denote polar coordinates forR2 ; they are related to rectangular coordinates by x = r cos \u0012; y = r sin \u0012: In CT-imaging we often encounter functions which are uniformly sampled on a polar grid. Let f (r; \u0012) be a function onR2 in terms of polar coordinates and let ˆ> 0and M 2Nbe \fxed. The set of values ff (jˆ; 2kˇ 240 CHAPTER 6. SAMPLING In more than one dimension there are several di\u000berent reasonable notions of \\bandlim- ited\" data. De\fnition 6.3.2. A function f de\fned inRn is B-bandlimited, with B =(B1;::: ;Bn)an n-tuple of positive numbers if ^f (˘1;::: ;˘n)=0 if j˘jj >Bj for j =1;::: ;n: (6.23) De\fnition 6.3.3. A function f de\fned inRn is R-bandlimited if ^f (˘1;::: ;˘n)=0 if k˘k >R (6.24) There are other reasonable choices as well. The results proved above carry over easily to B-bandlimited functions. However these generalizations are often inadequate to handle problems which arise in practice. Nyquist's theorem has an obvious generalization. Theorem 6.3.1 (Higher dimensional Nyquist Theorem). Let B =(B1;::: ;Bn) be an n-tuple of positive numbers. If f is a square integrable function which is B-bandlimited then f can be reconstructed from the samples ff ( j1ˇ 6.3. HIGHER DIMENSIONAL SAMPLING 241 is a periodic function. The Fourier coe\u000ecients of fp are related to the Fourier transform of f in much the same way as in one dimension: bfp(k)= Z [0;1]n fp(x)e −2ˇihx;kidx = ZRn f (x)e −2ˇihx;kidx = ^f (2ˇik): (6.25) Applying the Fourier series inversion formula with a function that is smooth enough and decays rapidly enough shows that X j2Zn f (x + j)= X k2Zn ^f (2ˇik)e 2ˇihx;ki: (6.26) This is the n-dimensional Poisson summation formula. The set of samples points is sometimes determined by the physical apparatus used to make the measurements. As such, one often has samples of a function, f on a non-uniform grid: ff (yk)g: To use computationally e\u000ecient methods it is often important to have samples on a uniform grid fxjg: To that end approximate values for f; at these points, are obtained by interpolation. Most interpolation schemes involve averaging the known values at nearby points. This sort of averaging does not usually lead to smoothing and introduces new sources of error, beyond aliasing. An e\u000ecient method for multi-variable interpolation is discussed in section 8.7. Another approach is to \fnd a computational scheme adapted to the non-uniform grid. An example of this is presented in section 8.4 Exercise 6.3.1. Prove Theorem 6.3.1. Exercise 6.3.2. Find an n-dimensional generalization of the Shannon-Whittaker interpo- lation formula (6.5). Exercise 6.3.3. Give a de\fnition of oversampling and a generalization of formula (6.7) for the n-dimensional case. Exercise 6.3.4. For a set of linearly independent vectors fv1;::: ; vng \fnd a notion of V- bandlimited so that a V-bandlimited function is determined by the samples ff (xj): j 2Zng and this result is optimal. Exercise 6.3.5. Using the results proved earlier about Fourier series give hypotheses on the smoothness and decay of f which are su\u000ecient for (6.26) to be true. 242 CHAPTER 6. SAMPLING Chapter 7 Filters This chapter discusses basic concepts in \fltering theory. A \flter is the engineering term for any process that maps an input or collection of inputs to an output or collection of outputs. As inputs and outputs are generally functions of a variable (or variables), in mathematical terms, a \flter is a map from one space of functions to another space of functions. Most of our discussion is devoted to linear \flters, recasting our treatment of the Fourier transform in the language of linear \fltering theory. The beginning of the chapter is mostly linguistic, introducing engineering vocabulary for concepts already presented from a mathematical standpoint. In imaging applications the functions of interest usually depend on two or three spatial variables. The measure- ments themselves are, of necessity also functions of time though this dependence is often suppressed or ignored. In most of this chapter we consider \flters acting on inputs which are functions of a single variable. There are three reasons for doing this: \frst the discussion is simpler, second, it reﬂects the origins of \fltering theory in radio and communications, and third, \flters acting on functions of several variables are usually implemented \\one vari- able at a time.\" Most linear \flters are expressed, at least formally as integrals. The fact that higher dimensional \flters are implemented one variable at a time reﬂects the fact that higher dimensional integrals are actually computed as iterated, one dimensional integrals, that is ZZ [a1;b1]\u0002[a2;b2] f (x; y)dxdy = b1Z a1 2 4 b2Z a2 f (x; y)dy 3 5 dx: In the second part we consider the implementation of shift invariant \flters on sampled data. Section7.6 presents the basic concepts of image processing. 7.1 Basic deﬁnitions 244 CHAPTER 7. FILTERS example, an input could be the sound produced by an orchestra. The output could be a tape recording of that sound. Evidently such an input is a function of time but it is also a function of the point in space where it is measured. With this in mind the output is really the recording made by a microphone at a \fxed location. An input could also be a function of spatial parameters such as the density of a photographic image as a function of location in the \flm plane or the X-ray absorption coe\u000ecient of an object as a function of a point in space. In the \frst case, the output might be a drawing which locates the sharp edges in the photographic image (an edge enhancing \flter). For the second case the Radon transform could be considered as a \flter which maps the absorption coe\u000ecient to its integrals over lines in space. Much of the terminology in \fltering theory is connected with the intended application, so a given mathematical concept, when connected to a \flter has one name if the \flter is used to process radio signals and a di\u000berent name if the \flter is used in imaging. Functional notation, similar to that used for linear transformations, is often used to denote the action of a \flter, that is a \flter A takes an input, x to the output A x: 7.1.1 Examples of ﬁlters In applications one rarely considers \\arbitrary\" \flters. Before beginning a careful analysis of \flters we consider typical examples of \flters acting on functions of a single variable. Example 7.1.1. The operation of scaling de\fnes a \flter Ax(t)= ax(t); here a is a positive number often called the ampli\fcation factor. Example 7.1.2. Shifting a signal in time de\fnes a \flter. Let ˝ denote a constant and de\fne A˝ x(t)= x(t − ˝ ): Example 7.1.3. Multiplying a signal by a function de\fnes a \flter, let (t) denote a function and de\fne M x(t)= (t)x(t): Example 7.1.4. Convolution with a function de\fnes a \flter. Let '(t) be a function and de\fne C'x(t)= 1Z −1 '(t − s)x(s)ds: Example 7.1.5. If x(t) is a signal depending on a single variable then di\u000berentiation de\fnes a \flter Dx(t)= dx 7.1. BASIC DEFINITIONS 245 engineering approaches to \fltering lies in the treatment of the spaces of inputs and outputs. Before a mathematician starts to discuss a map from a space of functions to another space of functions he or she likes to have well de\fned domain and target spaces, often equipped with norms. By contrast, engineers often describe a process or write down a formula without stipulating the exact nature of the inputs or the expected properties of the outputs. Of course, the implementation of the \flter requires that the actual inputs produce meaningful outputs. This inevitably entails approximations and the precise relationship between the implemented \flter and its theoretical description is rarely made explicit. All the examples considered so far are linear \flters, in mathematical language these are linear transformations or linear operators. De\fnition 7.1.1. A linear \flter A is an operation mapping inputs to outputs which satis\fes the conditions: (1). If x1 and x2 are a pair of inputs then A(x1 + x2)= A(x1)+ A(x2): (2). If x is an input and \u000b is a constant then A(\u000bx)= \u000b A(x): In order to be clear about this distinction we consider some examples of non-linear \flters. Example 7.1.7. The squaring operation, Sx(t)= (x(t))2 is a non-linear \flter. This \flter is the basis of FM radio. Example 7.1.8. Suppose that the input is a pair of signals, (x1(t);x2(t)) and the output is their product P (x1;x2)(t)= x1(t)x2(t): If we think of one signal as \fxed so the \flter really acts only on the other, e.g. x1 7! P (x1;x2); then this is a linear operation. However, as a \flter acting on a pair of inputs it is non-linear P (x1 + y1;x2 + y2)(t)=(x1(t)+ y1(t))(x2(t)+ y2(t)) 6= x1(t)y1(t)+ x2(t)y2(t)= P (x1;x2)(t)+ P (y1;y2)(t): (7.1) Example 7.1.9. An electrical diode is circuit element which only passes current moving in the positive direction. Its action is modeled by the formula Rx(t)= ˜[0;1)(x(t))x(t); In electrical engineering this \flter is called a recti\fer. Example 7.1.10. The process of quantization is de\fned by a non-linear \flter. Suppose that for each time t the binary representation of x(t)isgiven by x(t)=sign x(t) 1X j=−1 bj(x(t))2 j : 246 CHAPTER 7. FILTERS One scheme used for quantization is called truncation, we let QN;M x(t)= sign x(t) NX j=−M bj(x(t))2 j : Example 7.1.11. The function x(t) might represent a signal which we would like to measure and A x(t) is the result of our measurement. The measurement process itself de\fnes the \flter A; which can either be linear or non-linear. A simple model for measurement is evaluation of a weighted average, Al x(t)= 1Z −1 (t − s)x(s)ds: On the other hand many \\detectors\" become saturated when the signal is too strong. A model for such a device might be Anl x(t)= 1Z −1 (t − s)G[x(s)]ds: Here G(x) is a non-linear function which models the saturation of the detector, e.g. G(x)= 8 >< >: x if jxj <T; T if x \u0015 T; −T if x \u0014−T: A slightly di\u000berent model is given by A0 nlx(t)= G[(Alx)(t)]: Exercise 7.1.1. Show that the \flters in examples 7.1.1- 7.1.6 above are linear. Exercise 7.1.2. Show that examples 7.1.7- 7.1.10 are non-linear. 7.1.2 Linear ﬁlters 7.1. BASIC DEFINITIONS 247 The function a(t; s) is called the kernel function; it completely describes the action of the \flter on an `arbitrary' input. For example, the linear \flter A which assigns to an input, x(t) its anti-derivative, X(t) is expressed as an integral by setting X(t)= A x(t)= 1Z 0 a(t; s)x(s)ds; where a(t; s)= (1if 0 \u0014 s \u0014 t; 0if s> t: Sometimes the action of a \flter is expressed as an integral even though the integral does not, strictly speaking make sense. This is the case if the kernel function is a `generalized function.' The Hilbert transform is often \\de\fned\" by the expression Hx(t)= i 248 CHAPTER 7. FILTERS Perhaps the simplest general class of \flters are the multiplication \flters. If (t)isa function then the operation: M : x(t) 7! (t)x(t) de\fnes a \flter. This operation makes sense for very general types of inputs. In applications the multiplier is frequently taken to equal one for t in an interval and zero for t outside a larger interval. In this case M windows the input. If 1 and 2 are two functions then we have the relations M 1(M 2(x)) = M 1 2(x)= M 2(M 1 (x)): In other words, the order in which multiplication \flters are applied does not a\u000bect the outcome. Mathematically one says that multiplication \flters commute. The next section treats another class of \flters which have this property. 7.1.3 Shift invariant ﬁlters In applications, the most important class of \flters is called the shift invariant \flters. If the input is shifted in time and such a \flter is applied then the result is just shifted in time. In other words, the response of the \flter to an input does not depend on the time that the input arrives. De\fnition 7.1.2. A linear \flter A is called shift invariant if for all real numbers ˝ we have the identity A(x˝ )=(A x)˝ where x˝ (t)= x(t − ˝ ): A linear shift invariant \flter has a simpler kernel function. Suppose that A is such a \flter with kernel function a(t; s): Changing variables gives following equalities: A(x˝ )(t)= 1Z −1 a(t; s)x˝ (s)ds = 1Z −1 a(t; s)x(s − ˝ )ds = 1Z −1 a(t; ˙ + ˝ )x(˙)d˙; on the other hand (A x)˝ (t)= A(x)(t − ˝ )= 1Z −1 a(t − ˝; ˙)x(˙)d˙: Comparing these results shows that if a(t; s) de\fnes a shift invariant \flter then a(t; ˙ + ˝ )= a(t − ˝; ˙) for all ˙ 2R: (7.2) Setting ˙ =0 gives a(t; ˝ )= a(t − ˝; 0): In other words, the kernel function a(t; s); which describes the action of A; depends only on the di\u000berence t − s. Setting k(t)= a(t; 0) gives A x(t)= 1Z −1 k(t − s)x(s)ds = k \u0003 x(t): (7.3) 7.1. BASIC DEFINITIONS 249 Proposition 7.1.1. A linear \flter is shift invariant if and only if it can represented as a convolution. The exercise completes the proof of the proposition. Exercise 7.1.3. Suppose that a \flter A is given by convolution with a function A x(t)= 1Z −1 '(t − s)x(s)ds; show that A is a linear shift invariant \flter. 7.1.4 Harmonic components One often assumes that the input signal has a Fourier transform, ^x(˘): If the Fourier transform is written in terms of polar coordinates in the complex plane, ^x(˘)= j^x(˘)je i˚(˘) then j^x(˘)j is called the amplitude of the signal at frequency ˘ and ˚(˘) is called the phase. Because the complex exponential is 2ˇ-periodic, the phase is not unambiguously de\fned. That is e i˚ = e i(˚+2nˇ) for any n 2Z: How to choose the phase depends on the context. Often one \fxes an interval of length 2ˇ; for example [−ˇ; ˇ)or[0; 2ˇ); andtheninsists that ˚(˘) belong to this interval. A di\u000berent choice is to take the phase to be a continuous function of ˘: Example 7.1.12. Suppose that x(t)= eit2 : Using the \frst approach the phase ˚1(t)is computed by \fnding an integer n so that 0 \u0014 t2 − 2ˇn < 2ˇ; the phase is then ˚1(t)= t2 − 2ˇn: In the second approach the phase is simply ˚2(t)= t2: It is reasonable to enquire where the \\information\" in the Fourier transform lies. Is it more important to get the amplitude or phase correct? The answer depends on the intended application. In image processing it turns out that the phase is more important than the amplitude. Random errors in the amplitude produce little distortion in the reconstructed image whereas random errors in the phase produce serious artifacts. Intuitively this is reasonable as the phase of the Fourier transform encodes the relative positions of objects. Translating a function f by a vector ˝˝˝; produces an overall shift in the phase of the Fourier transform, bf˝˝˝ (˘˘˘)= e ih˘˘˘;˝˝˝ i ^f (˘˘˘): Figure 7.1(a) shows a grey scale image de\fned by a density function, f; \fgure 7.1(b) shows the results of random errors in the amplitude of ^f and \fgure 7.1(c) shows the results of random errors in the phase of ^f: By contrast, in audio signal processing, it is often asserted that the phase carries no \\useful information.\" 250 CHAPTER 7. FILTERS? (a) A natural scene.? (b) Errors in the ampli- tude of ^f:.? (c) Errors in the phase of ^f:. Figure 7.1: The e\u000bects of errors in the amplitude and phase of the Fourier transform on a reconstructed image. Physically one thinks of the Fourier transform as giving a decomposition of a signal into harmonic components. The actual operation of Fourier transform is somewhat at variance with an intuitive understanding of this concept. To determine the \\amount\" of a signal x(t) at a frequency ˘; that is ^x(˘); a knowledge of the signal for all times is required. This is because ^x(˘)= 1Z −1 x(t)e −it˘dt: Intuitively one thinks of a signal as having an \\instantaneous frequency.\" For example if x(t)=cos(!t)for t in an interval [t1;t2] then one would probably say that the frequency of x(t); in that time interval, is ! 7.1. BASIC DEFINITIONS 251 state of a system which is composed of collection of resonant modes. In magnetic reso- nance spectroscopy a complicated molecule is caused to vibrate and emit a radio frequency signal, x(t): This signal is composed of a collection of exponentially damped vibrations. Figure 7.2(a) shows a typical time series measurement. The useful information in x(t)is extracted by taking the Fourier transform as shown in \fgure 7.2(a). The locations of the peaks determine the frequencies of the di\u000berent vibrational modes and their widths give a measure of the damping. This information can in turn be used to deduce the structure of the molecule.? (a) The time series signal x(t):? (b) The amplitude of ^x: Figure 7.2: Using magnetic resonance to determine the vibrational modes of a molecule. Spectral analysis of this sort is used throughout science and engineering; it provides a di\u000berent perspective on the meaning of the Fourier transform. In magnetic resonance spectroscopy the signal decays exponentially, so little error results from cutting it o\u000b after a \fnite time and computing the Fourier transform of the time limited signal. In other applications, the signal does not decay, in any reasonable sense and so is regarded instead as a periodic signal. Such signals are analyzed using the Fourier series. In the world of real measurements and computation, where everything is done with \fnite data sets, the practical distinctions between the Fourier transform and Fourier series disappear. These distinctions remain important in the design of algorithms and the interpretation of results. Exercise 7.1.4. Compare the results of these two approaches to the spectral analysis of a function x(t) assuming that x(t) is periodic but with a period much larger than T: Exercise 7.1.5. Why might it be preferable to use a window function which goes smoothly from 1to0? Exercise 7.1.6. Is the Fourier transform a shift invariant \flter? 252 CHAPTER 7. FILTERS 7.1.5 The transfer function 7.1. BASIC DEFINITIONS 253 The analogous computation for a shift invariant \flter, in the frequency space descrip- tion, is the approximate determination of the pointwise product, ^k(˘)^x(˘): This is done by evaluating ^k(˘j)and ^x(˘j)for N values of ˘ and then computing the products f^k(˘j)^x(˘j)g: The matrix analogue is multiplying the vector (^x(˘1);::: ; ^x(˘N )) by the diagonal matrix kij = ^k(˘i)\u000eij: This requires O(N ) arithmetic operations. As we shall see, the approximate computation of the Fourier transform requires O(N log2 N ) operations, provided that N is a power of 2. In applications N =210 is not unusual, in this case N 2 =2 20;N log2(N ) ˇ 2 14; N 2 254 CHAPTER 7. FILTERS 7.1.6 The δ-function revisited 7.1. BASIC DEFINITIONS 255 The family of functions '\u000f(t)= 1 256 CHAPTER 7. FILTERS Exercise 7.1.9. The identity \flter is de\fned as the \flter which takes an input to itself, Id x(t)= x(t): Show that the impulse response of the identity \flter is \u000e(t): What is its transfer function. 7.1.7 Causal ﬁlters In the context of time dependent signals, there is a special subclass of \flters called causal \flters. De\fnition 7.1.4. A \flter is causal if the output, at a given time, depends only upon the behavior of the signal at earlier times. For a linear \flter this means that A x(t)= tZ −1 a(t; s)x(s)ds A linear, shift invariant \flter is causal if and only if its impulse response k(t) vanishes for t< 0: This condition is important when working with time dependent signals if a \flter must be implemented in \\real time.\" In the context of image processing this distinction it often less important because an image is represented as a function of spatial variables. To avoid aliasing in the data acquisition step, it is useful to attenuate the high frequency components before the signal is sampled. This is called \\low pass \fltering\" and must often be realized by a causal \flter. The transfer function of a causal \flter has an important analytic property. Proposition 7.1.2. If the \flter A de\fned by A x = k \u0003 x is causal then the ^k(˘) has a complex analytic extension to the lower half plane. Proof. The hypothesis that A is causal implies that k(t)=0 for t< 0 and therefore ^k(˘)= 1Z 0 k(t)e−itξdt: If we replace ˘ by z = ˘ + i˙; with ˙< 0then Re[−i(˘ + i˙)] = ˙t < 0 in the domain of the integration. Thus of ^k(z)= 1Z 0 k(t)e−itzdt = 1Z 0 k(t)eσte−itξdt; (7.7) the real exponential in the integrand is decaying. Di\u000berentiating under the integral sign shows that @¯z ^k = 0 in the lower half plane. 7.1. BASIC DEFINITIONS 257 7.1.8 Bandpass ﬁlters In \fltering theory there are certain idealized \flters which are frequently employed. De\fne the rectangle function rect(t)by rect(t)= ( 1 jtj\u0014 1=2; 0 jtj > 1=2 and the function rect[\u000b;\f] by rect[\u000b;\f](˘)= ( 1 \u000b \u0014 ˘ \u0014 \f; 0 otherwise. A bandpass \flter is de\fned in the Fourier representation by B[\u000b;\f]x = F −1[rect[\u000b;\f](j˘j)^x(˘)]; with 0 \u0014 \u000b< \f: The \fltered signal contains only the part of the input signal with frequencies in the band [\u000b; \f]; this is called the passband. Computing the inverse Fourier transform we see that B[\u000b;\f] is represented by convolution with b[\u000b;\f](t)= 2 Re \" e −i t(α+β) 258 CHAPTER 7. FILTERS damped by using a smoother approximation to ˜[−B;B] to de\fne the transfer function of an approximate low pass \flter. In imaging applications a \flter which attenuates the high frequencies and passes low frequencies with little change is called an apodizing \flter.Its transfer function is called an apodizing function. We consider two examples of such \flters. Example 7.1.14. A simple example is an analogue of the Fejer mean. Instead of ˜[−B;B] we use the \\tent\"-function ^tB(˘)= 1 7.1. BASIC DEFINITIONS 259 –4 –2 2 4 6 8 10 12 14 16 18 20 –3 –2 –1 1 2 3t Figure 7.4: Pointspread functions for lowpass \flters This function is even smoother than the tent function, having a continuous \frst deriva- tive. To compute its pointspread function we use the identity cos2(x)= cos(2x)+1 260 CHAPTER 7. FILTERS Exercise 7.1.12. Show that a high pass \flter cannot be causal. Exercise 7.1.13. The function ˜[0;1)(˘) de\fnes the transfer function of a \flter P which removes all negative frequency components. Show that P = 1 7.1. BASIC DEFINITIONS 261 Example 7.1.16. An extreme case is the rectangle function (2d)−1˜[−d;d](t): The corresponding \flters Ad average the signal over an interval of length of 2d: In these cases \u0001Ad;\u0014 =2d for all values of \u0014: Example 7.1.17. A less extreme case is provided by the tent functions td(t)= 8 >< >: 0if jtj\u0015 d; t + d if − d< t< 0; d − t if 0 \u0015 t<d: Letting Td denote the corresponding \flters we see that \u0001Td;\u0014 =2(1 − \u0014)d: First zero: If the pointspread function of a \flter vanishes then the locations of the \frst positive and negative zeros can be used to give another de\fnition of resolution. This is just the previous de\fnition with \u0014 =0: Suppose that k(t) is the pointspread function of a \flter A and it vanishes at positive and negative values. Let t− < 0be the largest negative zero and t+ > 0 be the smallest positive zero, de\fne \u0001A;0 = t+ − t−: Example 7.1.18. Let FB denote the Fejer mean \flter with transfer function ^tB(˘) and pointspread function tB(t)= 1 262 CHAPTER 7. FILTERS This is a measure of how much the \flter smears out a point source. The number \u0001A;ew is the width of the rectangle function enclosing the same signed area as the graph of k(t)=k(0): The fact that we use signed area has a signi\fcant e\u000bect on the value and interpretation of this number. It is not di\u000ecult to construct an example of a function k(t)sothat k(0) = 1 and 1Z −1 k(t)dt =0: For the \flter A; de\fned by this example, \u0001A;ew =0; in other words, by this measure there is no loss in resolution. More pertinent examples are provided by DB and FB; for these \flters we have \u0001FB ;ew = 4ˇ2 7.1. BASIC DEFINITIONS 263 With this de\fnition we see that \u0001DB;ny; 1 264 CHAPTER 7. FILTERS Example 7.1.19. Suppose that A1 x(t)= @tx(t) and A2 x(t)= 1Z −1 '(t − s)x(s)ds; where '(t) is a di\u000berentiable function, vanishing outside a bounded interval. The two possible compositions are A1 \u000eA2 x(t)= 1Z −1 @t'(t − s)x(s)ds; and A2 \u000eA1 x(t)= 1Z −1 '(t − s)@sx(s)ds: To implement the \frst case we need to approximate the convolution 't \u0003 x whereas in the second case we \frst need to approximate @tx and then the convolution ' \u0003 xt: Because of the di\u000eculties in approximating di\u000berentiation, the composition A1 \u000eA2 is much easier to implement than A2 \u000eA1 : Example 7.1.20. If A1 is shift invariant and A2 is not then generally A1 \u000eA2 6= A2 \u000eA1 : As an example let A1 x = @tx and A2 x(t)= 1Z −1 (s + t)x(s)ds: A direct computation shows that A1 \u000eA2 x(t)= 1Z −1 x(s)ds whereas, integrating by parts gives A2 \u000eA1 x(t)= − 1Z −1 x(s)ds: The transfer function for the cascaded \flter de\fned by impulse response h = hk \u0003\u0001\u0001\u0001 \u0003h1 is the product of the transfer functions ^h(˘)= ^hk(˘) \u0001\u0001\u0001 ^h1(˘): In the implementation of a cascade, using the Fourier representation, it is important to account for the limitations of \fnite precision arithmetic when selecting the order in which to multiply the terms in the transfer function. By grouping terms carefully one can take advantage of cancelations between large and small factors, thereby avoiding overﬂows or underﬂows. 7.1. BASIC DEFINITIONS 265 Exercise 7.1.17. Assuming that x(t)and xt(t) are absolutely integrable, prove the for- mulˆin example 7.1.20. 7.1.11 The resolution of a cascade of ﬁlters \u0003 In section 7.1.9 we discussed a variety of de\fnitions for the resolution available in the output of a linear, shift invariant \flter. If A1 and A2 are such \flters it is reasonable to enquire how the resolution of A1 \u000eA2 is related to the resolution of the components. The answers depend on the type of \flter and the choice of de\fnition. Full width \u0014-maximum: For values of \u0014 between 0 and 1 and general \flters it is di\u000ecult to relate \u0001A1 \u000eA2;\u0014 to \u0001A1;\u0014 and \u0001A2;\u0014: For the special case of \flters with Gaussian pointspread functions there is a simple relation. For each a> 0; set ga(t)= e −at2 and let Ga denote the \flter with this pointspread function. A simple calculation shows that \u0001Ga;\u0014 = r 266 CHAPTER 7. FILTERS The proof of this estimate uses the mean value theorem for integrals, Theorem B.8.4. We can suppose that k1(0) = k2(0) = 1 as this does not a\u000bect the equivalent width. Because both functions are non-negative, the MVT for integrals applies to give constants t1 and t2 such that k1 \u0003 k2(0) = 1Z −1 k1(t)k2(−t)dt = k1(t1) 1Z −1 k2(−t)dt = k2(t2) 1Z −1 k1(t)dt: On the other hand 1Z −1 k1 \u0003 k2(t)dt = 1Z −1 k1(t)dt 1Z −1 k2(t)dt: Thus \u0001A1 \u000eA2,ew = 1R −1 k1 \u0003 k2(t)dt 7.1. BASIC DEFINITIONS 267 7.1.12 Filters and RLC-circuits \u0003 In many applications it is important to \flter signals in real time. For example, before sampling a time series, it is preferable to pass the analogue signal through a low pass \flter. This reduces the e\u000bects of aliasing. A simple way to do this is to use RLC-circuits. These implement passive linear \flters and are built out of three basic components known as resistors, capacitors and inductors. These are \\ideal\" circuit elements, characterized by the relationship between the current I(t); through the device and the voltage V (t); across the device: (1). A resistor is characterized by its impedence which is a positive real number R: The voltage and current then satisfy the relationship V (t)= RI(t): (7.14) (2). A capacitor is characterized by its capacitance which is a positive number C: The voltage and current then satisfy the relationship I(t)= C dV 268 CHAPTER 7. FILTERS Kircho\u000b 's laws Using these components, circuits can be built which approximate low pass, high pass and bandpass \flters and operate in real time. Kircho\u000b 's laws allow such circuits to be analyzed. We brieﬂy describe them and then analyze some very simple circuits. To describe Kircho\u000b's laws, a circuit is considered to be a directed network consisting of nodes and edges. Each edge is oriented to de\fne the positive direction of current ﬂow. A cycle node edge node edge node edge node edge edge node edge edge Figure 7.5: A network One of the ideal circuit elements is placed along each edge, In circuit diagrams resistors, capacitors and inductors are represented by the symbols shown in \fgure 7.6. An InductorA Resistor A Capacitor Figure 7.6: Standard symbols for passive circuit elements Figure 7.7 shows two simple circuit diagrams. The circle indicates a voltage source which is used to model the input to the \flter de\fned by the circuit. Equations (7.14)- (7.16), give the relationships between the current and voltage along each edge. There is a current ﬂowing through each edge and a voltage drop across each circuit element. Kircho\u000b's 7.1. BASIC DEFINITIONS 269 laws relate the currents ﬂowing into a node and the voltage drops around closed paths or cycles in the circuit: \u000f At each node the sum of the currents entering and leaving is zero. Thinking of the current as a ﬂow of electrons, this is just the \\conservation of charge.\" \u000f A cycle is a closed path in a network. The sum of the voltage drops around any cycle is zero. This is the \\conservation of energy:\" a charged particle which travels through the circuit and returns to its starting point should experience no net change in its energy. Using the de\fning relations, (7.14), (7.15) and (7.16) and Kircho\u000b's laws one can obtain a system of di\u000berential equations which relate the voltages and currents for the edges of an RLC circuit. To use an RLC-circuit as a \flter one imagines that the input is a voltage source connected to a pair of nodes and the output is the voltage measured between two other nodes in the circuit. We now consider some very simple examples. Approximate high and low pass \flters\u0003 High and low pass \flters can be crudely approximated using just two circuit elements as shown in \fgure 7.7. The input is a voltage source V (t) and the output is the voltage V2(t); measured across the resistor. C R V 1 (t) V 2 (t) V(t) (a) RC-circuit, a \\high pass \flter.\" R V 1 (t) V 2 (t) V(t) L (b) RL-circuit, a \\low pass\" \flter. Figure 7.7: Simple RLC-circuits Example 7.1.21. Circuit (7.7)(a) is a capacitor \\in series\" with the voltage source. The circuit consists of a single loop so the current is the same everywhere. Kircho\u000b's law for voltage drops gives V1 + V2 = V: Using the de\fning relations (7.14) and (7.15) this implies that the current I(t)satis\fes C dV 270 CHAPTER 7. FILTERS The causal solution of this equation is given by I(t)= C tZ −1 e RC(t−s) dV 7.1. BASIC DEFINITIONS 271 Example 7.1.22. Now we analyze the a\u000bect of an inductor in series with a voltage source. Again the circuit consists of a single loop so the current is everywhere the same. Kircho\u000b's law for voltage drops gives V1 + V2 = V: Using the de\fning relations (7.14) and (7.16) this implies that the current I(t)satis\fes V (t)= RI + L dI 272 CHAPTER 7. FILTERS 0 0.2 0.4 0.6 0.8 1 2468 10x (a) Filter amplitude, j^l(˘)j –1.4 –1.2 –1 –0.8 –0.6 –0.4 –0.2 0 x (b) Filter phase shift, ˚(˘) Figure 7.9: The amplitude and phase of the transfer function of an RL-\flter Resonant circuits In magnetic resonance imaging a third RLC-circuit plays an important role. This is a circuit that has a resonant frequency. There is a basic di\u000berence between resistors, on the one hand and capacitors and inductors on the other. Resistors only dissipate energy, whereas capacitors and inductors can store it. The dissipative quality of resistors is clearly seen by considering the outputs of the circuits above when the voltage source is replaced by a wire. For the RC-circuit the \\un-forced\" solution is I(t)= ke −t 7.1. BASIC DEFINITIONS 273 LC V 1 V 2 Figure 7.10: A resonant circuit. As a \fnal example we examine the a\u000bect of a resonant circuit on a voltage source in series with a resistor. Example 7.1.23. Consider the circuit in \fgure 7.11. The input to the \flter is a voltage V (t); the output is the voltage measured across the inductor. Kircho\u000b's laws give the relations V = V1 + V2;V2 = V3; V = V1 + V3;I1 = I2 + I3: (7.23) Combining these equations with the de\fning relations, (7.14)- (7.16) gives a di\u000berential equation relating I3(t)and V (t): d2I3 274 CHAPTER 7. FILTERS de\fned to be !\u0006 = 1 7.2. FILTERING PERIODIC SIGNALS 275 Good basic references for electrical networks are [6] and [60]. Exercise 7.1.23. Find the transfer functions in the preceding example with V (t) the input and V1 or V2 as the output. Exercise 7.1.24. Analyze the RLC-circuit shown in \fgure 7.13. What happens to the transfer function at [LC] − 1 276 CHAPTER 7. FILTERS is evidently an L-periodic \flter. An L-periodic \flter, A is shift invariant if A(x˝ )=(A x)˝ : Recall that if f and g are L-periodic then their periodic convolution, de\fned by f \u0003 g(t)= LZ 0 f (t − s)g(s)ds is also L-periodic. The \\L-periodic\" unit impulse, \u000eL is given formally as the sum \u000eL = 1X j=−1 \u000e(t + jL): If A is an L-periodic, shift invariant, linear \flter then it is determined by its impulse response, k which is given by k(t)= A(\u000eL): The impulse response can be either an ordinary function or a generalized function. As before A has a representation as convolution with its impulse response, A f (t)= LZ 0 k(t − s)f (s)ds: Instead of the Fourier transform, the Fourier series now provides a spectral represen- tation for a shift invariant \flter. In this case the \\transfer function\" ^k(n); is de\fned for n 2Zby applying the \flter directly to the complex exponentials ^k(n)= A(e − 2πint 7.2. FILTERING PERIODIC SIGNALS 277 For each N the N th-partial sum operator is de\fned by SN (f ; t)= 1 278 CHAPTER 7. FILTERS 7.2.1 Resolution of periodic ﬁlters \u0003 Let A denote a shift invariant, linear L-periodic \flter with pointspread function k(t): Some of the de\fnitions of resolution given in section 7.1.9 can be adapted to the context of periodic \flters and signals. The full width \u0014-maximum de\fnitions carry over in an obvious way, at least for pointspread functions with a well de\fned maximum at zero. We can also use the \frst zero de\fnition for pointspread functions which vanish. The equivalent width de\fnition can be adapted if we use the integral of k(t)over a single period: \u0001A;ew = LR 0 k(t)dt 7.2. FILTERING PERIODIC SIGNALS 279 7.2.2 The comb ﬁlter and Poisson summation 280 CHAPTER 7. FILTERS The delta function is not a function, but arguing by analogy and comparing (7.29) to (7.30) gives 1Z −1 x(t)CL(t)dt = 1 7.3. THE INVERSE FILTER 281 This is easily incorporated into the kernel function for WR \u000e S: Letting C denote convo- lution with ; the kernel function for WR \u000e S \u000e C is aWRS = 1X n=−1 '(t − nL) (nL − s): (7.32) The kernel function for WR \u000e S \u000e C is an ordinary function. This is a reﬂection of the more realistic model for sampling incorporated into this \flter. Exercise 7.2.9. Show that 1Z −1 aWRS(t; s)x(s)ds =(WR \u000e Sf )(x): Exercise 7.2.10. While the \flter WR \u000e S \u000e C is fairly realistic it still involves an in\fnite sum. A further re\fnement is to collect only a \fnite number of samples. Let ˜(t)denote a function with bounded support. (1). Find the kernel function for the \flter x 7! WR \u000e S \u000e C (˜x): (2). Find the kernel function for the \flter x 7! WR \u000e S \u000e (˜C x): 7.3 The inverse ﬁlter Let k denote the impulse response of a shift invariant \flter A; A(x)= k \u0003 x: Suppose that x(t) is a signal that we would like to determine and k \u0003 x is the output of a measurement device. How can x(t) be reconstructed given only the available measurements? What is required is a \flter which undoes the action of A : Such a \flter is called an inverse \flter. For some types of \flters it is very clear that it is not possible to recover the original signal from the \fltered output. For example, if we apply the bandpass \flter B[\u000b;\f] to x then all the information about the signal at frequencies outside the passband is irrevocably lost. In other cases the Fourier transform suggests a way to try to recover x from the knowledge of the \fltered output k \u0003 x: In the Fourier representation the inverse \flter should be given by A−1 : x −! F −1 \"[k \u0003 x 282 CHAPTER 7. FILTERS If k is function which goes to zero as jtj! 1 in a reasonable way, for example Z jk(t)jdt < 1 then ^k(˘)goes to0as j˘j! 1. This means that the process of dividing by ^k(˘) takes the measured data and increasingly ampli\fes the high frequency components. If the measured data behaved like the convolution, k \u0003 x then this would not be a problem: the high frequencies in the original signal will have been attenuated. In a real situation there is noise; the measurement is then modeled as k \u0003 x + n where n is the noise. The noise part is not the result of sending a signal through the measurement device. In this case F(k \u0003 x + n) 7.3. THE INVERSE FILTER 283 x y Figure 7.14: Modi\fed Fourier Transform of the rectangle function the e\u000bective bandwidth of the data is larger than [−4ˇn; 4ˇn] then a less drastic approach would be to modify [F(rectn)(˘)]−1 in intervals containing the zeros of[rectn; for example one could let F(rectn)\u000f(˘)= 8 >< >: F(rectn)(˘)if jF(rectn)(˘)j >\u000f; \u000f if 0 < F(rectn)(˘) \u0014 \u000f; −\u000f if − \u000f \u0014F(rectn)(˘) \u0014 0: An approximate inverse \flter is then given by F −1 \u0014 ^x 284 CHAPTER 7. FILTERS Observe that if x is L-bandlimited then so is h\u000f \u0003 x; to see this we compute the Fourier transform:\\h\u000f \u0003 x = ^h\u000f(˘)^x(˘): (7.35) If we sample the \fltered function at the points f nˇ 7.4. HIGHER DIMENSIONAL FILTERS 285 7.4 Higher dimensional ﬁlters 286 CHAPTER 7. FILTERS transfer function (in imaging). It provides a frequency space description for the action of a shift invariant \flter: A f (x)= 1 7.4. HIGHER DIMENSIONAL FILTERS 287 Each of the \flters considered above de\fnes a partial inverse to the Fourier transform, in the sense that either SRf or MRf converges to f as R tends to in\fnity. Figure 7.16 shows that result of applying these \flters to the characteristic function of a square, ˜[−1;1]2(x)= ˜[−1;1](x1)˜[−1;1](x2): As the data has jump discontinuities the \fltered image exhibits \\Gibbs artifacts.\" Note the ringing artifact parallel to the edges of the square and also its absence in the \\Gibbs shadow\" formed by the vertex. This is an indication of the fact that the detailed analysis of the Gibbs phenomenon is more complicated in higher dimensions than it is in one dimension. Note also that the Gibbs artifact is more pronounced in (a). (a) SR applied to ˜[−1,1]2 : (b) MR applied to ˜[−1,1]2 : Figure 7.16: Low pass \flters in two dimensions. Example 7.4.4. A \flter can also act selectively in di\u000berent directions. For example a low pass \flter in the x1-direction is de\fned by the transfer function ˜[0;R](j˘1j): Given a unit vector ! 2 Sn−1 the transfer function ˜[0;R](jh!; ˘˘˘ij) removes all harmonic components whose frequency in the !-direction exceeds R: A linear transformation U :Rn !Rn is a rigid rotation if kU xk2 = kxk2 (7.38) for all vectors x 2Rn : Fixing an orthogonal coordinate system forRn the rigid rotations are de\fned by matrices U which satisfy the matrix equation U tU =Id= UU t: This collection of matrices is denoted O(n): The rigid rotations ofR2 are given by the matrices O(2) = ˆ\u0012 cos \u0012 sin \u0012 − sin \u0012 cos \u0012 \u0013 ; \u0012− sin \u0012 cos \u0012 cos \u0012 sin \u0012 \u0013 for \u0012 2 [0; 2ˇ) ˙ : A linear transformation, U de\fnes an action on functions by setting fU (x)= f (U x): 288 CHAPTER 7. FILTERS De\fnition 7.4.2. A \flter A acting on functions onRn is isotropic if it commutes with all rotations, that is A fU (x)= (A f )U (x)= A f (U x): An isotropic \flter can be linear or non-linear. For example, the \flter which takes a function f to its absolute value, jf j is a non-linear isotropic \flter. Linear, shift invariant isotropic \flters have a very simple characterization. Proposition 7.4.2. A linear, shift invariant \flter A is isotropic if and only if its impulse response (or transfer function) is a radial function. Proof. Let k = A \u000e denote the impulse response of A : From the results in section 3.3.4 it follows that k is a radial function if and only if ^k is a radial function. If k is a radial function then there is a function \u0014 such that k(x)= \u0014(kxk): Let U be a rigid rotation, then A fU (x)= ZRn \u0014(kx − yk)f (U y)dy = ZRn \u0014(kU (x − y)k)f (U y)dy = A f (U (x)): (7.39) The substitution y0 = U y is used to go from the second line to the last line. This shows that a radial impulse response de\fnes an isotropic \flter. The converse statement is even easier because \u000eU (x)= \u000e(x); this follows by formally changing variables in the integral de\fning \u000eU : ZRn \u000e(U x)f (x)dx = ZRn \u000e(x)f (U −1x)dx = f (0): The de\fnition of an isotropic \flter implies that kU (x)= A \u000eU (x)= A \u000e(x)= k(x) for all U 2 O(n): This shows that k(x) only depends on kxk: 7.4. HIGHER DIMENSIONAL FILTERS 289 Complex, higher dimensional \fltering operation are often assembled out of simpler pieces. If A1 and A2 are linear \flters then their composite, A1 \u000eA2 is as well. While in general A1 \u000eA2 6= A2 \u000eA1; shift invariant \flters do commute. If ki;i =1; 2 are the impulse responses of shift invariant \flters Ai;i =1; 2 then the impulse response of the cascade A1 \u000eA2 is k1 \u0003 k2 = k2 \u0003 k1: The transfer function is the product ^k1(˘˘˘)^k2(˘˘˘): The discussion of resolution for one dimensional \flters in section 7.1.9 can be repeated almost verbatim for higher dimensional, isotropic \flters. If the \flter is not isotropic then the situation is more complicated. For example, let A be a \flter acting on functions of n variables with impulse response, a(x): Suppose that a achieves its maximum at 0 and decays to zero as kxk tends to in\fnity. The set of points FWHM(a)= fx : a(x)= 1 290 CHAPTER 7. FILTERS Exercise 7.4.1. Prove Proposition 7.4.1 assuming the f is a smooth function with bounded support. Exercise 7.4.2. Prove that if A1 and A2 are shift invariant \flters then A1 \u000eA2 = A2 \u000eA1 : Exercise 7.4.3. Suppose that A1 and A2 are isotropic \flters, show that A1 \u000eA2 is as well. Exercise 7.4.4. Show that squared length of the gradient A f = nX j=1 \u0012 @f 7.5. IMPLEMENTING SHIFT INVARIANT FILTERS 291 and the frequency domain representation as a Fourier integral Hx(t)= 1 292 CHAPTER 7. FILTERS The sample Fourier transform is connected to discrete convolution in a simple way: ˝\\hs ?xs(˘)= 1X j=−1 hs ?xs(j˝ )e −ij˝ ˘˝ = 1X j=−1 1X k=−1 h((j − k)˝ )e −i(j−k)˝˘x(k˝ )e −ik˝ ˘˝ 2 = ^hs(˘)^xs(˘): (7.45) In other words, the sample Fourier transform of ˝hs ?xs is simply ^hs(˘)^xs(˘): The dual Poisson summation formula, (6.12) relates ^xs to ^x : ^xs(˘)= 1X j=−1 ^x(˘ + 2ˇj 7.5. IMPLEMENTING SHIFT INVARIANT FILTERS 293 An important variant on (7.48) is to use the exact transfer function ^h(˘) for the \flter (which is often known) instead of the sample Fourier transform, ^hs(˘); this gives h \u0003 x(l˝ ) ˇ 1 294 CHAPTER 7. FILTERS Notice that the summation in this formula is quite similar to (7.50), the exponential mul- tipliers have been replaced by their complex conjugates. This means that a fast algorithm for computing FN automatically provides a fast algorithm for computing F −1 N : De\fnition 7.5.2. Let (x0;::: ;xN −1) be a sequence of length N: Its N -periodic extension is de\fned by xj+lN = xj for 0 \u0014 j \u0014 N − 1;l 2Z: Periodic sequences can be convolved. De\fnition 7.5.3. Given two N -periodic sequences, (xj) N −1 0 ; (yj) N −1 0 their periodic convo- lution is de\fned by (x? y)k = N −1X j=0 xjyk−j: For example, the 0th component of x? y is (x? y)0 = N −1X i=0 xiy−i: Observe that[x? yk = 1 7.5. IMPLEMENTING SHIFT INVARIANT FILTERS 295 7.5.3 Approximation of Fourier coeﬃcients Let f be a function on [0; 1]; de\fne xj = f (j=N );j =0;::: ;N − 1: The sequence (^xj) denotes its \fnite Fourier transform. It is reasonable to expect that ^xk is an approximation for the kth Fourier coe\u000ecient of f; as it is de\fned by a Riemann sum for the integral: ^f (k)= 1Z 0 f (x)e −2ˇikxdx ˇ 1 296 CHAPTER 7. FILTERS –1 –0.5 0 0.5 1 12 34 5 6 x Figure 7.18: Bad interpolation using formula 7.55. usually gives poor results. Figure 7.18 shows the result of using formula (7.55) to interpolate sin(3x) using 11 sample points. Using the formula (7.54) with this many sample points gives an exact reconstruction. To summarize: the output of the \fnite Fourier transform, when indexed by frequency, should be interpreted to be (^x0; ^x1;::: ; ^x N−1 7.5. IMPLEMENTING SHIFT INVARIANT FILTERS 297 to ^f (k) obtained by sampling the integrands to approximate the following integrals ^f (k)= Z 1 0 f (x)e −2ˇikxdx; ^f (k)= 1 298 CHAPTER 7. FILTERS 7.5.5 Implementing ﬁlters on ﬁnitely sampled data We use the pieces developed so far to explain how the \fnite Fourier transform is used to approximate a shift invariant \flter using \fnitely sampled data. The inversion formula, (7.51) de\fnes the sequence, (xk) for all values of k as an N -periodic sequence, xk = xk+lN : Once the \fnite Fourier transform is used to approximate a \flter the data must be regarded as samples of a periodic function rather than a function with bounded support on the real line. This section treats the details of using the \fnite Fourier transform to approximately implement a shift invariant \flter on \fnitely sampled data. Let f be a function de\fned onRwith support in [0; 1]: Let h be the impulse response of a \flter H so that Hf (x)= h \u0003 f (x): In general h is not assumed to have support in [0; 1]: Suppose that f is sampled at the points f j 7.5. IMPLEMENTING SHIFT INVARIANT FILTERS 299 Thinking of f and h as being de\fned onRwith f e\u000bectively bandlimited to [−Nˇ; N ˇ] leads to h \u0003 f ( k 300 CHAPTER 7. FILTERS 1)˝ ]: The discussion is section 7.5.3 shows that \fnite Fourier transform of hs is given by ^hs(k)= 1 7.5. IMPLEMENTING SHIFT INVARIANT FILTERS 301 7.5.6 Zero padding reconsidered Padding the sequence of samples of f with N − 1 zeros is a purely mechanical requirement for using the \fnite Fourier transform to evaluate the discrete convolution: the \fnite sum in (7.60) must be seen as a periodic convolution of two sequences of equal length. There is also an analytic interpretation for zero padding. For each positive integer m de\fne the function fm(x)= ( f (x)for x 2 [0; 1]; 0for x 2 (1;m]: Let ^f (˘)= 1Z −1 f (x)e −ix˘dx be the Fourier transform of f thought of as a function de\fned on the whole real line, supported in [0; 1]: Fix a sample spacing ˝> 0 in the time domain and collect Nm = m 302 CHAPTER 7. FILTERS 7.5.7 Higher dimensional ﬁlters Similar considerations apply to implement shift invariant, linear \flters acting on inputs which depend on more than one variable. As in the one dimensional case the convolution is usually computed using a Fourier representation. As there is no essential di\u000berence between the 2-dimensional and n-dimensional cases we consider the general case. Boldface letters are used to denote points inRn ; e.g. t =(t1;::: ;tn); x =(x1;::: ;xn) etc. Suppose that f (t) is an input, with bounded support, depending continuously on n real variables. A uniform sample set inRn is speci\fed by a vector of positive numbers h =(h1;::: ;hn); whose coordinates are the sample spacings in the corresponding coordinate directions. In one dimension the sample points and samples are labeled by an integer, in n dimensions it is more convenient to use n-tuples of integers. The integer vector j = (j1;::: ;jn) 2Zn labels the sample point xj =(j1h1;::: ;jnhn) and the sample fj = f (xj): As it should cause no confusion, sets labeled by such integer vectors are usually called sequences. Riemann sum approximations Let a(t) denote the impulse response of a shift invariant \flter, A acting on a function of n variables, A f (t)= ZRn f (s)a(t − s)ds: An n-dimensional integral is computed by re-writing it as iterated, 1-dimensional integrals, A f (t)= 1Z −1 \u0001\u0001\u0001 1Z −1 f (s1;::: ;sn)a(t1 − s1;::: ;tn − sn)ds1 \u0001\u0001\u0001 dsn: The iterated integrals can, in turn be approximated by Riemann sums. Using the uniformly spaced samples de\fned by h to de\fne a partition, the integral is approximated by A f (t) ˇ 1X j1=−1 \u0001\u0001\u0001 1X jn=−1 f (j1h1;::: ;jnhn)a(t1 − j1h1;::: ;tn − jnhn)h1 \u0001\u0001\u0001 hn: At sample points this can be re-written using the more economical notation, introduced above, as A f (xk) ˇ h1 \u0001\u0001\u0001 hn X j2Zn fjak−j: (7.68) 7.5. IMPLEMENTING SHIFT INVARIANT FILTERS 303 In the sequel the sample of the output A f (xj) is denoted by (A f )j: Because the input, f is assumed to have bounded support, these sums can be replaced by \fnite sums. By translating the coordinates it can be assumed that fj is only non-zero for j belonging to the set JM = fj :1 \u0014 ji \u0014 Mi;i =1;::: ;ng; (7.69) here M =(M1;::: ;Mn): Altogether there are M1 \u0001\u0001\u0001 Mn potentially non-zero samples. The Riemann sum for (A f )k becomes (A f )k ˇ [h1 \u0001\u0001\u0001 hn] M1X j1=1 \u0001\u0001\u0001 MnX jn=1 fjak−j: (7.70) If ak is non-zero for most values of k and the numbers fMjg are powers of 2 then this is most e\u000eciently computed using the Fourier representation. To compute the sum in (7.70), for all indices which satisfy (7.69), requires a knowledge of ak for all indices in the set J2M = fj :1 − Mi \u0014 ki \u0014 Mi;i =1;::: ;ng: (7.71) To use the Fourier transform to compute (7.70) the set of samples ffjg; de\fned for j 2JM must be augmented so that fj is de\fned for all indices in J2M: As f is assumed to vanish outside the sample set, this is done by adding the samples ffj =0 for j 2J2M nJMg: As in one dimension, this is called zero padding.In n-dimensions this amounts to adding about (2n − 1)M1 \u0001\u0001\u0001 Mn zero samples. The \fnite Fourier transform The n-dimensional \fnite Fourier transform is de\fned by iterating the one dimensional transform. Suppose that fj is a collection of numbers parametrized by the set of indices JN = fj :0 \u0014 ji \u0014 Ni − 1;i =1;::: ;ng: (7.72) The \fnite Fourier transform of (fj) is the collection of numbers ( ^fk) de\fned by ^fk = 1 304 CHAPTER 7. FILTERS Suppose that (aj)and (bj) are sequences parametrized by JN: These sequences can be extended to all ofZn be requiring that they be periodic, of period Ni in the ith index. A convolution operation is de\fned for such periodic sequences by setting a?bk = X j2JN ajbk−j: As in the one dimensional case, convolution is intimately connected to the \fnite Fourier transform: FN(a?b)k = N1 \u0001\u0001\u0001 Nn h^a \u0001 ^bi : (7.75) Here ^a \u0001 ^b is the sequence whose kth-element is the ordinary product ^ak^bk: This relation is the basis for computing convolutions using the Fourier transform, a? b = N1 \u0001\u0001\u0001 NnF −1 N h^ak \u0001 ^bki (7.76) Exercise 7.5.6. If l 6= m show that F 1 l F 1 m = F 1 mF 1 l : The n-dimensional \fnite Fourier transform can be computed as an iterated sum. In the notation introduced above FN(fj)= F 1 n \u000e \u0001\u0001\u0001 \u000e F 1 1 (fj) (7.77) Exercise 7.5.7. If M is a power of 2 then the length M; \fnite Fourier transform can be computed using O(M log2 M ) arithmetic operations. Show that if each Mj;j =1;::: ;n is a power of 2 then the \\length (M1;::: ;Mn);\" n-dimensional, \fnite Fourier transform can be computed using O(M1 log2 M1 \u0001\u0001\u0001 Mn log2 Mn) arithmetic operations. Can you give a better estimate? Exercise 7.5.8. Using (7.77) \fnd a formula for the inverse of the n-dimensional, \fnite Fourier transform. Exercise 7.5.9. Prove the identity (7.75). The Fourier representation for shift invariant \flters If the sequence (fj) is obtained as uniformly spaced samples of a function then the \fnite Fourier transform has an interpretation as an approximation to the Fourier transform of f: Let h denote the sample spacing and M =(M1;::: ;Mn) the number of samples in each coordinate direction. The sample set and its Fourier transform are parametrized by JM: Because ^fk is periodic, it is also possible to parametrize it using the indices J 0 M = fk : − 1 − Mi 7.5. IMPLEMENTING SHIFT INVARIANT FILTERS 305 The e\u000bective bandwidth of the sample set, [− ˇ 306 CHAPTER 7. FILTERS 7.5.8 Appendix: The Fast Fourier Transform If N =2q then there is a very e\u000ecient way to compute the \fnite Fourier transform of a sequence of length N: The fast algorithm for the \fnite Fourier transform is the Cooley- Tukey or fast Fourier transform algorithm, usually referred to as the \\FFT.\" Let \u0010 = e 2πi 7.5. IMPLEMENTING SHIFT INVARIANT FILTERS 307 From the de\fnition of CN ,the (2k +1)st and (2k +2)th rows of C2N are given by (2k +1) st :(1; \u0016\u00162k 1 ; \u0001\u0001\u0001 ; \u0016\u00162k 2N −1) (2k +2) th :(1; \u0016\u00162k+1 1 ; \u0001\u0001\u0001 ; \u0016\u00162k+1 2N −1) Comparing these with the kth row of CN and using the relations in (7.82), rows of C2N can be expressed in terms of the rows of CN as follows: (2k +1) st :(1; \u0016\u00162k 1 ; \u0001\u0001\u0001 ; \u0016\u00162k N −1; \u0016\u00162k N ; \u0001\u0001\u0001 ; \u0016\u00162k 2N −1)= (1; \u0016\u0010 k 1 ; \u0001\u0001\u0001 ; \u0016\u0010 k N −1; 1; \u0016\u0010 k 1 ; \u0001\u0001\u0001 ; \u0016\u0010 k N −1) (7.83) and, (2k +2) th :(1; \u0016\u00162k+1 1 ; \u0001\u0001\u0001 ; \u0016\u00162k+1 N −1 ; \u0016\u00162k+1 N ; \u0001\u0001\u0001 ; \u0016\u00162k+1 2N −1) =(1; \u0016\u0010 k 1 \u0016\u00161; \u0016\u0010 k 2 \u0016\u00162 \u0001\u0001\u0001 ; \u0016\u0010 k N −1 \u0016\u0016N −1; \u0016\u0016N ; \u0016\u0010 k 1 \u0016\u0016N +1 \u0001\u0001\u0001 ; \u0016\u0010 k N −1 \u0016\u00162N −1) In terms matrices, C2N is essentially obtained by multiplying 2 copies of CN by another very simple matrix: C2N = C # N UN : De\fne the 2N \u0002 2N -matrix C # N = 0 B B B B B @ r1 0 0 r1 ... ... rN 0 0 rN 1 C C C C C A ; where the frig are the rows of CN and the vector 0 =(0;::: ; 0 | 308 CHAPTER 7. FILTERS Theorem 7.5.1. If N =2q then CN = E1E2 \u0001\u0001\u0001 Eq where each row of the N \u0002 N matrix Ei has 2 nonzero entries. It is not di\u000ecult to determine exactly which entries in each row of the matrices Ej are non-zero. For an arbitrary N -vector v =(v1;::: ;vN ), the computation of Ejv can be done using exactly N (2multiplications + 1addition): Using this factorization and the knowledge of which entries of the Ej are non-zero one can reduce the number of operations needed to compute the matrix product CN v to 3qN =3N log2 N: Indeed the combinatorial structures of the matrices, fEjg are quite simple and this has led to very e\u000ecient implementations of this algorithm. Each column of Ej also has exactly two non-zero entries and therefore the factorization of CN gives a factorization of C \u0003 N : C \u0003 N = E\u0003 q E\u0003 q−1 ::: E\u0003 1 : Example 7.5.2. For example we can factor the matrix 2C4 as 4C4 = 0 B B @ 1100 0011 1 −10 0 001 −1 1 C C A 0 B B @ 10 1 0 01 0 1 10 −10 0 −i 0 i 1 C C A : (7.84) For a more complete discussion of the fast Fourier transform see [53]. 7.6 Image processing 7.6. IMAGE PROCESSING 309 ? (a) An \\image.\" ? (b) Its Fourier transform. Figure 7.19: The Fourier transform of an image is not usually an image. 7.6.1 Basic concepts and operations We \frst introduce basic image processing operations and examples of \flters which im- plement them. Many image processing operations are non-linear. For simplicity we con- sider planar images which are represented by scalar functions of two variables. The value, f (x1;x2); represents the \\grey level\" or density of the image at \\(x1;x2):\" With this inter- pretation it is reasonable to assume that f assumes only non-negative values. Very similar considerations apply to \\color images\" which are usually represented by a triple of functions [r(x1;x2);g(x1;x2);b(x1;x2)]: These functions represent the intensities of three \\indepen- dent colors\" at (x1;x2): Higher dimensional images are treated using similar methods. A visual output device, such as a monitor, is required to pass from a functional descrip- tion of an image, i.e. f to a picture, in the ordinary sense. Such a device has a \fxed size and dynamic range. Mappings must be \fxed between the coordinates used in the parameteriza- tion of f and coordinates in the output device as well as between the values f assumes and grey levels (or colors) available in the output. When it is necessary to distinguish between these two coordinate systems we speak of the \\measurement plane\" and the \\image plane.\" Generally speaking f is de\fned on the measurement plane and the output device is the identi\fed with the image plane. Sometimes the image plane refers to the original image itself. When this distinction is not needed we implicitly identify the image plane with the measurement plane. In the \frst part of this section we consider \fltering operations de\fned on functions of continuous variables, at the end we brieﬂy discuss the problems of sampling continuous images and implementation of the \flters. The basic operations of image processing fall into several classes: Coordinate transformations Images are sometimes distorted because of systematic modeling (or measurement) errors. In this connection it is useful to make a distinction between the \\image plane\" and the \\measurement plane.\" In this paragraph the image plane is the plane in which the image lies; let (y1;y2) denote orthogonal coordinates in this plane. This means that if f (y1;y2)is 310 CHAPTER 7. FILTERS displayed as a grey level image in the y1y2-plane then the image appears undistorted. The measurement plane refers to the coordinates de\fned by the apparatus used to measure the image; let (x1;x2) denote coordinates in this plane. Suppose that f (x1;x2); for (x1;x2) lying in a subset D ofR2 ; are measurements of an image. The parameters (x1;x2)in the measurement plane may di\u000ber from the coordinates in the image plane. Displaying f (x1;x2)inthe x1x2-plane would then result in a distorted image. This is called geometric distortion. The following example illustrates this point. Example 7.6.1. Suppose that the measuring device is calibrated in polar coordinates with (x1;x2) corresponding to the point in the image plane with Cartesian coordinates y1 = x1 cos x2;y2 = x1 sin x2: (7.85) Displaying f in the x1x2-plane would result in a distorted image, see \fgure 7.20(a). De\fne a \flter Apr with (Apr f )(y1;y2)= f ( q 7.6. IMAGE PROCESSING 311 de\fned for (y1;y2) belonging to a subset, Di in the image plane. This de\fnes a mapping \b from Di to Dm =\b(Di); a subset of the measurement plane. Let f (x1;x2)for (x1;x2) 2 D denote the measurements of an image; then (A\b f )(y1;y2)= f \u000e \b(y1;y2)= f (g(y1;y2);h(y1;y2)) (7.86) de\fnes a \flter which maps the portion of the measured image lying over Dm \\ D to an image de\fned in the corresponding part of Di: As with the example of polar coordinates, this transformation may not be de\fned in the the entire image plane and there may be choices involved in the de\fnition of the map \b : (y1;y2) 7! (g(y1;y2);h(y1;y2)): This operation de\fnes a linear \flter which is usually not translation invariant. The kernel function for A\b is the generalized function a\b(y1;y2; x1;x2)= \u000e(x1 − g(y1;y2);x2 − h(y1;y2)); here \u000e is the two-dimensional delta function. Let g(y1;y2);h(y1;y2) be a pair of functions de\fned in a subset D ˆR2 : Setting \b(y1;y2)= (g(y1;y2);h(y1;y2)) de\fnes a map of D into a subset D0 =\b(D)ofR2 : Whether or not \b is a change of coor- dinates, formula (7.86) de\fnes a linear \flter carrying functions de\fned on D0 to functions de\fned on D: Exercise 7.6.1. Find conditions on (g; h) which imply that A\b is translation invariant. Exercise 7.6.2. Suppose that the image is a transparency lying in the y1y2-plane, f (y1;y2) describes the amount of incident light transmitted through the point (y1;y2): Suppose that the measurement is made by projecting the transparency onto a screen which lies in the plane y2 = y3 using a light source which produces light rays orthogonal to the y1y2-plane. (1). Using (x1;x2)= (y1; p 312 CHAPTER 7. FILTERS Noise reduction Images can be corrupted by noise. There are two main types of noise: uniform noise and binary noise. In the \frst case the noise is uniformly distributed and locally of mean zero, whereas binary noise consists of sparsely, but randomly, distributed large errors. It is often caused by sampling or transmission errors. In this section techniques are described for reducing the a\u000bects of uniform noise; binary noise is discussed in the section 7.6.2. Because uniform noise is locally of mean zero, replacing f by a weighted average generally has the e\u000bect of reducing the noise content. A shift invariant \flter of this type is de\fned by convolution with a weight function '(x1;x2) which satis\fes the conditions '(x1;x2) \u0015 0; ZR2 'dx1dx2 =1: (7.87) Let A' f (x1;x2)= ' \u0003 f (x1;x2) be the convolution \flter de\fned by ': The \frst condition ensures that a non-negative function is always carried to a non-negative functions while the second ensures that a constant function is mapped to a constant function. If ' has support in a small ball then the output of A' f at (x1;x2) only depends on values of f (y1;y2) for (y1;y2); near to (x1;x2) and the \flter acts in a localized manner. If the noise has a directional dependence then this can be incorporated into ': If, for example, the noise is isotropic then it is reasonable to use a radial function. The frequency representation of such a \flter is A' f = F −1 h ^' ^f i : (7.88) As ' is integrable, its Fourier transform tends to zero as k˘k tends to in\fnity. This means that A' is an approximate low pass \flter. If ^'(˘1;˘2) is a function satisfying the conditions ^'(0; 0) = 1; lim ˘!1 ^'(˘)=0; (7.89) then (7.88) de\fnes an approximate low pass \flter. Even if the inverse Fourier transform, ' assumes negative values, the e\u000bect of A' is to reduce uniform noise. In this generality there can be problems interpreting the output as representing an image. The \fltered output, A' f may assume negative values, even if f is pointwise positive; to represent A' f as image it is necessary to remap its range to the allowable range of densities. This operation is discussed below in the paragraph on contrast enhancement. If ^' vanishes outside a bounded set then ' cannot have bounded support, see Proposition 3.2.10. From the representation of A' f as a convolution, it follows that the value of A' f at a point (x1;x2) depends on values of f at points distant from (x1;x2): As observed above, reduction of the available resolution is an undesirable side e\u000bect of low pass \fltering. In an image this appears as blurring. Using statistical properties of the noise an \\optimal \flter\" can be designed which provides an balance between noise reduction and blurring. This is discussed in Chapter 11. Noise might be locally uniform, 7.6. IMAGE PROCESSING 313 but non-uniform across the image plane. In this case a non-shift invariant \flter might do a better job reducing the a\u000bects of noise. A heuristic, used in image processing to retain detail in a \fltered image, is to represent an image as a linear combination of the low pass \fltered output and the original image. That is, instead of using either A' f or f alone we use \u0016 A' f +(1 − \u0016)f; (7.90) where 0 \u0014 \u0016 \u0014 1: If the noise is non-uniform across the image plane then \u0016 could be taken to depend on (x1;x2): Sharpening An image may be blurred during acquisition, it is sometimes possible to \flter the image and recapture some of the \fne detail. In essence this is an inverse \fltering operation. The measured image is modeled as A f where f denotes the \\original\" image and A models the measurement process. If A is a shift invariant, linear \flter then the methods discussed in section 7.3 can be applied to try to approximately invert A and restore details present in f which were \\lost\" in the measurement process. In general this leads to an ampli\fcation of the high frequencies which can, in turn, exacerbate problems with noise. The \fne detail in an image is also \\high frequency information.\" A slightly di\u000berent approach to the de-blurring problem is to simply remove, or attenuate the low frequency information. In the frequency domain representation, such a \flter is given by A' f = F −1 h ^' ^f i where, instead of 7.89, ^' satis\fes ^'(0) = 0; lim ˘!1 ^'(˘˘˘)=1: (7.91) If the function ^' − 1 has an inverse Fourier transform, then this \flter has spatial repre- sentation as A' f = f − \u0003 f: In other words A' is the di\u000berence between the identity \flter and an approximate low pass \flter, hence it is an approximate high pass \flter. Edge detection Objects in an image are delimited by their edges. Edge detection is the separation of the boundaries of objects from other, more slowly varying features of an image. The rate at which a smooth function varies in direction ! is measured by its directional derivative D!f (x)= d 314 CHAPTER 7. FILTERS is the gradient of f: The Euclidean length of rf provides an isotropic measure of the variation of f; i.e. it is equally sensitive to variation in all directions. Thus points where krf k is large should correspond to edges. An approach to separating the edges from other features is to set a threshold, tedge so that points with krf (x)k >tedge are considered to belong to edges. A \fltered image, showing only the edges, would then be represented by A1 f (x)= ˜(tedge;1)(krf (x)k): (7.92) While this is a non-linear \flter, it is shift invariant and the computation of rf can be done very e\u000eciently using the Fourier transform. This approach is not robust as the gradient of f is also large in highly textured or noisy regions. If D is a region in the plane with a smooth boundary and f = ˜D then rf (x)= 0 if x =2 bD: From the point of view of sampling, it may be quite di\u000ecult to detect such a sharply de\fned edge. These problems can be handled by smoothing f before computing its gradient. Let ' denote a smooth function, with small support satisfying (7.87). In regions with a lot of texture or noise, but no boundaries, the gradient of f varies \\randomly\" so that cancelation in the integral de\fning r(' \u0003 f )= ' \u0003rf should lead to a relatively small result. On the other hand, along an edge, the gradient of f is dominated by a component in the direction orthogonal to the edge. Therefore the weighted average, '\u0003rf should also have a large component in that direction. Convolution smears the sharp edge in ˜D over a small region and therefore points where kr(' \u0003 ˜D)k is large are more likely to show up in a sample set. This give a second approach to edge detection implemented by the \flter A2 f (x)= ˜(tedge;1)(kr(' \u0003 f )(x)k): (7.93) Once again f 7! r' \u0003 f is a linear shift invariant \flter which can be computed e\u000eciently using the the Fourier transform. In order not to introduce a preference for edges in certain directions, a radial function should be used to do the averaging. The second \flter helps \fnd edges in regions with noise or texture but may miss edges where the adjoining grey levels are very close. In this case it might be useful to compare the size of the gradient to its average over a small region. In this approach the krf k (or perhaps kr' \u0003 f k) is computed and is then convolved with a second averaging function to give \u0003krf k (or \u0003kr' \u0003 f k). A point x then belongs to an edge if the ratio krf (x)k 7.6. IMAGE PROCESSING 315 There is a related though somewhat more complicated approach to edge detection which entails the use of the Laplace operator, \u0001f = @2 x1f + @2 x2f as a way to measure the local variability of f: There are three reasons for this approach: (1) The Laplace operator is rotationally invariant. (2) The singularities of the function \u0001f are the same as those of f: (3) There is some evidence that animal optical tracts use a \fltering operation of this sort to detect edges. The \frst statement is easily seen in the Fourier representation: F(\u0001f )(˘1;˘2)= −(˘2 1 + ˘2 2) ^f (˘1;˘2): The second property requires more advanced techniques to prove, see [17]. The point of (2) is that the sharp edges of objects are discontinuities of the density function and will therefore remain discontinuities of \u0001f: For the reasons discussed above, the Laplace operator is often combined with a Gaussian smoothing operation, G˙f (x)= ZZR2 e − jx−yj2 316 CHAPTER 7. FILTERS It not immediately obvious how to use the output of A˙ to locate edges. This is clari\fed by considering the special case of a sharp edge. Applying A˙ to the function ˜[0;1)(x1) gives A˙ ˜[0;1)(x1;x2)= 2cx1 7.6. IMAGE PROCESSING 317 Contrast enhancement The equipment used to display an image has a \fxed dynamic range. Suppose that the grey values that can be displayed are parametrized by the interval [dmin;dmax]: To output an image, a mapping needs to be \fxed from the range of f to [dmin;dmax]: That is, the values of f need to be scaled to \ft the dynamic range of the output device. Suppose that f assumesvaluesinthe interval [m; M ]: Ordinarily, the scaling map is a monotone map γ :[m; M ] ! [dmin;dmax]: This map is usually non-linear and needs to be adapted to the equipment being used. By choosing γ carefully, di\u000berent aspects of the image can be emphasized or enhanced. Suppose that there is a region R in the image where f varies over the range [a; A]: If A − a is very small compared to M − m then a linear scaling function, γ(t)= dmax t − m 318 CHAPTER 7. FILTERS 7.6.2 Discretized images In the previous section we considered an image to be a real valued function of a pair of continuous variables. As was the case with one dimensional signals, functions describing images are usually sampled on a discrete set of points and quantized to take values in a \fnite set. In this section we brieﬂy discuss sampling of images and implementation of \flters on the sampled data. As before we restrict our attention to two dimensional images. The sample set in imaging is usually a uniform rectangular grid, f(jh1;kh2): j; k 2Zg; where h1 and h2 are positive numbers. To simplify the discussion, we do not consider questions connected with quantization and work instead with the full range of real numbers. Suppose that the function f (x1;x2) describing the image is supported in the unit square, [0; 1] \u0002 [0; 1]: If the image has bounded support then this can always be arranged by scaling. The actual measurements of the image consist of a \fnite collection of samples, collected on a uniform rectangular grid. Let M and N denote the number of samples in the x1 and x2 directions respectively. The simplest model for measurements of an image is the set of samples fjk = f ( j 7.6. IMAGE PROCESSING 319 sort of averaging and therefore the measurement process itself incorporates a low pass \flter. A shift invariant measurement process is modeled as samples of a convolution '\u0003f: In either case the samples are the result of evaluating some function. For the remainder of this section we consider the sample set to be the values of a function at the sample points. Suppose that ffjkg are samples of an image. To use these samples to reconstruct an image, the image plane is divided into rectangles. Each rectangle is called a picture element of pixel.The jkth pixel, denoted pjk is the rectangle pjk = f(x1;x2): j 320 CHAPTER 7. FILTERS \u000f De\fne (A\b f )lm as a weighted average: (A\b f )lm = jlmX q=1 wlm q f (x lm q ): A de\fnition of \\nearest neighbors\" and a scheme for assigning weights is needed to imple- ment this procedure. Exercise 7.6.5. Explain how the \frst procedure used to de\fne (A\b f )lm can be interpreted as an interpolation scheme. Linear, shift invariant \flters Suppose that a(x1;x2) is the impulse response of a linear shift invariant, two dimensional \flter A : Its action in the continuous domain is given by A f (x1;x2)= ZZR2 a(x1 − y1;x2 − y1)f (y1;y2)dy1dy2: A Riemann sum approximation for this \flter, at sample points is A f ( j 7.6. IMAGE PROCESSING 321 Example 7.6.2. A standard method to reduce uniform noise in a uniformly sampled image is to average the \\nearest neighbors.\" A simple example of such a \flter is (Saf )jk = 1 322 CHAPTER 7. FILTERS Example 7.6.5. The Laplace operator provides an example of this phenomenon. Let \u0001s denote the \fnite di\u000berence approximation to \u0001 de\fned in example 7.6.4. In the Fourier representation (\u0001sf )jk =(F −1 2 ^dmn \u0001 ^fmn)jk; where F −1 2 is the inverse of the 2-dimensional \fnite Fourier transform and ^dmn are the Fourier coe\u000ecients of \u0001s thought of as an (N +2) \u0002 (N + 2)-periodic matrix. A simple calculation shows that ^dmn =(2N 2)[cos \u0012 2ˇl 7.7. GENERAL LINEAR FILTERS\u0003 323 7.7 General linear ﬁlters\u0003 324 CHAPTER 7. FILTERS If ft1;::: ;tN g are the sample points then A x(tj) ˇ N −1X k=1 a(tj;tk)x(tk)(tk+1 − tk): If the kernel function is a generalized function then it must \frst be approximated by an ordinary function before it is sampled. This calculation can only be done e\u000eciently (faster than O(N 2)) if the kernel function has special structure. An approximate inverse can be found by solving the system of linear equations NX k=1 a(tj;tk)x(tk)(tk+1 − tk)= y(tj): If a \fnite matrix arises from approximating a linear transformation of a function space then, at least for large N; the properties of the approximating transformations mirror those of the in\fnite dimensional operator. The theory of numerically solving systems of linear equations is a very highly developed \feld in applied mathematics, see [19] or [78]. Exercise 7.7.1. Let fa0(t);::: ;an(t)g be non-constant functions onR: The di\u000berential operator Df = nX j=0 aj(t) djf 7.8. LINEAR FILTER ANALYSIS OF IMAGING HARDWARE\u0003 325 r r f(r) g(r ) detector r s_1 source plane object ) plane s_2 θ R plane plane h(r Figure 7.25: Arrangement of an imaging device with a source, object and detector. \\most\" of the energy in the data lies. It is clear that, even with perfect measurements, one cannot expect to \\resolve\" objects that are smaller than the sample spacing. In this section we use linear \fltering theory to analyze the distortion of the data that results from using real, physical measuring devices. These are limitations which are present in the measurements before any attempt is made to process the data and reconstruct the image. In this section we use geometric optics to model X-rays as a diverging ﬂux of particles in much the same spirit as in section 2.2. This discussion is adapted from [4] where the reader can \fnd, inter alia a careful discussion of γ-ray detectors, X-ray sources and collimators. 7.8.1 The transfer function of the scanner In this section we consider a three dimensional situation, beginning with the simple setup in \fgure 7.25. A source of radiation is lying in the source plane with a distribution f (r), i.e. f (r)dr is the number of photons per unit time emitted by an area on the source of size dr located at position r: The source output is assumed to be independent of time. In a parallel plane, at distance s1 from the source plane is an object which is described by a transmittance function g(r0): The fraction of the incident photons transmitted by an area dr0 located at the point r0 in the object plane is given by g(r0)dr0: Usually g(r0)takes values between 0 and 1: It is sometimes useful to think of g as the probability that a photon incident at r0 will be transmitted. The object plane is usually thought of as a thin slice of a three dimensional object. If the width of the slice is \u000f then transmittance is related to absorption coe\u000ecient in Beer's law by g(r 0)= 1 − exp[− Z \u000f 0 \u0016ds]: Here \u0016 is an absorption coe\u000ecient and the integral is along the line perpendicular to object plane through the point r0: Finally a detector lies in a second parallel plane, at distance s2 from the object plane. For the moment assume that the detector is perfect, i.e. everything incident on the detector is measured. Later on a more realistic detector will be incorporated into the model. To analyze the source-object-detector geometry, \frst assume that the object is transparent, that is g(r0)= 1: 326 CHAPTER 7. FILTERS cos θ cos θ r r’’ θ θ A B= A B R R Figure 7.26: Computing the solid angle. It is convenient to use a di\u000berent systems of coordinates in each plane, r; r0; r00: As above dr;dr0 and dr00 denote the corresponding area elements. A point source is isotropic if the ﬂux through an area A on the sphere of radius ˆ; centered on the source, is proportional to the ratio of A to the area of the whole sphere 4ˇˆ2: The constant of proportionality is the intensity of the source. The solid angle Ω subtended by a region D; relative to a point p is de\fned by projecting the region onto the sphere of radius 1 centered at p; along lines through the center of the sphere. If D0 is the projected image of D then the solid angle it subtends is de\fned to be Ω = area of the region D0: From the de\fnition it is clear that the solid angle assumes values between 0 and 4ˇ: To \fnd the ﬂux through a region, D due to a planar distribution of isotropic sources with density f (r) we need to \fnd the contribution of each in\fnitesimal area element dr: If Ω(D; r) is the solid angle subtended at r by D then the contribution of the area element centered at r to the ﬂux through D is f (r)drΩ(D; r)=4ˇ: In our apparatus we are measuring the area element on a plane parallel to the source plane. As shown in \fgure 7.26, the solid angle subtended at a point r in the source plane by an in\fnitesimal area dr00 at r00 in the detector plane is cos \u0012dr00 7.8. LINEAR FILTER ANALYSIS OF IMAGING HARDWARE\u0003 327 r r r r- r r- r R s_1 s_2 Figure 7.27: The similar triangle calculation. Now we include the e\u000bect of an absorbing material. The measured ﬂux at r00 is an integral along the source plane given by h(r 00)= 1 328 CHAPTER 7. FILTERS 0 s_1 s_2 Figure 7.28: A pinhole camera. Now, the measurement is expressed as h(r 00)= ( a 7.8. LINEAR FILTER ANALYSIS OF IMAGING HARDWARE\u0003 329 7.8.2 The resolution of an imaging system We now estimate the resolution of the transmission imaging system described in section 7.8.1 using the FWHM criterion. We consider the problem of resolving two spots in the object plane. The source is assumed to have constant intensity over a disk of diameter dfs;in this case f is the characteristic function of a disk f (r)= ˜D1(2jrj=dfs): In this section D1 denotes the disk of radius 1 centered at (0; 0): The source is projected onto the detector plane, passing it through an object composed of two identical opaque spots separated by some distance. Mathematically it is equivalent to think of the object as being entirely opaque but for two disks of perfect transmittance separated by the same distance. It is clear that the outputs of the two con\fgurations di\u000ber by a constant. For the calculation the object is modeled as the sum of two \u000e-functions, g(r 0)= \u000e(r 0 − r 0 1)+ \u000e(r 0 − r 0 2): Of course this is not a function taking values between 0 and 1: This situation is approxi- mated by a transmittance given by g\u000f = ˜D1( jr0 − r0 1j 330 CHAPTER 7. FILTERS d fs d fsb/a o s_1 s_2 o Figure 7.29: The image of two dots. If s2 = 0 then \u000e0 fs = 0. If the object is sitting on the detector then no matter how close the two point sources are, they can be distinguished. As s1 ! 0, \u000e0 fs ! dfs; so the resolution is better for objects closer to the detector. This simple model is completed by including a detector response function. Suppose that k(r00) is the impulse response of the detector. The complete imaging system with source f; transmittance g and detector response k is given by h(r 00)= \u0010 a 7.8. LINEAR FILTER ANALYSIS OF IMAGING HARDWARE\u0003 331 source X−ray detector 1 P1 object P2 P3 Figure 7.30: Beam spreading. The parameter \u0015 = s1+s2 332 CHAPTER 7. FILTERS PQ rs L b D b 0 (a) D b (b) Figure 7.31: The geometry of a collimator called the bore diameter and bore length, see \fgure 7.31. Often collimators are used in arrays. In this case, each collimator is better modeled as narrow pipe made of X-ray absorbing material. In our discussion it is assumed that only photons which pass through the collimator bore reach the detector. This is also a three dimensional analysis. The collimator is circularly symmetric about its axis, which implies that the response to a point source depends only on the distance, z from the point source to the front of the collimator and the distance, rs from the source to the axis of the collimator. The impulse response of a collimator can be deduced from the the analysis in sec- tion 7.8.1. The e\u000bect of the collimator on a X-ray beam is identical to the e\u000bect of two concentric pinholes of diameter Db; cut into perfectly absorbing plates, lying in parallel planes a distance Lb apart. Let f (r) model the X-ray source, g1(r0) the lower pinhole and g2(r00) the upper pinhole. From section 7.8.1, we have that the photon ﬂux incident on the upper plate is h(r 00)= 1 7.8. LINEAR FILTER ANALYSIS OF IMAGING HARDWARE\u0003 333 L b D b P z (a) sr D b Q /2 z+ Lb/2 Lb/2 (b) Figure 7.32: Evaluating the point spread function of a collimator. For the sort of collimator considered here, g2 is the same: g2(r 00)= ˜D1 \u0012 2jr00j 334 CHAPTER 7. FILTERS Lb Lb M M/2 FWHM = +2zDb/2 πD /4b 2 2 D b Figure 7.33: The graph of p(rs; z); for a \fxed z: As z increases, the resolution gets worse, as observed in section 7.8.2. If the object is sitting right on the collimator face, i.e. z = 0 then \u000e(0) = Db which is just the diameter of the collimator bore. The other parameter of interest for a detector is its sensitivity. The sensitivity of a detector is important as it determines how intense a source is needed to make a usable image. To analyze the sensitivity we use a uniform planar source, instead of the point source used in the resolution analysis. Imagine that a uniform radioactive source is spread on a plane at distance z from the face of the collimator. The intensity of the \\standard source\" is 1\u0016C= cm2 : For a typical radioactive material, this translates into a photon ﬂux of 3:7 \u0002 104= cm2 sec. The sensitivity of the measuring device is given by the number of photons arriving at the detector. For the collimator described above we obtain S = 3:7 \u0002 104 7.8. LINEAR FILTER ANALYSIS OF IMAGING HARDWARE\u0003 335 Exercise 7.8.4. What is the e\u000bect of the collimator in the previous exercise if Db1 <Db2? How about if Db1 >Db2? 336 CHAPTER 7. FILTERS Chapter 8 Reconstruction in X-ray tomography 338 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY L inR3 : If Ω 2 S2 is the direction of L and x0 is a point on L then the line is given parametrically by L = fsΩ+ x0 : s 2Rg: Let I(s) denote the intensity of the photon beam at the point sΩ+ x0; Beer's law states that dI 339 As a practical matter, a larger cross section increases the energy in the beam which, in turn, improves the signal-to-noise ratio in the measurements. On the other hand, poorer spatial resolution is also a consequence of a larger cross section. In our initial discussion of the reconstruction algorithms we model the measurements as line integrals of a slice, fc(x1;x2)= \u0016(x1;x2;c): In other words we assume that the X-ray beam is one dimensional. A linear model for the e\u000bect of a three dimensional beam is to replace these line integrals by weighted averages of such integrals. As usual, averaging with an integrable weight is a form of low pass \fltering. This is very important because it reduces the e\u000bects of aliasing which result from sampling. These e\u000bects are easily analyzed using properties of the Radon transform and are considered in sections 8.5- 8.6. For the bulk of this chapter the third dimension is rarely mentioned explicitly. x 1 x 2 x 3 C d= slice thickness Beam profile a Figure 8.1: A 3-dimensional X-ray beam. Let (x; y) denote Cartesian coordinates in the slice x3 = c; which is heretofore \fxed. The two dimensional object we would like to image lies in DL; the disk of radius L; centered at (0; 0) in this plane. In most of this chapter the X-ray source is assumed to be monochromatic of energy E and the object is described by its X-ray absorption coe\u000ecient f at this energy. As the object lies in DL;f is assumed to vanish outside this set. Our goal is to use samples of Rf to approximately determine the values of f on a uniform reconstruction grid, R˝ = f(xj ;yk)=(j˝; k˝ ): j; k 2Zg; in the (x; y)-plane. Here ˝> 0 denotes the sample spacing. It is selected to reﬂect the resolution available in the data. The reconstruction grid can also be thought of as dividing the plane into a grid of squares of side ˝: Each square is called a pixel. As each slice is 340 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY really a three dimensional slab, and hence each square is really a cuboid, the elements in the reconstruction grid are often called voxels. The value reconstructed at a point (xj;yk) 2R˝ should be thought of as a weighted average of f over the voxel containing this point. Of course f is only reconstructed at points of R˝ lying in [−L; L] \u0002 [−L; L] ˙ DL: We assume that f is bounded, and regular enough for its Radon transform to be sampled. As the actual measurements involve averaging \u0016 with a continuous function, this is not a restrictive assumption. voxel pixel 2d τ Figure 8.2: The reconstruction grid. In most of this section it is assumed that measurements are made with in\fnite precision and that the full set of real numbers are available. Real measurements have errors and are quantized to \ft into a \fnite number of bits. The accuracy and precision of the measurements are determined by the accuracy and sensitivity of the detectors and the stability of the X-ray source. The number of bits used in the quantization of the measurements is the ultimate mathematical constraint on the contrast available in the reconstructed image. This numerical resolution reﬂects the accuracy of the measurements themselves and the computations used to process them. It should not be confused with spatial resolution which is a function of the sample spacing and the point spread function of the reconstruction algorithm. A detailed discussion of X-ray sources and detectors can be found in [86] or [4]. 8.1 Reconstruction formulæ We now review the inversion formulˆ derived earlier and consider how they might be ap- proximated in a real computation. These formulˆ are consequences of the Fourier inversion formula and central slice theorem, relating Rf to ^f; fRf (r; !)= 1Z −1 Rf (t; !)e −irtdt = ^f (r!): 8.1. RECONSTRUCTION FORMULÆ 341 Writing the Fourier inversion formula in polar coordinates gives f (x; y)= 1 342 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY derivations of the reconstruction algorithms we overlook this point, the e\u000bects of the \fnite beam width are incorporated afterwards. A good reconstruction algorithm is characterized by accuracy, stability and e\u000eciency. An accurate algorithm is often found by starting with an exact inversion formula and making judicious approximations. In CT, stability is the result of low pass \fltering and the continuity properties of the exact inversion formula. Whether an algorithm can be implemented e\u000eciently depends on its overall structure as well as the hardware available to do the work. An X-ray CT-machine is not a general purpose computer, but rather a highly specialized machine designed to do one type of computation quickly and accurately. Such a machine may have many processors, allowing certain parts of the algorithm to be \\parallelized.\" As we shall soon see, the measuring hardware naturally divides the measurements into a collection of views. The most e\u000ecient algorithms allow the data from a view to be processed as soon as it is collected. 8.2 Scanner geometries The structure of a reconstruction algorithm is dictated by which samples of Rf are available. Before discussing algorithms we therefore need to consider what kind of measurements are actually made. In broad terms there are two types of CT-machine: (a) Parallel beam scanner, see \fgure 8.3(a). (b) Divergent beam scanner, see \fgure 8.4. The earliest scanner was a parallel beam scanner. This case is considered \frst because the geometry and algorithms are simpler to describe. Because the data can be collected much faster, most modern machines are divergent or fan beam scanners. Algorithms for these machines are a bit more involved and are treated later. In a parallel beam scanner approximate samples of Rf are measured in a \fnite set of directions, f!(k\u0001\u0012)for k =0 ::: ;M g where \u0001\u0012 = ˇ 8.2. SCANNER GEOMETRIES 343 Detectors X−ray source L−L θ d (a) A parallel beam scanner. -L L t ωπ0 (b) The parallel beam sam- ple space. Figure 8.3: A parallel beam scanner and sample set. Parallel beam data therefore consists of the samples fRf (jd; !(k\u0001\u0012));j = −N;::: ;N; k =0;::: ;M g: (8.7) Because of the symmetry of the Radon transform, Rf (−t; −!)= Rf (t; !); (8.8) measurements are only required for angles lying in [0;ˇ): Sometimes it is useful to make measurements over a full 360\u000e; the extra measurements can then be averaged as a way to reduce the e\u000bects of noise and systematic errors. The individual measurements are often called rays.A view, for a parallel beam machine, consists of the measurements of Rf (t; !) for a \fxed !: These are the integrals of f along the collection of equally spaced parallel lines fljd;!(k\u0001\u0012);j = −N;::: ;N g: In (t; \u0012)-space, parallel beam data consists of equally spaced samples on the vertical lines shown in \fgure 8.3(b). The other type of scanner in common use is called a divergent beam or fan beam scanner. A point source of X-rays is moved around a circle centered on the object being measured. The source is pulsed at a discrete sequence of angles, measurements of Rf are collected for a \fnite family of lines passing through the source. In a machine of this type, data is collected by detectors which are usually placed on a circular arc. There are two di\u000berent designs for fan beam machines which, for some purposes, need to be distinguished. In a third generation machine the detectors are placed on a circular arc centered on the source. The detectors and the source are rotated together. In a fourth generation machine the 344 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY detectors are on a \fxed ring, centered on the object. Only the source is rotated, again on a circle centered on the object, within the ring of detectors. These designs are shown schematically in \fgure 8.4. For a fan beam machine machine it is useful to single out the central ray.For a third generation machine this is the line that passes through the source and the center of rotation. The central ray is well de\fned no matter where the source is positioned. Since all the rays that are sampled pass through a source position, the natural angular parameter, for this geometry is the angle, ˚ between a given ray and the central ray. Suppose that the source is at distance R and the central ray makes an angle with the positive x-axis. The a\u000ene parameter for the line passing through the source, making an angle ˚ with the central ray, is given by t = R sin(˚); (8.9) see \fgure 8.5(a). In a third generation machine the source is placed at a \fnite number of equally spaced angles k 2f 2ˇk 8.2. SCANNER GEOMETRIES 345 source R t t=Rsin (ϕ) L−L detectors ϕ θ ψ (a) A third generation scanner. \u0000 \u0000 \u0000 \u0001 \u0001 \u0001 \u0000 \u0000 \u0001 \u0001 \u0000 \u0000 \u0001 \u0001 \u0000\u0000 \u0000\u0000 \u0001\u0001 \u0001\u0001 \u0000 \u0000 \u0001 \u0001 \u0000 \u0000 \u0000 \u0001 \u0001 \u0001 \u0000\u0000 \u0000\u0000 \u0000\u0000 \u0001\u0001 \u0001\u0001 \u0001\u0001 \u0000 \u0000 \u0001 \u0001 R Detector circle Source circle Central ray ϕ ψ L (b) A fourth generation scanner. Figure 8.4: The two di\u000berent divergent beam geometries. 346 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY For a fourth generation machine the source moves and the detectors remain \fxed. Each time the source emits a pulse of X-rays, a sample, can in principle be collected by any detector which can \\see\" the source. In practice, data is collected for rays which pass through the target area, as shown in \fgure 8.4(b). In this way the data is grouped into views according to the detector position not the source position. For many purposes third and fourth generation machines can be treated together. Indeed we can still use the coordinates in \fgure 8.5(a) to parametrize the data, with the following small changes: the detectors now lie on the circle of radius R and denotes the angle from the positive x-axis to the \\central ray,\" which is now a line joining (0; 0) toa\fxed detector.The parameter ˚ measures the angle between lines through a detector and its central ray. With this understood, the sample set and data collected are essentially the same for third and fourth generation machines. A view for a fourth generation machine consists of all the rays passing through a given detector. At a basic level, third and fourth generation scanners are quite similar, though they di\u000ber in many subtle points. These di\u000berences are considered in greater detail in section 8.6.4. In the engineering and medical literature one sometimes sees the raw measurements represented as density plots in (t; !)or( ;˚)-space. Such a diagram is called a sinogram. These are di\u000ecult for a human observer to directly interpret, however as they contain all the information available in the data set, they may be preferable for machine based assessments. ? Figure 8.6: An example of a sinogram. Exercise 8.2.1. For a third generation, fan beam machine, the set of angles f˚kg could be selected so that the sample spacing in the a\u000ene parameter is constant. To get a sample spacing d what set of angles f˚kg should be used? 8.3. RECONSTRUCTION ALGORITHMS FOR A PARALLEL BEAM MACHINE 347 8.3 Reconstruction algorithms for a parallel beam machine 348 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY For realistic data this is a very large number. The size of this computation can be vastly reduced by using the fast Fourier transform algorithm. The two dimensional FFT is only usable if samples of ^f are known on a uniform grid in a rectangular coordinate system. Our data f ^f (rj!(k\u0001\u0012))g are ^f sampled on a uniform grid in a polar coordinate system. To use the 2-dimensional FFT, the data must \frst be interpolated to get simulated measurements on a uniform, rectangular grid. Using nearest neighbor, linear interpolation, the amount of computation required to do this is a mod- est O(K 2) calculations. Assuming that K is a power of 2; the FFT leading to ~f (xj;yk) would require O((K log2 K)2) calculations. The full reconstruction of ~f; at the grid points, from the parallel beam data would therefore require O(K 2(log2 K)2 +2MN log2 N )com- putations. The 2M log2 N term comes from using the 1-dimensional FFT to compute < ^f (rj!(k\u0001\u0012)) > from < Rf (jd; !(k\u0001\u0012)) >: For realistic values of M and N this is a much smaller number than what was computed above for a direct inversion of the Fourier transform. Indeed, this algorithm is the fastest algorithm for implementation on a general purpose computer. However, with real data, simple linear interpolation leads to unac- ceptably large errors in the approximation of ^f: This is due in part to the fact that ^f is complex valued and the extreme sensitivity of the reconstructed image to errors in the phase of the Fourier transform, see 7.1.4. A more sophisticated interpolation scheme is needed. A technique of this sort is presented in section 8.7. Exercise 8.3.1. Explain how to use zero padding to obtain a approximation to ^f (r!)on a \fner grid in the r-variable. Is this justi\fed in the present instance? Is there any way to reduce the sample spacing in the angular direction? 8.3.2 Filtered backprojection Formula (8.6) organizes the approximate inversion in a di\u000berent way. The Radon transform is \frst \fltered G Rf (t; !)= −iH@t Rf (t; !); and then backprojected to \fnd f at (x; y). The operation Rf 7! G Rf is a 1-dimensional, linear shift invariant \flter. On a parallel beam scanner, the data for a given ! de\fnes a single view and is collected with the source-detector array in a \fxed position. Once the data from a view has been collected it can be \fltered. In this way, a large part of the processing is done by the time all the data for a slice has been collected. Supposing, as before, that sampling only occurs in the angular variable, the data set for a parallel beam scanner would be the samples fRf (t; !(k\u0001\u0012)) : k =0;::: ;M g: In a \fltered backprojection algorithm each view, Rf (t; !(k\u0001\u0012)) is \fltered immediately after it is measured, giving G Rf (t; !(k\u0001\u0012)): When all the data has been collected and \fltered, the image is approximately reconstructed by using a Riemann sum approximation to the backprojection: ~f (x; y)= 1 8.3. RECONSTRUCTION ALGORITHMS FOR A PARALLEL BEAM MACHINE 349 here use is made of the symmetry (8.15). Assuming that all the necessary values of G Rf are known, the backprojection step requires O(MK 2) operations to determine ~f on the reconstruction grid, R˝ : This step is also highly parallelizable: with O(K 2) processors the backprojection could be done simultaneously for all points in R˝ in O(M ) cycles. The serious work of implementing this algorithm is in deciding how to approximate the \flter G on data which is sampled in both t and !: In real applications the approximation to the impulse response of G is chosen to be the inverse Fourier transform of a function which satis\fes certain properties. Denote the approximate impulse response by ˚(t) and de\fne Q˚f (t; !)= 1 350 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY can be adjusted to achieve certain aims. In the imaging literature, ^˚ is often expressed as a product ^˚(r)= A(r)jrj: Here A(r) is a function which tends to zero as jrj!1; it is called an apodizing function. Remark 8.3.2 (Important notational remark.). In the sequel, the notation f˚ refers to an approximate reconstruction of f using an integral like that in (8.13) whereas ~f˚ refers to a discretization of this integral, as in (8.14). This family of reconstruction formulˆ has a very important feature which follows from thefactthatthe quantity h(x; y);!(k\u0001\u0012)i− jd is the signed distance from the line ljd;!(k\u0001\u0012) to the point (x; y) The algorithm de\fned by ˚ can be expressed as ~f˚(x; y)= d 8.3. RECONSTRUCTION ALGORITHMS FOR A PARALLEL BEAM MACHINE 351 this implies that ˚(\u000b(k +1)d +(1 − \u000b)(kd) − ld)= \u000b˚((k +1 − l)d)+(1 − \u000b)˚((k − l)d): (8.17) For each (xm;yl) 2R˝ and direction !(k\u0001\u0012) there is an integer nklm 2 [−N; N ] such that nklmd< h(xm;yl);!(k\u0001\u0012)i\u0014 (nklm +1)d: Thus there is a real number \u000bklm 2 [0; 1] so that h(xm;yl);!(k\u0001\u0012)i = \u000bklm(nklm +1)d +(1 − \u000bklm)nklmd: The trivial but crucial observation is that (8.17) implies that, for any integer l; h(xm;yl);!(k\u0001\u0012)i− ld = \u000bklm[(nklm +1)d − ld]+(1 − \u000bklm)[nklmd − ld]: (8.18) If ˚ is linearly interpolated between sample points then Q˚ ~f (h(xm;yl);!(k\u0001\u0012)i;!(k\u0001\u0012)) = \u000bklmQ˚ ~f ((nklm +1)d; !(k\u0001\u0012)) +(1 − \u000bklm)Q˚ ~f (nklmd; !(k\u0001\u0012)): (8.19) In other words Q˚ ~f (h(xm;yl);!(k\u0001\u0012)i;!(k\u0001\u0012)) is a weighted average of Q˚ ~f at sample points. In fact the interpolation step can be done as part of the backprojection ~f˚(xm;yl)= 1 352 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY backprojection is interpolation of a real valued function and is much less damaging than that used in the direct Fourier reconstruction. Empirically it leads to an overall blurring of the image but does not introduce complicated oscillatory artifacts or noise. Approximately MK 2 calculations are required once the values fQ˚ ~f (jd; !(k\u0001\u0012))g are computed. Using an FFT the computation of Q˚ requires about MN log2 N -steps and using a direct convolution about MN 2-steps. The FFT is clearly faster but the backprojection step already requires a comparable number of calculations to that needed for a direct convolution. Exercise 8.3.3. How should formula (8.14) be modi\fed if sampling is done around the full circle, i.e. \u0012 varies between 0 and 2ˇ? 8.3.4 Shepp-Logan analysis of the Ram-Lak ﬁlters 8.3. RECONSTRUCTION ALGORITHMS FOR A PARALLEL BEAM MACHINE 353 With this choice ^˚(˘)= \f \f \f \f 2 354 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY Graphs of Q˚fr are shown on \fgures 8.8-8.9 with r =1 and d = :1;:04;:02:; :005 For comparison, the exact \fltered function, − i 8.3. RECONSTRUCTION ALGORITHMS FOR A PARALLEL BEAM MACHINE 355 so that ˚(t)= ˇ 356 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY If I(t)= 1 8.3. RECONSTRUCTION ALGORITHMS FOR A PARALLEL BEAM MACHINE 357 2K+1 2K+1 −L L (a) The reconstruction grid. ∆θ π π π L d d ∆θ (b) Sampling in frequency space. Figure 8.10: How to choose sample spacings. Apriori each of these numbers could be chosen independently, however once one is \fxed, it is possible to determine reasonable values for the others. The data is e\u000bectively bandlimited and the sample spacings are chosen to reﬂect the essential bandwidth of the data. The sample spacing in the a\u000ene parameter is d = L 358 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY gorithm. This is complicated by the observation that the map from the \\input\" f to the output f ~f (xm;yl)g is not, in any reasonable sense, a shift invariant \flter and so it does not have a point spread function, per se. We return to this question after we have consid- ered various physical limitations of the measurement process. This problem is su\u000eciently complicated that a \fnal determination for the various parameters must be done empirically. Exercise 8.3.7. Explain why the e\u000bective bandwidth of a function f (x; y) and its Radon transform Rf (t; !); in the t variable, are the same. 8.4 Filtered backprojection in the fan-beam case We now consider reconstruction algorithms a fan beam scanner. A view for a parallel beam scanner consists of measurements of Rf for a family of parallel lines and therefore the central slice theorem applies to give an approximation to the Fourier transform of the attenuation coe\u000ecient along lines passing through the origin. A view, for either type of fan beam scanner, consists of samples of Rf for a family of lines passing through a point and so the central slice theorem is not directly applicable. There are two general approaches to reconstructing images from the data collected by a fan beam machine: 1. Re-sort and interpolate to obtain the data needed to apply the parallel beam algorithms discussed earlier, 2: Work with the fan geometry and \fnd algorithms well adapted to it. Herman, Lakshminarayanan and Naparstek \frst proposed an algorithm of the second type in [21]. Our derivation of this algorithm closely follows the presentation in [39]. 8.4.1 Fan beam geometry It is convenient to use a di\u000berent parameterization for the lines in the plane from that used earlier. As before (t; \u0012) denotes parameters for the line with oriented normal !(\u0012)at distance t from the origin. Consider the geometry shown in \fgure 8.11. Here S denotes the intersection point of the lines de\fning a view. It lies a distance D from the origin. The central ray (through S and (0; 0)) makes an angle \f with the positive y-axis. The other lines through S are parametrized by the angle, γ they make with the central ray. These are the natural fan beam coordinates on the space of oriented lines, they are related to the (t; \u0012)variables by \u0012 = γ + \f and t = D sin γ: (8.24) 8.4. FILTERED BACKPROJECTION IN THE FAN-BEAM CASE 359 S θ t D γ β θ= γ+β Figure 8.11: Fan beam geometry. We now derive the continuous form of the approximate reconstruction formula used in fan beam algorithms. This is the analogue of the formula, f˚(x; y)= 1 360 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY β- L -L D γ γ L L (a) Physical parameters in a fan beam scanner. γ ϕ,β)r,(l β D S (x,y) ϕ β−ϕ r (b) Variables for the reconstruction formula. Figure 8.12: Quantities used in the fan beam, \fltered backprojection algorithm. The function f is supported in the disk of radius L: The limits of integration in the γ- integral, \u0006γL are chosen so that the lines corresponding to the parameters f(\f; γ): \f 2 [0; 2ˇ); −γL \u0014 γ \u0014 γLg include all those intersecting DL: The data actually collected with a fan beam machine is an approximation to uniformly spaced samples in the (\f; γ)-coordinates. To simplify the notation we introduce Pf (\f; γ) d =Rf (D sin γ; \f + γ): In terms of this data the formula reads f\u0014(r; ')= 1 8.4. FILTERED BACKPROJECTION IN THE FAN-BEAM CASE 361 As functions of (r; '; \f); the distance, l and the angle γ0 are given by: l(r; '; \f)= p 362 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY The function, h(γ) has compact support and therefore the order of integration can be interchanged. As an iterated integral, H(γ0)= 1 8.4. FILTERED BACKPROJECTION IN THE FAN-BEAM CASE 363 8.4.3 Implementing the fan beam algorithm Using (8.32) we can describe a reasonable algorithm for reconstructing an image from fan beam data which is well adapted to the geometry of this type of scanner. The fan beam data is Pf (\fj;n\u000b)where \fj = 2ˇj 364 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY Example 8.4.1. Using the \flter function de\fned by Shepp and Logan for g(n\u000b)gives gSL(n\u000b)= ( 2 8.4. FILTERED BACKPROJECTION IN THE FAN-BEAM CASE 365 In fan beam coordinates this is equivalent to Pf (\f; γ)= Pf (\f +2γ + ˇ; −γ): Sampling Pf on the range \f 2 [0;ˇ]; and − γL \u0014 γ \u0014 γL and using t = D sin γ; and \u0012 = \f + γ; gives the diagram, in (t; \u0012) space, shown in \fgure 8.13(b). The numbers show how many times a given projection is measured for (\f; γ)in this range. Some points are measured once, some twice and some not at all. β2 β 1 γ1 -D D γ2 (c) Symmetry in (γ; \f)- coordinates. γ L −γ π−γ π+γ L L L 1 t θ 0 2 0 2 (d) Data collection in (t; \u0012)-coordinates. −γ L γ L 1 1 β 2 2 γ (e) Data collection in (γ; \f)-coordinates. Figure 8.13: Collecting data for fan beam scanners. In order to gather a complete data set it is necessary for \f go from 0 to ˇ +2γL: Of course even more values are now sampled twice. The algorithm above can be used with measurements gathered over such a range, however care must be taken to count each projection exactly once. This can be done by multiplying the projection data by a windowing function. For example one could use w\f(γ)= ( 00 \u0014 \f \u0014 2γL +2γ; 1 otherwise : As usual a sharp cuto\u000b produces its own artifacts so it is preferable to use a smoother window. The window should be continuous and satisfy the conditions (a) w\f1(γ1)+ w\f2(γ2)=1; for pairs (\f1;γ1); (\f2;γ2) satisfying (8.35), (b) w\f(γ) \u0015 0: 366 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY Better results are obtained if w has continuous \frst derivatives. Given the known regularity of the data, a bounded though not necessarily continuous, \frst derivative should usually su\u000ece. An example is given by w\f(γ)= 8 >>< >>: sin 2 h ˇ\f 8.5. THE EFFECT OF A FINITE WIDTH X-RAY BEAM 367 Here w(u) is weighting function. It models the distribution of energy across the X-ray beam as well as the detector used to make the measurements. This function is sometimes called the beam pro\fle. The relationship between Rf and RW is a consequence of the convolution theorem for the Radon transform. In the imaging literature it is due to Shepp and Logan. Theorem 8.5.1 (Shepp and Logan). The weighted Radon transform RW f (t; \u0012) is the Radon transform of f k(x; y)= 2ˇZ 0 1Z 0 f (x − ˆ cos \u000b; y − ˆ sin \u000b)k(ˆ)ˆdˆd\u000b where k(ˆ)= − 1 368 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY and therefore the \fnite strip width leads to low pass \fltering of Rf in the a\u000ene parameter. This has the desirable e\u000bect of reducing the aliasing artifacts that result from sampling in the t-parameter. In X-ray tomography this is essentially the only way to low pass \flter the data before it is sampled. Of course this averaging process also leads to a loss of resolution; so the properties of the averaging function w need to be matched with the sample spacing. As we saw in section 7.3 the e\u000bects of such averaging, can to some extent be removed, nonetheless algorithms are often evaluated in terms of their ability to reconstruct samples of f \u0003 k rather than f itself. As mentioned above the beam pro\fle models both the source and detector. If I(u) describes the intensity of the X-ray beam incident on the detector then the output of the detector is modeled as 1Z −1 wd(u)I(u)du: Suppose that that ws(u) models the source; if the source and detector are \fxed, relative to one another then the combined e\u000bect of this source-detector pair would be modeled by the product w(u)= ws(u)wd(u): This is the geometry in a third generation, fan beam scanner. In some parallel beam scanners and fourth generation scanners the detectors are \fxed and the source moves. In this case the model for the source detector pair is the convolution w = ws \u0003 wd: The detector is often modeled by a simple function like w1 or w2 de\fned above while the source is sometimes described by a Gaussian ce − u2 8.5. THE EFFECT OF A FINITE WIDTH X-RAY BEAM 369 To derive this expression we use the Taylor expansions: e −x =1 − x + x2 370 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY8.5. THE EFFECT OF A FINITE WIDTH X-RAY BEAM 371 µ 0µ 1 x 1-x X-ray beam Figure 8.15: Rectangle with small inclusion 0 0.01 0.02 0.03 0.04 0.2 0.4 0.6 0.8 1x Figure 8.16: Relative errors with small inclusion reconstruction algorithms. That is, our algorithm assumes that what is collected are sam- ples of Rf k(t; !); because of the \fnite width of a real strip and the partial volume e\u000bect this is not so. Even if we could measure a projection for all relevant pairs (t; !) our algo- rithm would not reconstruct f k exactly but rather some further non-linear transformation applied to f: In real images the partial volume e\u000bect appears as abnormally bright spots near hard object or streaks. 8.5.3 Some mathematical remarks \u0003 In the foregoing sections we consider algorithms for approximately reconstructing an un- known density function f from \fnitely many samples of its Radon transform fRf (tj;!k)g: It is reasonable the enquire what is the \\best\" one can do in approximating f from such data and, if g is the \\optimal solution,\" then what does f − g \\look like.\" A considerable amount of work has been done on these two questions, we brieﬂy describe the results of Logan and Shepp, see [47] and [46]. Assume that f is an L2-function which is supported in the disk of radius 1: It is assumed that Rf (t; !j)isknown for n-distinct directions. Logan and Shepp examine the following problem: Find the function g 2 L2(D1) such that Rf (t; !j)= Rg(t; !j);j =0;::: ;n − 1; (8.38) 372 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY which minimizes Var(g)= Z D1 (g(x; y) − \u0016g) 2dxdy; where \u0016g = 1 8.6. THE PSF 373 8.6 The point spread function and linear artifacts 374 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY The problem with this approach is that it mixes artifacts caused by physical measurement errors with those caused by algorithmic errors. A very important innovation in medical imaging was introduced by Larry Shepp. In order to isolate the algorithmic errors he replaced the (physical) phantom with a mathematical phantom. Figure 8.17: A mathematical phantom. Instead of using real measurements of a known object, he suggested that one give a mathematical description of a phantom to create simulated and controlled data. In this way, algorithmic errors could be separated from measurement errors. A mathematical phantom is created as follows: \frst a simpli\fed model of a slice of the human head (or other object) is described as an arrangement of ellipses and polygons. Each region is then assigned a density or attenuation coe\u000ecient, see \fgure 8.17. Next the continuous model is digitized by superimposing a regular grid and replacing the piecewise continuous densities by their averaged values over the squares that make up the grid. Finally measurements are simulated by integrating the digitized model over a collection of strips, arranged to model some particular measurement apparatus. One can incorporate di\u000berent sorts of measurement errors, e.g. noise, beam hardening, patient motion, etc. into the simulated measurements to test the robustness of an algorithm to di\u000berent sorts of measurement errors. The main point is that by using mathematical phantoms you know, apriori, exactly what is being measured and can therefore compare the reconstructed image to a known, exact model. Mathematical phantoms are very useful in the study of artifacts caused by sampling errors and noise. Exercise 8.6.1. Derive (8.40) by using the family of functions '\u000f(x; y)= 1 8.6. THE PSF 375 measurement is a sample of RW f (t; !)= 1Z −1 w(u)Rf (t − u; !): If \\all\" the data fRW f (t; !): t 2 [−L; L];! 2 S1g were available, then the \fltered backprojection reconstruction, with \flter function ˚; would be f˚;w(x; y)=(R\u0003Q˚ RW f )(x; y): Recall that R\u0003 is the backprojection operation and Q˚g(t; !)= 1Z −1 g(t − s; !)˚(s)ds = 1 376 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY therefore Ψ0(x; y)= 1 8.6. THE PSF 377 If \u000f = 0 (no regularizing function) or \u000e = 0 (1-dimensional X-ray beam) then the integrals de\fning Ψ0 exists as an improper Riemann integrals, Ψ0(\u000e;0)(ˆ)= 1 378 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY 0 10 20 30 40 50 0.1 0.2 0.3 0.4 0.5 0.6 x (a) Several PSFs. 0 0.2 0.4 0.6 0.8 1 10 20 30 40 50 60 r (b) And their MTFs Figure 8.20: Examples of PSF and MTF with exponential regularization. Example 8.6.3. As a \fnal example we consider the Shepp-Logan \flter. The regularizing \flter has (one dimensional) transfer function ^˚(r)= jrj \f \f \f \fsinc \u0012 dr 8.6. THE PSF 379 0 2 4 6 8 10 12 0.2 0.4 0.6 0.8 1x (a) Several PSFs. 0 0.2 0.4 0.6 0.8 1 10 20 30 40 50 60 r (b) And their MTFs Figure 8.21: Examples of PSF and MTF with Shepp-Logan regularization. It is apparent in the graphs of the PSFs with exponential and Shepp-Logan regulariza- tion that these functions do not have long oscillatory tails and so the e\u000bect of convolving a piecewise continuous, bounded function with Ψ0 should be an overall blurring, without oscillatory artifacts. They are absent because the MTF decays smoothly and su\u000eciently rapidly to zero. The PSFs obtained using a sharp cut-o\u000b in frequency have long oscillatory tails which, in turn produce serious Gibbs artifacts in the reconstructed images. Oscillatory artifacts can also result from sampling. This is considered in the following section. From both (8.46) and (8.47) it is clear that the roles of the beam width function w(t)and the regularizing function (t) are entirely interchangeable in the total, unsampled PSF. This is no longer the case after sampling is done in the t-parameter. These examples all indicate that, once the beam width is \fxed, the full width half max- imum of the PSF is not very sensitive to the sample spacing. However, smaller sample spacing produces a sharper peak which should in turn lead to less blurring in the recon- structed image. From the limiting case shown in \fgure 8.19(a) it is clear that the resolution is ultimately limited by the beam width. Since the PSF tends to in\fnity the FWHM de\fni- tion of resolution is not applicable. Half of the volume under the PSF (as a radial function onR2 ) lies in the disk of radius d=2; indicating that the maximum available resolution, with the given beam pro\fle is about half the width of the beam. This is in good agreement with experimental results that show that having several samples per beam width leads to a better reconstruction, though little improvement is seen beyond 4 samples per beam width, see [56] or [38]. To measure the resolution of a CT-machine or reconstruction algorithm it is customary to use a \\resolution phantom.\" This is an array of disks of various sizes with various spacings. An example is shown in \fgure 8.22. 380 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY? (a) A resolution phantom.? (b) Its reconstruction using a fan beam algorithm. Figure 8.22: Resolution phantoms are used to gauge the resolution of a CT-machine or reconstruction algorithm. Exercise 8.6.2. Using the formula for R\u000e(a;b) derive the alternate expression for Ψ0(x; y): Ψ0(x; y)= 1 8.6. THE PSF 381 8.6.2 The PSF with sampling (a) A head phantom. (b) Reconstruction using par- allel beam data. (c) Reconstruction using fan beam data. Figure 8.23: Reconstructions of a mathematical phantom using \fltered backprojection algorithms. Real measurements entail both ray and view sampling. For a parallel beam machine, ray sampling refers to sampling in the t parameter and view sampling to the ! (or \u0012) param- eter. For the sake of simplicity these e\u000bects are usually considered separately. We follow this procedure, \frst \fnding the kernel function incorporating ray sampling and then view sampling. Each produces distinct artifacts in the reconstructed image. As ray sampling is not a shift invariant operation, the measurement and reconstruction process can no longer be described by a single PSF, but instead requires a di\u000berent integrand for each point in 382 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY the reconstruction grid. For the purpose of comparison, the PSFs are often evaluated at the center of reconstruction grid (i.e. (0; 0)), though it is also interesting to understand how certain artifacts depend on the location of the input. In the previous section we ob- tained the PSF for unsampled data, with a reasonable \flter function it was seen to produce an overall blurring of the image, without oscillatory e\u000bects. Both aliasing artifacts and the Gibbs phenomenon are consequences of slow decay in the Fourier transform which is typical of functions that change abruptly. One therefore would expect to see a lot of os- cillatory artifacts produced by inputs with sharp edges. To test algorithms one typically uses the characteristic functions of disks or polygons placed at various locations in the im- age. Figure 8.23 shows a mathematical phantom and its reconstructions made with \fltered backprojection algorithms. Note the oscillatory artifacts parallel to sharp boundaries as well as the pattern of oscillations in the exterior region. Ray sampling Suppose that d is the sample spacing in the t-parameter and that the image is reconstructed using a Ram-Lak (linearly interpolated) \flter. For the purposes of this paragraph we suppose that sampling is only done in the a\u000ene parameter so that fRW f (jd; !): j = −N;::: ;N g is collected for all ! 2 S1: With ˚ the Ram-Lak \flter, the reconstructed image is ~f˚;w(x; y)= 1 8.6. THE PSF 383 To evaluate the last sum we use the dual Poisson summation formula, (6.12), obtaining d 1X j=−1 w(jd −h(a; b);!i)e −ijdr = e −ih(a;b);r!i 1X j=−1 ^w(r + 2ˇj 384 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY At issue here is the relationship between d; the sample spacing and the \\width\" of the w: Colloquially one asks for the \\number of samples per beam width.\" With the given parameters, the FWHM(w) is very close to 2: The overall \fltering operation is no longer shift invariant. The function \b(x; y;0; 0) is a radial, \fgure (8.24) shows this function (of ˆ) with various choices for d; with and without the e\u000bects of ray sampling. The dotted line is the unaliased PSF and the solid line the aliased. As before, smaller values of d give rise to sharper peaks in the PSF. The corresponding MTFs are shown in \fgure 8.24(d). 0 0.05 0.1 0.15 0.2 1234x (a) d =2 0 0.05 0.1 0.15 0.2 1234 5 6 x (b) d =1 0 0.05 0.1 0.15 0.2 1234 5 6 x (c) d = :25 0.2 0.4 0.6 0.8 1 –8 –6 –4 –2 2 4 6 8 r (d) The corresponding MTFs. Figure 8.24: The e\u000bect of ray sampling on the PSF. The graphs on the right hand side include the e\u000bect of aliasing while those on the left are the unaliased MTFs, as d decreases, the \\passband\" of the MTF broadens. With this choice of beam pro\fle and regularizing \flter, once there is at least one sample per beam width, the resolution, measured by FWHM, is not a\u000bected very much by aliasing. Though it is not evident in the pictures, these PSFs have long oscillatory tails. The very 8.6. THE PSF 385 small amplitude of these tails is a result of using a smooth, rapidly decaying regularizing function. Exercise 8.6.5. When doing numerical computations of \b it can be very useful to use the fact that d 1X j=−1 w(jd −h(a; b);!i)e −ijdr is a periodic function. Explain this observation and describe how it might \fgure in a practi- cal computation. It might be helpful to try to compute this sum using both representations. Exercise 8.6.6. Continue the computation begun in example 8.6.4 and draw graphs of \b(x; y; a; b)for some (a; b) 6=(0; 0): Note that \b is no longer a radial function of (x; y): 8.6.3 View sampling We now turn to the artifacts which result from using \fnitely many views and begin by considering the reconstruction of a mathematical phantom made out of constant density elliptical regions. In \fgure 8.25 note the pattern of oscillations in the exterior region along lines, tangent to the boundary of ellipse and the absence of such oscillations in the interior. A somewhat subtler observation is that the very pronounced, coherent pattern of oscillations does not begin immediately but rather at a de\fnite distance from the boundary of the ellipse. This phenomenon is a consequence of the sampling in the angular parameter and the \fltering operations needed to approximately invert the Radon transform. Our discussion of these examples closely follows that in [70]. Figure 8.25: Filtered backprojection reconstruction of elliptical phantom Example 8.6.5. Suppose the object, E is of constant density 1 with boundary the locus of points, x2 386 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY points js+(t; \u0012) − s−(t; \u0012)j is the Radon transform of f . Plugging the parametric form of the line into the equation for the ellipse and expanding gives s2( sin2 \u0012 8.6. THE PSF 387 Doing the exact \fltration, Qf (t; !(\u0012)) = 1 388 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY Figure 8.27: Filtered backprojection reconstruction of square phantom f(\u00061; \u00061)g then for j\u0012j < ˇ 8.6. THE PSF 389 where Q˚;wf (t; !)= 1Z −1 RW f (s; !)˚(t − s)ds: Here RW f denotes the \\averaged Radon transform\" of f: We now consider the e\u000bect of sampling in the !-parameter, leaving t as a continuous parameter. Equation (8.42) shows that, in this case, ˚ and w are interchangeable; the e\u000bects of \fnite beam width and regu- larizing the \flter are both captured by using ˚ \u0003 w as the \flter function. We analyze the di\u000berence between f˚;w(x; y) and the Riemann sum approximation ~f˚;w(x; y)= 1 390 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY the support of f \u0003 k is a small enlargement of the support of f itself. We henceforth assume that, for points of interest, f (a;b) ˚;w (x; y) ˇ 0 (8.54) and therefore any deviation of ~f (a;b) ˚;w (x; y) from zero is an error. If f is the characteristic function of a disk of radius r then Q˚;wf (t) falls o\u000b rapidly for jtj >> r; see example 8.3.1. There is a j0 so that j0\u0001\u0012 − '< ˇ 8.6. THE PSF 391 This explains why the oscillatory artifacts only appear at points the are at a de\fnite distance from the object: for small values of R the sum itself is very small however, for su\u000eciently large R the terms of the sum start to become signi\fcant. Suppose, for example that R\u0001\u0012 is such that all but the \frst term in this sum are negligible, then ~f (a;b) ˚;w (x; y) ˇ 1 392 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY Example 8.6.7. To understand formula (8.57) we consider (a; b)=(0; 0); and f (x; y)= ˜D 1 8.6. THE PSF 393 radially symmetric object. For a fan beam machine the pattern displays a similar circular symmetry but the center of the circle no longer agrees, in general, with the center of the object, see \fgure 8.30. ? (a) Parallel beam. ? (b) Fan beam. Figure 8.30: Examples comparing view aliasing in parallel beam and fan beam scanners. Exercise 8.6.9. Given that we use the Poisson summation formula, why is it still possible to use w(u)= (2\u000e)−1˜[−\u000e;\u000e](u) in this analysis. Exercise 8.6.10. Show that the PSF for the Ram-Lak reconstruction, incorporating the beam width function w and sampling in the !-parameter is \b(x; y; a; b)= \u0001\u0012 394 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY \u000f The \fltered backprojection algorithm is very sensitive to errors that vary abruptly from ray to ray or view to view and is relatively tolerant of errors that very gradually. This feature is a reﬂection of the fact that the \flter function, ^˚(˘) ˇj˘j; attenuates low frequencies and ampli\fes high frequencies. Our discussion is adapted from [71], [34] and [35]. For this analysis we suppose that the measurements, made with a parallel beam scanner, are the samples fP (tj;!(k\u0001\u0012));k =0;::: ;M;j =1;::: ;N g of RW f (t; !): The coordinates are normalized so that the object lies in [−1; 1] \u0002 [−1; 1]: The angular sample spacing is \u0001\u0012 = ˇ 8.6. THE PSF 395 ? (a) Shepp-Logan phantom. ? (b) A few bad rays. Figure 8.31: A reconstruction with a few bad rays. A bad ray in each view A bad ray might result from a momentary surge in the output of the X-ray tube. If, on the other hand, a single detector in the detector array is malfunctioning then the same ray in each view will be in error. Let \u000fk denote the error in P (tj0;!(k\u0001\u0012)): In light of the linearity of (8.58), the error at (x; y)is now \u0001 ~f˚(x; y)= MX k=0 \u000fk˚(x cos(k\u0001\u0012)+ y sin(k\u0001\u0012) − tj0) 396 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY reason the error is only large in half the circle is that samples are only collected for 0 \u0014 \u0012 \u0014 ˇ: If data is collected over the full circle then the result of an error \u000f in the jth 0 ray is approximately \u0001 ~f˚ ˇ \u000f 8.6. THE PSF 397 Remark 8.6.2. In a third generation machine a single bad detector would result in the situation we have just analyzed: the measurement of the same ray would be erroneous in every view. This is because a view, for a third generation machine, is determined by the source position. In a fourth generation scanner a mis-calibrated detector could instead result in every ray from a single view being in error. This is because a view is determined, in a fourth generation machine, by a detector. Exercise 8.6.11. Explain why the error is only large in a semi-circle if samples are only collected for 0 \u0014 \u0012 \u0014 ˇ: Exercise 8.6.12. Provide a more detailed explanation for the smallness of the errors in the half of the circle where ˇ< ' < 2ˇ: Abad view We now analyze the e\u000bect on the reconstruction of having an error in every measurement of a single view. For simplicity we consider the consequence of such an error using the parallel beam algorithm. Suppose that \u000fj is the error in the measurement P (tj;!(k0\u0001\u0012)); then the reconstruction error at (x; y)is \u0001 ~f˚(x; y)= 1 398 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY where \u000e is the distance between (x; y) and the central ray of the bad view. The integral from (−1+ \u000e)to (1 − \u000e) is approximated by 2 8.6. THE PSF 399 the total energy in the output. The energy content of the X-ray beam is described by its spectral function, S(E); it satis\fes Ii = 1Z 0 S(E)dE: If a (thin) X-ray beam is directed along the line lt;!; then the measured output is Io;(t;!) = 1Z 0 S(E)exp 2 4− 1Z −1 f (s^! + t!; E)ds 3 5 dE: Here f (x; y; E) is the attenuation coe\u000ecient, with its dependence on the energy explicitly noted. A typical spectral function is shown in \fgure 2.7. Due to this non-linear distortion, the raw measurements are not the Radon transform of f ; in the imaging literature it is often said that such measurements are inconsistent. Applying the Radon inversion formula to such data leads to streaking in the reconstructed image, see \fgure 8.34. ? Figure 8.34: Streaks caused by beam hardening. Suppose that D is a bounded object whose absorption coe\u000ecient, f (E) only depends on the energy. Even in this case, the function, log \u0014 Ii 400 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY Because S(E)and f (E) are non-negative functions it is immediate that Hf (T ) is a strictly, monotonely decreasing function. This implies that the inverse function, H −1 f is well de\fned. Thus by measuring or computing Hf (T ); for T in the relevant range, its inverse function can be tabulated. The absorption coe\u000ecient of water, fw(E)as wellas Hw(T ); for a typical spectral function, are shown in \fgure 8.35(a-b). ? (a) The absorption coe\u000ecient of wa- ter. ? (b) The non-linear absorption func- tion for water. Figure 8.35: Beam hardening through water. Using H −1 f the Radon transform of ˜D(x; y) can be determined from X-ray absorption measurements R˜D(t; !)= H −1 f \u0012 log \u0014 Ii 8.7. THE GRIDDING METHOD 401 Applying H −1 w to these measurements gives the Radon transform of ˆ; H −1 w \u0012log \u0014 Ii 402 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY operations. Samples of the ^f (˘) on a uniform rectangular grid allows the use a fast direct Fourier inversion to obtain an approximation to f in O(N 2 log2 N ) operations. Neither the parallel beam nor fan beam machine collects data which is easily converted to uniformly spaced samples of ^f: In this section we discuss the gridding method which is a family of e\u000ecient methods for passing from realistic data sets to uniformly spaced samples of ^f: These methods work in any number of dimensions, so we describe this method inRn for any n 2N: Our discussion follows the presentation in [68]. Let f denote a function de\fned onRn which is supported in a cube D =[−L; L] \u0002 \u0001\u0001\u0001 \u0002 n−times [−L; L] and ^f its Fourier transform. Suppose that ^w(˘) is a function with small support, such that w(x)doesnot vanish in D: The basis of the gridding method is the following observation: Suppose that we can e\u000eciently compute an approximation to the convolution ^g(˘k)= ^w \u0003 ^f (˘k)for ˘k on a uniformly spaced grid. The exact inverse Fourier transform of ^g equals w(x)f (x): If this too can be e\u000eciently computed on a uniformly spaced grid, then at grid points xj 2 D f (xj) ˇ g(xj ) 8.7. THE GRIDDING METHOD 403 We suppose that L \u0014 ˇ=d so that spatial aliasing does not occur if our sample spacing in the Fourier domain is d: Thesamplepoints in G are also indexed by Sunif with Sunif 3 k $ ˇ 404 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY Noise in the measurements is expected to have \\mean zero,\" however the larger the region one averages over, the more the noise from di\u000berent samples has the opportunity to cancel. For simple interpolation one uses a weight function ^w with K =1: For a larger value of K the computation of ^g involves averaging ^f over a larger region and therefore leads to greater suppression of the noise in the measurements. Two considerations limit how large K can be taken. The amount of computation needed to do the gridding step is O((KN )n): For example if n = 2 this is O(K 2N 2): The amount of computation needed for the \fltered backprojection algorithms is O(N 3): For the gridding method to remain competitive we see that K<< p 8.8. CONCLUDING REMARKS 405? (a) 1970s era brain section.? (b) 1990s era brain section. Figure 8.37: Mathematical analysis has led to enormous improvements in CT-images. Our discussion of X-ray CT applies to large parts of the reconstruction problems for any \\non-di\u000bracting\" imaging technique, e.g. positron emission tomography and magnetic resonance imaging. Much of the mathematical technology is also used in the study of \\di\u000bracting\" modalities such as ultrasound, impedence tomography and infrared imaging. The \\inverse problems\" for these latter modalities are essentially non-linear and to date they lack complete mathematical solutions. In consequence of this fact these modalities have not yet come close to attaining their full potential. It stands to reason that the \frst step in \fnding a good approximation is having a usable formula for the exact result. The challenge for tomorrow is to \\solve\" the mathematical reconstruction form, for these modalities. 406 CHAPTER 8. RECONSTRUCTION IN X-RAY TOMOGRAPHY Chapter 9 Algebraic reconstruction techniques Algebraic reconstruction techniques (ART) are techniques for reconstructing images which have no direct connection to the Radon inversion formula. Instead these methods make direct use of the fact that the measurement process is linear and therefore the reconstruction problem can be posed a system of linear equations. Indeed the underlying mathematical concepts of ART can be applied to approximately solve many types of large systems of linear equations. This chapter contains a very brief introduction to these ideas. An extensive discussion can be found in [24]. In this chapter boldface letters are used to denote vector or matrix quantities, while the entries of a vector or matrix are denoted in regular type with subscripts. For example r is amatrix, frig are its rows (which are vectors) and rij its entries (which are scalars). 9.1 Algebraic reconstruction The main features of the Radon transform of interest in ART are (1) the map f 7! Rf is linear, (2) for a function de\fning a simple object with bounded support, Rf has a geometric interpretation. The \frst step in an algebraic reconstruction technique is the choice of a \fnite collection of basis functions, fb1(x; y);::: ;bJ (x; y)g: Certain types of apriori knowledge about the expected data and the measurement process itself can be \\encoded\" in the choice of the basis functions. At a minimum, it is assumed that the absorption coe\u000ecients one is likely to encounter are \\well approximated\" by functions in the linear span of the basis functions. This means that for some choice of constants fxjg; the di\u000berence f (x; y) − JX j=1 xjbj(x; y); is small, in an appropriate sense. For medical imaging, it is reasonable to require that the \fnite sum approximate f in that the grey scale images that they de\fne look similar, see 407 408 CHAPTER 9. ALGEBRAIC RECONSTRUCTION TECHNIQUES j K+1 K r ray i ij 5431 K (a) Using 1-dimensional rays. r strip i j K+1 K ij 5431 K (b) Using 2-dimensional strips. Figure 9.1: Pixel basis section 7.6. It is also important that Rf (x; y) ˇ JX j=1 xj Rbj: A second criterion is to choose basis function for which Rbj can be e\u000eciently approximated. The pixel basis is a piecewise constant family of functions often used in ART. Suppose that the support of f lies in the square, [−1; 1]\u0002 [−1; 1]: The square is uniformly subdivided into a K \u0002 K grid. When using ART methods it is convenient to label the sub-squares sequentially one after another as in \fgure 9.1(a). The elements of the K \u0002 K pixel basis are de\fned by b K j (x; y)= ( 1if (x; y) 2 jth − square; 0 otherwise. If xj is the average of f in the jth-square then \u0016f K(x; y)= JX j=1 xjbK j provides an approximation to f in terms of the pixel basis. It is easy to see that the fbK j g are orthogonal with respect to the usual inner product on L2(R2 )and that \u0016f K is the orthogonal projection of f into the span of the fbK j g: For f a continuous function with bounded support the sequence < \u0016f K > converges uniformly to f as K !1: If f represents an image, in the usual sense of the word, then as K !1 the image de\fned by \u0016f K also converges to that de\fned by f: Because the Radon transform is linear R \u0016f K(x; y)= JX j=1 xj RbK j : 9.1. ALGEBRAIC RECONSTRUCTION 409 Another advantage of the pixel basis is that RbK j (t; !) is, in principle, very easy to compute, being simply the length of the intersection of lt;! with the jth square. This is a good basis to keep in mind, it has been used in many research papers on ART as well as in commercial applications. Expressing a function as a linear combination of basis functions is, in fact, the same process used in our earlier analysis of the Radon inversion formula. The only di\u000berence lies in the choice of basis functions. In ART methods one typically uses a localized basis like the fbK j g; where each function has support in a small set. For our analysis of the Radon transform we adopted the basis provided by the exponential functions, fei˘˘˘\u0001xg: These basis functions are well localized in frequency space but are not localized in physical space. The exponential basis is very useful because it diagonalizes the linear transformations used to invert the Radon transform. However it su\u000bers from artifacts like the Gibbs phenomenon which are a consequence of its non-localized, oscillatory nature. Wavelets bases are an attempt to strike a balance between these two extremes, they are localized in space but also have a fairly well de\fned frequency. A good treatment of wavelets can be found in [27]. Returning now to our description of ART, assume that fbjg is a localized basis, though not necessarily the pixel basis. As before the measurements are modeled as samples of Rf: The samples are labeled sequentially by i 2f1;::: ;Ig; with Rf sampled at f(t1;!1); (t2;!2);::: ; (tI ;!I )g: Unlike the \fltered backprojection algorithm, ART methods are insensitive to the precise nature of the data set. De\fne the measurement matrix by setting rij =Rbj(ti;!i);i =1;::: ;I: The measurement matrix models the result of applying the measurement process to the basis functions. De\fne the entries in the vector of measurements, p as pi =Rf (ti;!i);i =1;::: ;I: The reconstruction problem is now phrased as a system of I equations in J unknowns: 410 CHAPTER 9. ALGEBRAIC RECONSTRUCTION TECHNIQUES so that I ' 19; 000: That gives a 19; 000 \u0002 16; 000 system of equations. Even today, it is not practical, to solve a system of this size directly. Indeed, as is typical in ART, this is an overdetermined system, so it is unlikely to have an exact solution. Consulting \fgure (9.1) it is apparent that for each i there are about K values of j such that rij 6=0: A matrix with \\most\" of its entries equal to zero is called a sparse matrix. Since K ' p 9.2. KACZMARZ’ METHOD 411 Expanding the inner product gives t2hrv; rvi +2thrv; r~x − pi + hr~x − p; r~x − pi: Hence, the derivative at t = 0 vanishes if and only if 2hrv; r~x − pi =2hv; r t(r~x − p)i =0: Since v is an arbitrary vector it follows that r tr~x = r tp: (9.3) These are sometimes called the normal equations.If r has maximal rank then rtr is invert- ible, which implies that the minimizer is unique. One might consider solving this system of equations. However for realistic imaging data, it is about a 104 \u0002 104 system, which again is too large to solve directly. Moreover, the matrix rtr may fail to be sparse even though r is. Exercise 9.1.1. If f is a continuous function with bounded support show that \u0016f K converges uniformly to f as K !1: Exercise 9.1.2. Let f = ˜[−a;a](x)˜[−b;b](y): By examining \u0016f K show that there is no \\Gibbs phenomenon\" for the pixel basis. In what sense does \u0016f K converge to f ? Exercise 9.1.3. Suppose that f is a piecewise continuous function, \fnd norms j\u0001 j1; j\u0001 j2 so that j \u0016f K − f j1 and j R \u0016f K − Rf j2 tend to zero as K !1: Exercise 9.1.4. Prove directly that if r has maximal rank then the normal equations have a unique solution. 9.2 Kaczmarz’ method Most of the techniques used in medical imaging are iterative. Instead of attempting to solve an equation like (9.3), we use an algorithm which de\fnes a sequence < x(k) >; of vectors that get closer and closer to a solution (or approximate solution). The principal method used in medical imaging derives from the Kaczmarz method or method of projections.The idea can be explained using a very simple 2 \u0002 2-example r11x1 + r12x2 = p1; r21x1 + r22x2 = p2: For i =1; 2 ri1x1 + ri2x2 = pi de\fnes a line li in the plane. The solution for the system of equation is the point of intersection of these two lines. The method of projections is very simple to describe geometrically: 412 CHAPTER 9. ALGEBRAIC RECONSTRUCTION TECHNIQUES l 1 l 2 (0) (0,1) (0,2) (1,1)(1,2) xxx x x (a) Convergence in in\fnitely many steps. l 1 l 2 (0) x(0,2) x x(0,1) =x(1) (b) Convergence in one iteration. Figure 9.2: Method of projections (1) Choose an arbitrary point and call it x(0). (2) Orthogonally project x(0) onto l1; denote the projected point by x(0;1). Orthogo- nally Project x(0;1) onto l2; denote the projected point by x(0;2). This completes one iteration, set x(1) d = x(0;2). (3) Go back to (2), replacing x(0) with x(1); etc. This gives a sequence < x(j) > which, in case the lines intersect, converges, as j !1 to the solution of the system of equations. If the two lines are orthogonal, a single iteration is enough. However, the situation is not always so simple. Figure 9.3(a) shows that it does not converge for two parallel lines - this corresponds to an inconsistent system which has no solution. Figure 9.3(b) depicts an over-determined, inconsistent 3 \u0002 2 system, the projections are trapped inside the triangle but do not converge as j !1: The equations which arise imaging applications can be rewritten in the form ri \u0001 x = pi;i =1;::: ;I; where ri is the ith-row of the measurement matrix, r: Each pair (ri;pi) de\fnes a hyperplane inRJ fx : ri \u0001 x = pig: Following exactly the same process used above gives the basic Kaczmarz iteration: (1) Choose an initial vector x(0): (2) Orthogonally project x(0) into r1 \u0001 x = p1 ! x(0;1), Orthogonally project x(0;1) into r2 \u0001 x = p2 ! x(0;2), ... Orthogonally project x(0;I−1) into rI \u0001 x = pI ! x(0;I) d = x(1): 9.2. KACZMARZ’ METHOD 413 l 1 l 2 (i,1) (i,2) x x x(0) (a) Two parallel lines. l 2 1l l 3 3 4 5 6 7 8 1 2 9 (b) Overdetermined system. Figure 9.3: Examples where the projection algorithm does not converge. (3) Go back to (2) replacing x(0) with x(1); etc. To do these computations requires a formula for the orthogonal projection of a vector into a hyperplane. The vector ri is orthogonal to the hyperplane ri \u0001 x = pi. The orthogonal projection of a vector y onto ri \u0001 x = pi is found by subtracting a multiple of ri from y.Let y(1) = y − \u000bri; then \u000b must satisfy pi = y(1) \u0001 ri = y \u0001 ri − \u000bri \u0001 ri: Solving this equation gives \u000b = y \u0001 ri − pi 414 CHAPTER 9. ALGEBRAIC RECONSTRUCTION TECHNIQUES ikik x x z (k) (k+1) r x = p. Figure 9.4: One step in the Kaczmarz algorithm. Theorem 9.2.1. Let < ri > be a sequence of vectors inRJ : If the system of equations ri \u0001 x = pi;i =1;::: ;I: has a solution, then the Kaczmarz iteration converges to a solution. Proof. For the proof of this theorem it is more convenient to label the iterates sequentially x (1); x (2) :::; x (k);::: instead of x (0,1);::: ; x (0,I); x (1,0);::: ; x (1,I);:::: Thus x (j,k) $ x (jI+k): Let z denote any solution of the system of equations. To go from x (k) to x (k+1) entails projecting into a hyperplane rik \u0001 x = pik ; see \fgure 9.4. The di\u000berence x (k) − x (k+1) is orthogonal to this hyperplane, as both x (k+1) and z lie in this hyperplane it follows that hx (k) − x (k+1); x (k+1) − zi =0: The Pythagorean theorem implies that kx (k+1) − zk2 + kx (k) − x (k+1)k2 = kx (k) − zk2 )kx (k+1) − zk2 \u0014kx (k) − zk2: (9.4) The sequence < kx (k) − zk2 > is a non-negative and decreasing hence, it converges to a limit. This shows that < x (k) > lies in a ball of \fnite radius and so the Bolzano-Weierstrass theorem implies that it has a convergent subsequence x (kj ) ! x \u0003. Observe that each index kj is of the form lj + nI where lj 2f0;::: ;I − 1g. This means that, for some l; there must be an in\fnite sub-sequence, fkji g so that kji = l + niI: All the vectors fx (kji ) : i =1; 2;::: g lie in the hyperplane frl \u0001 x = plg: As a hyperplane is a closed set this implies that the limit, x \u0003 = lim m x (kjm ) also belongs to the hyperplane rl \u0001 x = pl: On the other hand, it follows from (9.4) and the fact that kx (k) − zk converges that lim j!1 kx (kj +1) − x (kj )k =0: Thus x (kj +1) also converges to x \u0003: The de\fnition of the Kaczmarz algorithm implies that x (kj +1) 2 fx : rl+1 \u0001 x = pl+1g. As above, this shows that x \u0003 is in this hyperplane as well. Repeating this argument I times we conclude that x \u0003 2fri \u0001 x = pig; for all i =1;::: I: 9.2. KACZMARZ’ METHOD 415 That is, x \u0003 is a solution of the original system of equations. To complete the proof we need to show that the original sequence, < x (k) > converges to x \u0003. Recall that kx (k) − zk tends to a limit as k !1 for any solution z. Let z = x \u0003 then kx (k) − x \u0003k! \u0015. For the subsequence fkjg, it follows that lim j!1 kx (kj ) − x \u0003k =0 Thus \u0015 = 0 and limk!1 x (k) = x \u0003: 416 CHAPTER 9. ALGEBRAIC RECONSTRUCTION TECHNIQUES Proof. Suppose that A :RJ !RI with I \u0014 J; a matrix of maximal rank. The solution to Ax = y with minimal l2-norm is given by A tu; where u is the unique solution to AA tu = y: To see this let x0 be any solution to Ax = y, and let v 2 ker A be such that kx0 + vk2 is minimal. The minimal norm solution is the one perpendicular to ker A so that 0= d 9.3. A BAYESIAN ESTIMATE 417 9.3 A Bayesian estimate A small modi\fcation of the ART algorithm leads to an algorithm which produces a \\Bayesian estimate\" for an optimal solution to (9.1). Without going into the details, in this approach one has prior information that the solution should be close to a known vector v0: Instead of looking for a least squares solution to the original equation we try to \fnd the vector which minimizes the combined error function: Bˆ(x) d = ˆkrx − pk 2 + kx − v0k: Here ˆ is a \fxed, positive number. It calibrates the relative weight given to the measure- ments versus the prior information. If ˆ = 0 then the measurements are entirely ignored, as ˆ !1 less and less weight is given to the prior information. In many di\u000berent measure- ment schemes it is possible to use the measurements alone to compute the average value, \u0016x of the entries x: If we set v0 = \u0016xe; then the \\prior information\" is the belief that the variance of the solution should be as small as possible. The vector xˆ which minimizes Bˆ(x) can be found as the minimal norm solution of a consistent system of linear equations. In light of Theorem 9.2.1 and Lemma 9.2.1 this vector can then be found using the Kaczmarz algorithm. The trick is to think of the error u = rx − p as an independent variable. Let \u0012u z \u0013 denote an I + J-column vector and E the I \u0002 I identity matrix. The system of equations we use is [Eˆr] \u0012u z \u0013 = ˆ [p − rv0] : (9.5) Theorem 9.3.1. The system of equations (9.5) has a solution. If \u0012uˆ zˆ \u0013 is its minimal norm solution then xˆ = zˆ + v0 minimizes the function Bˆ(x): Proof. That (9.5) has solutions is easy to see. For any choice of x setting u = ˆ [p − rz − rv0] gives a solution to this system of equations. A minimal norm solution to (9.5) is orthogonal to the null space of [Eˆr]: This implies that it belongs to the range of the transpose, that is zρ = ˆr tuρ: (9.6) On the other hand a vector xρ minimizes Bρ if and only if it satis\fes the variational equation: ˆr t(rxρ − p)= v0 − xˆ: (9.7) The relation, (9.6) between uρ and zρ implies that ˆ(rzρ − p)= −(uρ + ˆrv0) 418 CHAPTER 9. ALGEBRAIC RECONSTRUCTION TECHNIQUES and therefore ˆr t(rzρ − p)= −zρ − ˆr trv0: This in turn shows that ˆr t(rxρ − p)= −zρ = v0 − xρ: Thus xρ satis\fes the variational equation (9.7) and therefore minimizes Bρ: 9.4. VARIANTS OF THE KACZMARZ METHOD 419 i = p i r. x(k) <2 x =1 =2 λ λ (k) 1< λ (k) (k) λ (k) 0< <1 Figure 9.6: Ranges relaxation parameters. parameters. Instead of applying the full correction, a scalar multiple is used instead. To that end, the ART algorithm is modi\fed by putting in factors, f\u0015kg to obtain x (k;i) ! x (k;i) − \u0015k x(k;i) \u0001 ri − pi 420 CHAPTER 9. ALGEBRAIC RECONSTRUCTION TECHNIQUES the measurements, so that the expected correlation between successive measurements is small. 9.4.2 Other related algorithms There are many variants of the sort of iteration used in the Kaczmarz method. For example one can think of \u0001x(k;i) j = x(k;i) j − x(k;i−1) j = pi − ri \u0001 x(k;i−1) 9.4. VARIANTS OF THE KACZMARZ METHOD 421 relaxation parameters, noisy or inconsistent data can also be e\u000bectively handled. Though most present day machines use some form of \fltered backprojection, the \frst algorithm, in the \frst commercial CT-scanner was of this general type. A very complete discussion of these methods, along with references to the extensive literature in given in [24]. A more recent description of the usage of these techniques in the context of positron emission tomography in given in [25]. 422 CHAPTER 9. ALGEBRAIC RECONSTRUCTION TECHNIQUES Chapter 10 Probability theory and random variables Up to this point we have considered only deterministic systems. These are systems where a known input produces a known output. We now begin to discuss probability theory, which is the language of noise analysis. But what is noise? There are two essentially di\u000berent sources of noise in the mathematical description of a physical system or measurement process. The \frst step in building a mathematical model is to isolate a physical system from the world in which it sits. Once such a separation is \fxed, the e\u000bects of the outside world on the state of the system are often modeled, a posteriori as noise. In our model for CT-imaging it is assumed that every X-ray which is detected is produced by our X-ray source. In reality, there are many other sources of X-rays which might also impinge on our detectors. Practically speaking it is not possible to give a complete description of all such external sources, sometimes it is possible to describe them probabilistically. Many physical processes are inherently probabilistic and that is the second source of noise. In CT-imaging the interaction of an X-ray \\beam\" with an object is a probabilistic phenomenon. The beam is in fact a collection of discrete photons. Whether or not a given photon, entering an object on one side re-emerges on the other side, traveling in the same direction, depends on the very complicated interactions this photon has with the microscopic components of the object it passes through. Practically speaking one cannot model the details of these interactions in a useful way. If \u0016 is the absorption coe\u000ecient at x and I is the incident ﬂux of photons then Beer's law says that the change in the ﬂux, I(t +\u0001t) − I(t)over a smalldistance\u0001t is I(t +\u0001t) − I(t) ˇ−\u0016\u0001tI(t) or I(t +\u0001t)= (1 − \u0016\u0001t)I(t): This can be interpreted as the statement that an incident photon has probability (1 − \u0016\u0001t) of being emitted. Beer's law describes the average behavior of \\a photon\" and is only useful if the X-ray beam is composed of a very large number of photons. For a particular measurement it is not possible to predict the exact discrepancy between Beer's law and the actual life history of the given X-ray beam. Instead one has a proba- bilistic description which describes the statistics of this discrepancy. A certain number of 423 424 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES transmitted photons, N (t) are measured, one can think of the measurement as having two parts: N (t)= Nd(t)+ v(t) where Nd is a deterministic quantity, i.e. the part predicted by Beer's law and v is a random process which models the noise in the system. A good probabilistic model for the noise component aids in interpretation of the measurements. Note that even the model of an X- ray beam as a constant ﬂux of energy is an approximation to the truth; the actual output of an X-ray source also has a useful probabilistic description. In this chapter we review some of the basic concepts of measure theory and probability theory. It is not intended as a development of either subject ab ovo, but merely a presen- tation of the main ideas so that we can later discuss random processes and noise in image reconstruction. These concepts are presented using the mathematical framework provided by measure theory. It is not necessary to have a background in measure theory, we use it as a language to give precise de\fnitions of the concepts used in probability theory and later to pass to random processes. Historically, probability theory preceded measure theory by many decades, but was found, in the early twentieth century to be somewhat lacking in rigorous foundations. Kolmogorov discovered that measure theory provided the missing foundations. A basic introduction to probability can be found in [11], an introduction to random processes in [13] or [54] and an introduction to measure theory in [16] or [65]. 10.1 Measure theory Mathematical probability theory is a subset of measure theory. This is the branch of mathematics which gives a framework in which to study integration. For concreteness we discuss probability theory from the point of view of doing an \\experiment.\" We are then interested in quantifying the likelihood that the experiment has this or that outcome. In measure theory one works with a measure space. This is a pair (X; M)where X is the underlying space and M is a collection of subsets of X called a ˙-algebra. 10.1.1 Allowable events From the point view of probability theory, X is called the sample space,it is the set of all possible outcomes of the experiment. Subsets of X are collections of possible outcomes, which in probability theory are called events. The subsets of X in M are \\allowable\" events. These are events which can be assigned a well de\fned probability of occurring. A simple physical example serves to explain, in part why it may not be possible to assign a probability of occurrence to every event, i.e. to every subset of X: Example 10.1.1. Suppose that the experiment involves determining where a particle strikes a line. Thesamplespace X =R: For each n 2Z; the measuring device can determine whether on not the particle fell in [n; n + 1) but not where in this interval the particle fell. For each n; the event An = the particle fell in [n; n +1) is therefore an allowable event. On the other hand the event: \\the particle fell in [:3;:7)\" is not an allowable event, as the measurements cannot determine whether or not this occurs. For the set of allowable events we take arbitrary unions of the subsets fAn : n 2Zg: 10.1. MEASURE THEORY 425 Example 10.1.2. Perhaps the simplest, interesting experiment is that of \\ﬂipping a coin.\" This is an experiment that has two possible outcomes which we label H and T: The sample space X1 = fH; T g: In ordinary language the possible events are \u000f We get a head. \u000f We get a tail. \u000f We get a head or a tail. These events correspond to the following subsets of X1 : fHg; fT g; fH; T g: Example 10.1.3. Suppose that instead of ﬂipping a coin once, the experiment involves ﬂipping a coin two times. The possible outcomes are the sequences of length two in the symbols H and T : X2 = f(H; H); (H; T ); (T; H); (T; T )g: If the experiment involves ﬂipping a coin N -times then the sample space is all sequences of length N using the symbols H and T: We denote this space by XN : In each of these examples the sample space is a \fnite set. In such cases all events are usually allowable and therefore we take MN to be all the subsets of XN : The collection, M of allowable events provides a mathematical framework for describing the operation of a measuring apparatus. It has the following axiomatic properties: (1) X 2M, this is the statement that X is the collection of all possible outcomes. (2) If A 2M and B 2M,then A [ B 2M, in other words if the events A and B are each allowable then the event \\A or B\" is also allowable. (3) If A 2M,then X n A 2M; in other words if the event \\A occurs\" is allowable then the event \\A does not occur\" is also allowable. (4) If we have a countable collection of allowable events Aj 2M then 1[ j=1 Aj 2M as well. Condition (4) is a technical condition which is essential to have a good mathematical theory of integration. It is very important when taking limits of sequences, but does not have a simple intuitive explanation. Subsets belonging to M are called allowable events or measurable sets. A collection of subsets of a space which satisfy these axioms is called a ˙-algebra. 426 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES As a consequence of these axioms we need to introduce the notion of the empty set.It is the subset of X which contains no elements and is denoted by ;: This arises because (2) and (3) imply that if A and B are allowable events then so is A \\ B: However, if A and B have no points in common then A \\ B = ;: This is mostly a linguistic device, encapsulating the idea that the experiment has no outcome. As we shall see this \\event\" always has probability zero of occurring. The list given in example 10.1.2 is not quite all of M1; but rather M1 = f;; fHg; fT g; fH; T gg: Example 10.1.4. Some examples of allowable events for the case XN de\fned above are \u000f All N ﬂips produce heads. \u000f Half the ﬂips are tails. \u000f If k \u0014 N then the kth ﬂip is a head. \u000f At most N=3ﬂips are heads. Example 10.1.5. Suppose that the result of our experiment is a real number, then X =R: Allowable events might include \u000ffxg for an x 2R; the outcome of the experiment is the number x: \u000f [0; 1); the outcome of the experiment is a non-negative number. \u000f (1; 2); the outcome of the experiment is a number between 1 and 2: The smallest collection of subsets ofRwhich includes all intervals and is a ˙-algebra is called the Borel sets. It is discussed in [16]. Exercise 10.1.1. Show that the collection of allowable events, M in example 10.1.1 has the following description. To each A 2M there are, possibly bi-in\fnite, sequences faig and fbig of integers so that \u0001\u0001\u0001 <ai <bi <ai+1 <bi+1 < \u0001\u0001\u0001 and A = 1[ j=−1 [aj;bj): Exercise 10.1.2. Show that the collection of sets de\fned in example 10.1.1 is a ˙-algebra. Exercise 10.1.3. Show that the space in example 10.1.3 XN has 2N elements. How many di\u000berent allowable events are there, that is how large is MN ? Exercise 10.1.4. Let A ˆ X then the complement of A in X is the subset of X de\fned by A c = X n A = fx 2 X : x=2 Ag: Show that if A; B ˆ X then (A [ B) c = A c \\ Bc: Conclude that if A; B 2M then A \\ B 2M as well. 10.1. MEASURE THEORY 427 10.1.2 Measures and probability 428 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES Example 10.1.6. For the case of a single coin toss, the allowable events are M1 = ffHg; fT g; fH; T g; ;g: The function \u00171 : M1 ! [0; 1] is \fxed once we know the probability of H: For say that \u00171(H)= p: Because \u00171(H [ T )= 1 and H and T are mutually exclusive events, it follows that \u00171(T )=1 − p: Example 10.1.7. For the case of general N the allowable events are all collections of se- quences a =(a1;::: ;aN )where aj 2fH; T g for j =1;::: ;N: The most general probability function on MN is de\fned by choosing numbers fpa : a 2 XN g so that 0 \u0014 pa \u0014 1 and X a2XN pa =1: If A 2MN is an event then \u0017(A)= X a2A pa: In most instances one uses a much simpler measure to de\fne the probability of events in MN : Instead of directly assigning probabilities to each sequence of length N we use the assumption that an H occurs (at any position in the sequence) with probability p and a T with probability 1 − p: We also assume that outcomes of the various ﬂips are independent of one another. With these assumptions one can show that \u0017p;N (fag)= pma(1 − p) N −ma where ma is the number of Hs in the sequence a: Example 10.1.8. Suppose that X =R; with M the Borel sets. For any a<b the half open interval [a; b) 2M: Let f (t) be a non-negative, continuous function de\fned on X with the property that 1Z −1 f (t)dt =1: De\fne the probability of the event [a; b)to be Prob([a; b)) = bZ a f (t)dt: It is not di\u000ecult to show that this de\fnes a function on M satisfying the properties enu- merated above, see [16]. 10.1. MEASURE THEORY 429 Example 10.1.9. In the situation described in example 10.1.1 X =R: Choose a bi-in\fnite sequence fan : n 2Zg of non-negative numbers such that 1X n=−1 an =1: and de\fne \u0017([n; n +1)) = an for n 2Z: This means that an is the probability that the particle fell in [n; n +1): Using the prop- erties above, \u0017 is easily extended to de\fne a measure on the allowable sets de\fned in example 10.1.1. Again note that it is not possible to assign a probability to the event \\the particle fell in [:3;:7)\" as our measurements are unable to decide whether or not this happens. Example 10.1.10. Suppose that X is the unit disk in the plane and let M be the \\Lebesgue measurable\" subsets of X: These are the subsets whose surface area is well de\fned. The outcome of our experiment is a point in X: For a measurable set A; de\fne the probability of the event \\the point lies in A\"to be Prob(A)= area(A) 430 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES For an indicator function it is clear that the only reasonable way to de\fne the integral is to set Z X ˜A(x)d\u0017(x)= \u0017(A): As \u0017(X) = 1 this de\fnes \u0017(A) as the fraction of X occupied by A: De\fnition 10.1.2. A function f is called a simple function if there are sets Aj 2M;j = 1;::: ;m and real constants fajg so that f (x)= NX j=1 aj˜Aj (x): It is not di\u000ecult to show that a simple function is measurable. Since the integral should be linear it is clear that we must de\fne Z X NX j=1 aj˜Aj (x)d\u0017(x)= NX j=1 aj\u0017(Aj): While this formula is intuitively obvious, it requires proof that the integral is well de\fned. This is because a simple function can be expressed as a sum, in di\u000berent ways X aj˜Aj = MX k=1 bk˜Bk: It is necessary to show that NX j=1 aj\u0017(Aj)= MX k=1 bk\u0017(Bk): for any other representation. This is left as an exercise for the interested reader. Suppose that f is a bounded, measurable function. Fix a positive integer N; for each j 2Zset AN;j = f −1 \u0012[ j 10.1. MEASURE THEORY 431 (2). Z X FN (x)d\u0017(x)= 1X j=−1 j 432 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES These conditions imply that if fAjg is a collection of pairwise disjoint subsets belonging to M and fajg is a bounded sequence of numbers then the function f (x)= 1X j=1 aj˜Aj (x) is an integrable function with Z X fd\u0017 = 1X j=1 aj\u0017(Aj): Note that here we consider in\fnite sums whereas previously we only considered \fnite sums. Example 10.1.11. Let (XN ; MN ;\u0017p;N ) be the probability space introduced in the exam- ple 10.1.7. Since MN contains all subsets of XN ; any function on XN is measurable. Using the properties of the integral listed above it is not di\u000ecult to show that if f is a function on XN then Z XN f (x)d\u0017p;N (x)= X a2XN f (a)\u0017p;N (a): For a \fnite probability space the integral reduces to an ordinary sum. Example 10.1.12. If f (x) is a non-negative, continuous function onRwith ZR f (x)dx =1 then we can de\fne a probability measure by setting \u0017f (A)= Z A f (x)dx = ZR ˜A(x)f (x)dx: Here A is assumed to be a Borel set. With this de\fnition it is not di\u000ecult to show that for any bounded, measurable function g we have ZR g(x)d\u0017(x)= ZR g(x)f (x)dx: Example 10.1.13. Let X =Rand let fang be a sequence of non-negative numbers such that 1X j=−1 aj =1: We let M be the collection of all subsets ofRand de\fne the measure \u0017 by letting \u0017(A)= X fn : n2Ag an: 10.1. MEASURE THEORY 433 Any function g(x) is measurable and ZR g(x)d\u0017(x)= 1X n=−1 ang(n): Example 10.1.14. Suppose that F (x) is a non-negative function de\fned onRwhich satis\fes the conditions (1). F is a monotone non-decreasing function: x< y implies that F (x) \u0014 F (y); (2). F is continuous from the right: for all x; F (x) = limy!x+ F (y); (3). limx!−1 F (x)= 0; limx!1 F (x)=1: Such a function de\fnes a measure on the Borel subsets ofR: The measure of a half ray is de\fned to be \u0017F ((−1;a]) d = F (a); and the measure of an interval (a; b] is de\fned to be \u0017F ((a; b]) d = F (b) − F (a): Note that if (a; b] is written as a disjoint union (a; b]= 1[ j=1 (aj;bj] then F (b) − F (a)= 1X j=1 F (bj) − F (aj): (10.2) This condition shows that the \u0017F -measure of an interval (a; b] is well de\fned. If a1 <b1 \u0014 a2 <b2 \u0014 \u0001\u0001\u0001 then the measure of the union of intervals is de\fned by linearity (of the integral) \u0017F 0 @ 1[ j=1 (aj;bj] 1 A = 1X j=1 F (bj) − F (aj): The measure can be extended to an arbitrary Borel set by approximating it by intersections of unions of intervals. The measure \u0017F is called the Lebesgue-Stieltjes measure de\fned by F: The \u0017F -integral of a function g is usually denoted 1Z −1 g(x)dF (x): It is called the Lebesgue-Stieltjes integral de\fned by F: 434 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES A similar construction can be used to de\fne measures onRn : Here F (x1;::: ;xn)is assumed to be a non-negative function which is monotone, non-decreasing and continuous from the right in each variable separately, satisfying lim xj!−1 F (x1;::: ;xj;::: ;xn)=0 for j =1;::: ;n: The measure of (−1;a1] \u0002 \u0001\u0001\u0001 \u0002 (−1;an] is de\fned to be \u0017F ((−1;a1] \u0002 \u0001\u0001\u0001 \u0002 (−1;an]) = F (a1;::: ;an): By appropriately adding and subtracting one de\fnes the measure of a \fnite rectangle. For example, if n = 2 then \u0017F ((a1;b1] \u0002 (a2;b2]) = F (b1;b2)+ F (a1;a2) − F (a2;b1) − F (a1;b2): (10.3) A discussion of Lebesgue-Stieltjes integrals can be found in [12]. How is the notion of probability introduced above related to the outcomes of experi- ments? We consider the case of ﬂipping a coin, suppose that heads occurs with probability p and tails occurs with probability 1 − p. What is meant by that? Suppose we ﬂip a coin N times, and let H(N )and T (N ) be the number of heads and tails respectively. Intuitively the statement Prob(fHg)= p is interpreted to mean lim N !1 H(N ) 10.1. MEASURE THEORY 435 Exercise 10.1.9. Show that if A 2M then ˜A is a measurable function. Exercise 10.1.10. For the ˙-algebra M de\fned in example 10.1.1 what are the measurable functions? Exercise 10.1.11. Provethatif f is a measurable function then the sets AN;j de\fned in (10.1) belongs to M for every j and N: Exercise 10.1.12. Show that Z X FN d\u0017(x) \u0014 Z X FN +1d\u0017(x): Exercise 10.1.13. Show that if f is a measurable function such that jf (x)j\u0014 M for all x 2 X then Z X f (x)d\u0017(x) \u0014 M: What can be concluded from the equality condition in this estimate? Exercise 10.1.14. If f is a bounded function then, for each N there exists an MN so that AN;j = ; if jjj\u0015 MN : Exercise 10.1.15. With M de\fned in example 10.1.1 and \u0017 de\fned in example 10.1.9, which functions are integrable and what is the integral of an integrable function? Exercise 10.1.16. Give a method for extending the de\fnition of the integral to non- negative functions, which may not be bounded. Exercise 10.1.17. If (X; M;\u0017) is a probability space, fAjgˆ M are pairwise disjoint subsets and fajg is a bounded sequence then show that 1X j=1 jajj\u0017(Aj) < 1: Exercise 10.1.18. Suppose that F (x) is a di\u000berentiable function with derivative f ,show that 1Z −1 g(x)\u0017F (x)= 1Z −1 g(x)f (x)dx: (10.4) Exercise 10.1.19. Suppose that F has a weak derivative f: Does (10.4) still hold? Exercise 10.1.20. Let F (x)=0for x< 0and F (x)= 1 for x \u0015 0: For a continuous function, g what is 1Z −1 g(x)\u0017F (x)? 436 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES 10.1.4 Independent events Suppose that a coin is ﬂipped several times in succession. The outcome of one ﬂip should not a\u000bect the outcome of a successive ﬂip, nor is it a\u000bected by an earlier ﬂip. They are independent events. This is a general concept in probability theory. De\fnition 10.1.4. Let (X; M;\u0017) be a probability space. Two allowable events, A and B are called independent if Prob(A \\ B)= Prob(A)Prob(B): (10.5) Earlier we said that two events A and B were mutually exclusive if A \\ B = ;; in this case Prob(A [ B)= Prob(A)+Prob(B): Note the di\u000berence between these concepts. Example 10.1.15. Let XN = f(a1;a2;::: ;aN )jai 2fH; T gg be the sample space for ﬂipping acoin N times. Suppose that Prob(H)= p and Prob(T )= 1 − p. If successive ﬂips are independent then formula (10.5) implies that Prob((a1;a2;::: ;aN )) = NY i=1 Prob(ai)= pma(1 − p) N −ma: To see this, observe that the event a1 = H is the set f(H; a2;::: ;aN ): aj 2fH; T gg;ithas probability p because it evidently only depends on the outcome of the \frst ﬂip. Similarly the event a1 = T has probability 1 − p: Indeed for any \fxed j; Prob(aj = H)= p and Prob(aj = T )= 1 − p: The event Ak = fai = H; i =1;::: k and aj = T; j = k +1;::: ;N g can be expressed \" k\\ i=1 fai = Hg # \\ 2 4 N\\ j=k+1 faj = T g 3 5 : Since this is an intersection of independent events it follows that Prob(Ak)= pk(1 − p) N −k: A similar argument applies if we permute the order in which the heads and tails arise. For each integer 0 \u0014 k \u0014 N de\fne the event Hk = fa : k of the ai are Hg: An element of Hk is a sequence of length N consisting of k H's and (N − k)T's. Let us choose one of them: (H;::: ;H | 10.1. MEASURE THEORY 437 The probability of this event is Prob((H;::: ;H | 438 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES They are related by Bayes' law Prob(BjA)= Prob(AjB)Prob(B) 10.2. RANDOM VARIABLES 439 10.2 Random variables The sample space for the coin tossing experiment is XN : For many questions XN contains more information than is needed. For example, if we are only interested in the number of heads then we could use the simpler sample space f0; 1;::: ;N g: The probability of the event fkgˆf0; 1;::: ;N g; which is the same as the event Hk ˆ XN ; is Prob(fkg)= \u0012 N k \u0013 pk(1 − p) N −k The sample space f0;::: ;N g contains strictly less information than XN ; but for the purpose of counting the number of heads, it is su\u000ecient. It is often useful to employ the simplest possible sample space. Another way of thinking of the sample space f0;::: ;N g is as the range of a function on the \\full\" sample space XN : In our example, de\fne the function ˜H on X = fH; T g by ˜H(H)=1;˜H (T )= 0: Similarly, on XN de\fne ˜H N (a)= NX i=1 ˜H(ai); where a =(a1;::: ;aN ). The set f0;::: ;N g is the range of this function. The event that k heads arise is the event fa : ˜H N (a)= kg; its probability is Prob(Hk)= \u0017(fa : ˜H N (a)= kg)= \u0012N k \u0013 pk(1 − p) N −k: The expression on the right hand side can be thought of as de\fning a probability measure on the space f0;::: ;N g with Prob(fkg)= \u0012N k \u0013 pk(1 − p) N −k: De\fnition 10.2.1. Let (X; M; Prob) be a probability space. Recall that a real valued function f is measurable if for every t 2Rthe set f −1((−1;t]) belongs to M: Areal valued, measurable function on the sample space is called a random variable. Acomplex valued function is measurable if its real and imaginary parts are measurable. A complex valued, measurable function is a complex random variable. Example 10.2.1. The function ˜H N is a random variable on XN : Example 10.2.2. Let X be the unit interval [0; 1]; M the Borel sets in [0; 1] and \u0017([a; b]) = b − a: The function f (x)= x is a measurable function. It is therefore a random variable on X: For each k 2Zthe functions e2ˇikx are measurable and are therefore complex random variables. Thinking of the sample space X as all possible outcomes of an experiment, its points give a complete description of the possible states of the system under study. With this interpretation, one would not expect to be able to determine, completely which x 2 X is the outcome of an experiment. Instead one expects to be able to measure some function of x and this is the way that random variables enter in practical applications. 440 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES Example 10.2.3. Consider a system composed of a very large number, N of gas particles contained in a \fxed volume. The sample space, X =R6N describes the position and mo- mentum of every particle in the box. To each con\fguration in X we associate a temperature T and a pressure P: These are real valued, random variables de\fned on the sample space. In a realistic experiment one can measure T and P; though not the actual con\fguration of the system at any given moment. Given the probabilistic description of an experiment (X; M;\u0017) and a random variable ˜(x); which can be measured, it is reasonable to enquire what value of ˜ we should expect to measure. Because \u0017(X)=1; the integral of a function over X is a weighted average. In probability theory this is called the expected value. De\fnition 10.2.2. Let (X; M;\u0017) be a probability space and ˜ a random variable. De\fne the expected value or mean of the random variable ˜ by setting \u0016˜ = E[˜]= Z X ˜(x)d\u0017(x): If either ˜\u0006 has integral +1 then ˜ does not have an expected value. In the literature <˜> is often used to denote the expected value of ˜: We avoid this notation as we have already used < \u0001 > to denote sequences. In this text \u0016 is also used to denote the absorption coe\u000ecient. The meaning should be clear from the context. Because the expectation is an integral and an integral depends linearly on the integrand, the expectation does as well. Proposition 10.2.1. Suppose that (X; M;\u0017) is a probability space and the random vari- ables ˜ and have \fnite expected valued, then so does their sum and E[˜ + ]= E[˜]+ E[ ]: Example 10.2.4. One may ask how many heads will occur, on average among N tosses. This is the expected value of the function ˜H N : E[˜H N ]= NX k=0 k Prob(f˜H N (x)= kg)= NX k=0 k \u0012N k \u0013 pk(1 − p) N −k = pN: The expected value can be expressed as the integral over XN : E[˜H N ]= Z XN ˜H N (a)d\u0017p;N (a): Example 10.2.5. Suppose we play a game: we get one dollar for each head, and lose one dollar for each tail. What is the expected outcome of this game? Note that the number of tails in a sequence a is ˜T N (a)= N − ˜H N (a): The expected outcome of this game is the expected value of ˜H N − ˜T N : It is given by E[˜H N − (N − ˜H N )] = E[2˜H N − N ]= 2E[˜H N ] − E[N ]=2pN − N =(2p − 1)N: If p = 1 10.2. RANDOM VARIABLES 441 Example 10.2.6. Suppose that X is the unit disk and the probability measure is dA =ˇ: The radius r = p 442 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES The function p˜(t) is called the density or distribution function for ˜: In terms of the distribution function Prob(a \u0014 ˜ \u0014 b)= bZ a p˜(t)dt: Heuristically p˜(t) is the \\in\fnitesimal\" probability that the value of ˜ lies between t and t + dt: Since probabilities are non-negative this implies that p˜(t) \u0015 0 for all t: The third property of the cumulative distribution implies that 1Z −1 p˜(t)dt =1: The expected value of ˜ can be computed from the distribution function: E[˜]= Z X ˜(x)d\u0017(x)= 1Z −1 tp˜(t)dt: Notice that we have replaced an integration over the probability space X by an integration over the range of the random variable ˜: Often the sample space X and the probability measure \u0017 on X are not explicitly de\fned. Instead one just speaks of a random variable with a given distribution function. The \\random variable\" can then be thought of as the coordinate on the real line and the cumulative distribution de\fnes a Lebesgue-Stieltjes measure onR: Example 10.2.7. A random variable f is said to be Gaussian with mean zero if Pf (t)= 1 10.2. RANDOM VARIABLES 443 If ˜ has a distribution function p˜ then this is equivalent to the condition that 1Z −1 jtkjp˜(t)dt < 1: The kth moment of ˜: is then de\fned to be E[˜k]= Z X ˜k(x)d\u0017(x): In terms of a distribution function E[˜k]= 1Z −1 tkp˜(t)dt: A more useful quantity is the kth-centered moment with is E[(˜ − \u0016˜)k]: The centered moments measure the deviation of a random variable from its mean value. The moments of a random variable may not be de\fned. For example suppose that a real valued, random variable, ˜ has cumulative distribution: Prob(˜ \u0014 t)= 1 444 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES 10.2.2 The variance Of particular interest in applications is the second centered moment, or variance of a random variable. It is de\fned by ˙2 ˜ d = E[(˜ − \u0016˜) 2]: The variance is a measure of how frequently a random variable di\u000bers from its mean. In experimental science it is a measure of the uncertainty in a measured quantity. It can be expressed in terms of the expectation of ˜2 and E[˜]; ˙2 ˜ = E[(˜ − E[˜]) 2] = E[˜2] − 2E[˜] 2 + E[˜] 2 = E[˜2] − E[˜] 2: (10.8) As the expected value of non-negative random variable, it is always non-negative. The positive square root of the variance, ˙˜ is called the standard deviation. Zero standard deviation implies that, with probability one, ˜ is equal to its mean. One could also use E[j˜−\u0016˜j] as a measure of the deviation of a random variable from its mean. In applications the variance occurs much more frequently because it is customary and because computations involving the variance are much simpler than those for E[j˜ − \u0016˜jk]if k 6=2: Example 10.2.8. In the coin tossing example, E[(˜H N ) 2]= NX k=0 k2 \u0012N k \u0013 pk(1 − p) N −k = pN [p(N − 1) + 1] Using (10.8), the variance is E[(˜H N − E[˜H N ]) 2]= pN [p(N − 1) + 1] − p2N 2 = p(1 − p)N: If p = 0, the standard deviation is zero and the coin always falls on tails. A fair coin, i.e., p = 1 10.2. RANDOM VARIABLES 445 This indicates why the variance is regarded as a measure of the uncertainty in the value of a random variable. Exercise 10.2.8. Why does E[˜ − \u0016˜] not provide a good measure of the deviation of ˜ from its mean? Exercise 10.2.9. Let ˜ be a Gaussian random variable with distribution function p˜(t)= 1 446 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES For this reason the E[e−2ˇi\u0015˜]issometimescalled the generating function for the moments of ˜: Note however that the expected value of e−2ˇi\u0015˜ always exists, while the moment themselves may not. Example 10.2.10. In the coin tossing example, the characteristic function is E[e −2ˇi\u0015˜H N ]= NX k=0 e −2ˇi\u0015k \u0012N k \u0013 pk(1 − p) N −k =(1 − p) N NX k=0 \u0012N k \u0013 \u0002e −2ˇi\u0015 p 10.2. RANDOM VARIABLES 447 This function is monotone non-decreasing and continuous from the right in each variable and therefore de\fnes a Lebesgue-Stieltjes measure \u0017˜1;˜2 onR2 ; see example 10.1.14. The measure of a rectangle is given by the formula \u0017˜1;˜2((a; b] \u0002 (c; d]) = Prob(˜1 \u0014 b; and ˜2 \u0014 d)+Prob(˜1 \u0014 a; and ˜2 \u0014 c)− Prob(˜1 \u0014 a; and ˜2 \u0014 d) − Prob(˜1 \u0014 b; and ˜2 \u0014 c) (10.13) If there is a function p˜1;˜2(x; y) de\fned onR2 such that Prob(˜1 \u0014 s; and ˜2 \u0014 t)= sZ −1 tZ −1 p˜1;˜2(x; y)dydx: then we say that p˜1;˜2 is the joint distribution function for the pair of random variables (˜1;˜2). It is clear that Prob(˜1 \u0014 s; and ˜2 \u00141)=Prob(˜1 \u0014 s)and Prob(˜2 \u0014 s; and ˜1 \u00141)=Prob(˜2 \u0014 s): (10.14) This is reasonable because the condition ˜i \u00141 places no restriction on ˜i: This is ex- pressed in terms of the distribution functions by the relations sZ −1 1Z −1 p˜1;˜2(x; y)dydx = sZ −1 p˜1(x)dx; sZ −1 1Z −1 p˜1;˜2(x; y)dxdy = sZ −1 p˜2(y)dy: (10.15) The joint distribution function therefore, is not independent of the distribution functions for individual random variables. It must satisfy the consistency conditions: p˜2(y)= 1Z −1 p˜1;˜2(x; y)dx; and p˜1(x)= 1Z −1 p˜1;˜2(x; y)dy: Recall that two events A and B are independent if: Prob(A \\ B)= Prob(A)Prob(B); Similarly two random variables, ˜1 and ˜2 are independent if Prob(˜1 \u0014 s and ˜2 \u0014 t)= Prob(˜1 \u0014 s)Prob(˜2 \u0014 t): In terms of their distribution functions this is equivalent to p˜1;˜2(x; y)= p˜1(x)p˜2(y): 448 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES The expected value of a product of random variables, having a joint distribution function is given by E[˜1˜2]= ZZ xy \u0001 p˜1;˜2(x; y)dxdy (10.16) Whether or not ˜1 and ˜2 have a joint distribution function, this expectation is an integral over the sample space and therefore E[˜1˜2] satis\fes the Cauchy-Schwarz inequality. Proposition 10.2.2. Let ˜1 and ˜2 be a pair of random variables de\fned on the same sample space with \fnite mean and variance then E[˜1˜2] \u0014 p 10.2. RANDOM VARIABLES 449 On the other hand we compute the probability: Prob(0 \u0014j sin 2ˇxj\u0014 1 450 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES Example 10.2.12. Let XN be the sample space for N -coin tosses. As noted above we usually assume that the results of the di\u000berent tosses in the sequence are independent of one another. In example 10.1.15 we showed that the probability that a sequence a has k heads and N − k tails is pk(1 − p)N −k: The corresponding measure on XN is denoted \u0017p;N : To translate this example into the language of random variables we de\fne the functions ˜j : XN !f0; 1g by letting ˜j(a)= (1if aj = H; 0if aj = T: With the probability de\fned by \u0017p;N these random variables are pairwise independent. Observe that ˜j is only a function of aj so the various ˜j are also functionally independent of one another. Using a di\u000berent probability measure we could arrange to have ˙˜i˜j 6=0; so that the f˜jg are no longer independent as random variables. If ˜ is a random variable recall that the characteristic function is de\fned by M˜(\u0015) d = E[e −2ˇi\u0015˜]= Z e −2ˇi\u0015˜(˘)d\u0017(˘): If ˜ has a distribution function, then M˜(\u0015)= Z e −2ˇi\u0015tp˜(t)dt =^p˜(2ˇ\u0015): As a nice application of the characteristic function formalism we compute the distribution function for the sum of a pair of independent random variables. Suppose ˜1;˜2 are in- dependent random variables with distribution functions p˜1;p˜2 respectively. What is the distribution function for ˜1 + ˜2? It is calculated as follows: M˜1+˜2(\u0015)= E[e −2ˇi\u0015(˜1+˜2)]= E[e −2ˇi\u0015˜1 e −2ˇi\u0015˜2] = E[e −2ˇi\u0015˜1 ]E[e −2ˇi\u0015˜2 ] = M˜1(\u0015)M˜2 (\u0015): The second line comes from the fact that ˜1 and ˜2 are independent. On the other hand M˜(\u0015)= ^p˜(2ˇ\u0015); hence ^p˜1+˜2 =^p˜1 ^p˜2: This implies p˜1+˜2 = p˜1 \u0003 p˜2 and therefore Prob(˜1 + ˜2 \u0014 t)= tZ −1 p˜1+˜2(s)ds = tZ −1 1Z −1 p˜1(s − y)p˜2(y)dyds = 1Z −1 t−yZ −1 p˜1(x)dxp˜2(y)dy: (10.20) 10.2. RANDOM VARIABLES 451 We can understand the last expression intuitively: Heuristically the given probability can be written as Prob(˜1 + ˜2 \u0014 t)= [y2(−1;1) Prob(˜1 \u0014 t − y; ˜2 = y): Note that p˜2(y)dy is the \\in\fnitesimal \\probability\" that ˜2 = y: The argument is not rigorous, in part because the right hand side is an uncountable union of events and the axioms of a ˙-algebra only assure good behavior for countable unions! Exercise 10.2.14. Suppose that (X; M;\u0017) is a probability space and ˜1 and ˜2 are random variables. Express E[˜1˜2] as an integral over X: Exercise 10.2.15. Give a geometric explanation for formula (10.13). When a joint distri- bution function exists show that \u0017˜1;˜2((a; b] \u0002 (c; d]) reduces to the expected integral. Exercise 10.2.16. Suppose that ˜1 and ˜2 are random variables with \fnite mean and variance show that −1 \u0014 ˆ˜1˜2 \u0014 1: Exercise 10.2.17. In the situation of the previous exercise show that jˆ˜1˜2j =1 if and only if ˜2 = a˜1 + b for some constants a; b: More precisely Prob(˜2 = a˜1 + b)=1: Exercise 10.2.18. Prove the expectation version of the Cauchy inequality, (10.2.2). Exercise 10.2.19. Prove the statement in example 10.2.12, that ˜j and ˜k are independent random variables if j 6= k: Exercise 10.2.20. Suppose that ˜ is a random variable with distribution function p˜(x): Let f and g be functions, show that Prob(f (˜) \u0014 a; g(˜) \u0014 b) can be expressed in the form Prob(f (˜) \u0014 a; g(˜) \u0014 b)= Z Ef,g p˜(x)dx; where Ef;g is a subset ofR: Exercise 10.2.21. Suppose that ˜1 and ˜2 are independent random variables and f; g are functions. Show that f (˜1)and g(˜2) are also independent random variables. Exercise 10.2.22. Suppose that ˜1 and ˜2 are random variables and that f and g are functions. Does E[˜1˜2]= E[˜1] \u0001 E[˜2] imply that E[f (˜1)g(˜2)] = E[f (˜1)] \u0001 E[g(˜2)]? Give a proof or counterexample. 452 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES Exercise 10.2.23. A probability measure space is de\fned onR2 by Prob(A)= 1 10.2. RANDOM VARIABLES 453 Once again it is useful to have a statistical measure of independence. The expected values of the products E[˜j˜k]is an m \u0002 m-matrix called the correlation matrix and the di\u000berence Cov(˜j;˜k)= E[˜j˜k] − E[˜j]E[˜k] is called the covariance matrix. The dimensionless version is the normalized correlation matrix de\fned by ˆ˜j;˜k = Cov(˜j;˜k) 454 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES Exercise 10.2.27. Show that if f˜1;::: ;˜mg are independent random variables then for each pair i 6= j the variables ˜i and ˜j are independent. Is the converse statement true, i.e. does pairwise independence imply that a collection of random variables are independent? Exercise 10.2.28. Suppose that the random variables f˜1;::: ;˜mg are pairwise indepen- dent. Show that Cov(˜j;˜k)=0: Exercise 10.2.29. Let f˜1;::: ;˜mg be random variables with joint distribution function p˜1;:::;˜m: Show that p˜1;:::;˜m−1(t1;::: ;tm−1)= 1Z −1 p˜1;:::;˜m(t1;::: ;tm−1;s)ds: Exercise 10.2.30. Show that if f˜1;::: ;˜mg have a joint distribution function and 1 \u0014 i1 < \u0001\u0001\u0001 <ik \u0014 m then f˜i1;::: ;˜ikg also have a joint distribution function. Give a formula for it. Exercise 10.2.31. Suppose that f˜1;::: ;˜ng are independent random variables with means f\u00161;::: ;\u0016ng and variances f˙2 1;::: ;˙2 ng: Let \u0016˜ = ˜1 + \u0001\u0001\u0001 + ˜n 10.3. SOME IMPORTANT RANDOM VARIABLES 455 10.3.1 Bernoulli Random Variables A Bernoulli random variable is speci\fed by two parameters, p 2 [0; 1] and N 2N.The variable ˜ assumes the values f0; 1;::: ;N g with probabilities given by Prob(˜ = k)= \u0012N k \u0013 pk(1 − p) N −k: (10.27) The number of heads in N -independent coin tosses is an example of a Bernoulli random variable. Sometimes these are called binomial random variables. Recall that in the coin tossing experiment we de\fned a function ˜H N such that ˜H N ((a1;::: ;aN )) = number of heads in a: There is a similar model for a γ-ray detector. The model is summarized by the following axioms \u000f Each photon incident on the detector is detected with probability p. \u000f Independence axiom: The detection of one photon is independent of the detection of any other. Let ˜ denote the number of photons detected out of N arriving at the detector. The probability that k out of N incident photons are detected is given by (10.27). We see that expected value : E[˜]= pN; variance : ˙2 = E[(˜ − Np) 2]= p(1 − p)N: If p = 1, that means we have a perfect detector, hence there is no variance. There is also no variance if p =0: In the latter case the detector is turned o\u000b. Suppose we know the detector, i.e., p is known from many experiments.. The number N characterizes the intensity of the source. We would like to know how many photons were emitted by the source. If we measure M photons a reasonable guess for N is given by pN = M . Of course we do not really expect that this is true as the variance is in general not zero. What this means is that, if all our assumptions are satis\fed, and we repeat the measurement many times then the average value of the measurements should approach pN: 10.3.2 Poisson Random Variables A Poisson random variable ˜ assumes the values f0; 1; 2;:::g and it is characterized by the following probability distribution Prob(˜ = k)= \u0015k 456 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES The expected value is given by E[˜]= X k \u0015k 10.3. SOME IMPORTANT RANDOM VARIABLES 457 Integrating gives formulˆ for the mean and variance E[˜]= \u0016; E[(˜ − \u0016) 2]= ˙2: The standard deviation has the interpretation that the probability that ˜ lies between \u0016−˙ and \u0016 + ˙ is about 2=3: From the de\fnition it is clear that the converse statement is also true: The distribution function of a Gaussian random variable is determined by its mean and variance. The characteristic function of a Gaussian random variable is M˜(\u0015)= e 2ˇi\u0016\u0015e − σ2(2πλ)2 458 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES More generally, a collection of m random variables, f˜1; \u0001\u0001\u0001 ;˜mg is Gaussian if and only if the density of the joint distribution function has the form p˜1;:::;˜m(t1;::: ;tm)= s 10.3. SOME IMPORTANT RANDOM VARIABLES 459 Exercise 10.3.3. Suppose that ˜1 and ˜2 are Gaussian random variables, show that for any constants a; b the linear combination a˜1 + b˜2 is also a Gaussian random variable. Compute its mean and standard deviation. Exercise 10.3.4. Suppose that ˜1 and ˜2 are Gaussian random variables, show that there is a invertible matrix \u0012ab cd \u0013 so that a˜1 + b˜2 and c˜1 + d˜2 are independent, Gaussian random variables. Exercise 10.3.5. Suppose that real numbers, f\u00161;::: ;\u0016mg and a positive de\fnite m \u0002 m matrix, rij are given. Find a sample space X; a probability measure on X and random variables f˜1;::: ;˜mg de\fned on X which are jointly Gaussian, satisfying (10.31). Exercise 10.3.6. Suppose that f˜1;::: ;˜mg are jointly Gaussian random variables with means f\u00161;::: ;\u0016mg: Show that they are pairwise independent if and only if Cov(˜i;˜j)= \u000eij˙2 ˜i: Exercise 10.3.7. Suppose that f˜1;::: ;˜mg are jointly Gaussian random variables, show that they are independent if and only if they are pairwise independent. 10.3.4 The Central Limit Theorem Random variables are often assumed to be Gaussian. This is, of course not always true, but the following theorem explains why it is often a reasonable approximation. Theorem 10.3.1 (Central Limit Theorem). Let f˜1;˜2;::: ; g be a sequence of identi- cally distributed, independent random variables with mean \u0016 and variance ˙2.Let Zn = ˜1 + \u0001\u0001\u0001 + ˜n − n\u0016 460 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES f˜ig: Since they are the result of performing the same experiment, over and over, they can also be assumed to be identically distributed. As above set \u0016˜n = ˜1 + \u0001\u0001\u0001 + ˜n 10.3. SOME IMPORTANT RANDOM VARIABLES 461 For simplicity, we assume that the characteristic function of ˜i − \u0016 has two derivatives at the origin. Using the Taylor expansion and the relations (10.33) gives ^p(˘)=1 − ˙2˘2 462 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES Example 10.3.1. Let us denote by X1 the set of all in\fnite sequences of heads and tails. Let Prob(H)= p; Prob(T )=1 − p: We assume that this probability holds for all ﬂips, and each ﬂip is independent of the others. Let ˜i be the random variable de\fned by ˜i((a1;::: ;an;:::)) = ( 1if ai = H; 0if ai = T: These are identically distributed, independent random variables with expected value and variance given by E[˜i]= p; ˙2 ˜i = p(1 − p): The central limit theorem implies that Prob(Zn \u0014 t) ! tZ −1 e − x2 10.3. SOME IMPORTANT RANDOM VARIABLES 463 0 0.1 0.2 0.3 0.4 2468 10x (a) p = :1;n =10 0 0.05 0.1 0.15 0.2 5 10 15 20 25 30 x (b) p = :1;n =30 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 10 20 30 40 50 60 x (c) p = :1;n =60 Figure 10.1: Comparisons of Bernoulli and Gaussian distribution functions with p = :1. 0 0.05 0.1 0.15 0.2 0.25 2468 10x (a) p = :5;n =10 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 5 10 15 20 25 30 x (b) p = :5;n =30 0 0.02 0.04 0.06 0.08 0.1 10 20 30 40 50 60 x (c) p = :5;n =60 Figure 10.2: Comparisons of Bernoulli and Gaussian distribution functions with p = :5. Example 10.3.2. We now consider a di\u000berent limit of the Bernoulli distribution. The Bernoulli distribution is used to model the number of radioactive decays occurring in a \fxed time interval. Suppose there are N particles and each has a probability p of decaying in a \fxed time interval, [0;T ]: If we suppose that the decay of one atom is independent of the decay of any other and let ˜ denote the number of decays occurring in [0;T ]then Prob(˜ = k)= \u0012N k \u0013 pk(1 − p) N −k: The number of decays is therefore a Bernoulli random variable. An actual sample of any substance contains O(1023) atoms. In other words N is a huge number which means that p must be a very small number. Suppose that we let N !1 and p ! 0insucha way that Np ! \u0016\u0015 for some constant \u0016\u0015> 0. It is not di\u000ecult to \fnd the limit of the Bernoulli distribution under these hypotheses. Assuming that Np = \u0016\u0015 we get that \u0012N k \u0013 ( \u0016\u0015 464 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES Since (1 − \u000b 10.3. SOME IMPORTANT RANDOM VARIABLES 465 provided that j\u0015 − kj= p 466 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES The probability that the detector observes k photons, Po(d = k) is therefore Po(d = k)= 1X N =k Ps(N )Pd(kjN ) = 1X N =k \u0012N k \u0013 pk(1 − p) N −k \u0016\u0015N e−\u0016\u0015 10.3. SOME IMPORTANT RANDOM VARIABLES 467 By hypothesis (3) it follows that the probability that a particle incident at a emerges at b is the product of these probabilities pab;m ˇ mY k=1 pk;m: (10.39) This is an approximate result because we still need to let m tend to in\fnity. If \u0016(s)= \u00160 is a constant, then it is an elementary result that the limit of this product, as m !1 is e−\u00160(b−a): Hence a single particle incident at a has a probability e−\u00160(b−a) of emerging at b:. The independence of the individual photons implies that the probability that k out of N photons emerge is P (k; N )= \u0012N k \u0013 e −k\u00160(b−a)(1 − e −\u00160(b−a)) N −k: If N photons are incident then number of photons expected to emerge is therefore E[˜N ]= e −\u00160(b−a)N; see example 10.2.4. The variance, computed in example 10.2.8 is ˙2 ˜N = Ne −\u00160(b−a)(1 − e −\u00160(b−a)): For an experiment where the outcome is a random variable, the signal-to-noise ratio is a number which reﬂects the expected quality of the data. If ˜ is the random variable then SNR(˜) d = expected value 468 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES As m tends to in\fnity, the right hand side of (10.41) converges to − bZ a \u0016(s)ds: Hence the probability that a particle incident at a emerges at b is p\u0016 =exp 2 4− bZ a \u0016(s)ds 3 5 : Arguing exactly as before we conclude that, if N photons are incident then the the proba- bility that k \u0014 N emerge is P (k; N )= \u0012N k \u0013 pk \u0016(1 − p\u0016) N −k (10.42) and therefore the expected number to emerge is E[˜N ]= N exp 2 4− bZ a \u0016(s)ds 3 5 : (10.43) This is exactly Beer's law! The variance is Var(˜N )= p\u0016(1 − p\u0016)N (10.44) so the signal-to-noise ratio is SNR(˜N )= s 10.4. STATISTICS AND MEASUREMENTS 469 Exercise 10.3.10. Suppose that the number of X-ray photons emitted is a Poisson random variable with intensity N and the Bernoulli detector has a probability q of detecting each photon. Show that the overall system of X-ray production, absorption and detection is a Poisson random variable with intensity p\u0016qN: Exercise 10.3.11. Suppose that the process of absorption of X-ray photons through a slab is modeled as a Poisson random variable. If the expected number of emitted photons is given by Beer's law, what is the variance in the number of emitted photons? Is this a reasonable model? 10.4 Statistics and measurements We close our discussion of probability theory by considering how these ideas apply in a simple practical situation. Suppose that ˜ is a real valued, random variable which describes the outcome of a experiment. By describing the outcome of the experiment in these terms we are acknowledging that the measurements involved in the experiment contain errors. While,at the same time, asserting that the experimental errors have a statistical regularity in that they are distributed according to some de\fnite (but apriori unknown) law. Let p˜(x) denote the density function for ˜ so that, for any a<b Prob(a \u0014 ˜ \u0014 b)= bZ a p˜(x)dx: (10.46) Often times one knows that p˜ belongs to a family of distributions. For example, if ˜ is the number of radioactive decays which occur in a \fxed time interval then, we know ˜ is a Poisson random variable and is therefore determined by its intensity, \u0015 = E[˜]: On the other hand, the general type of distribution may not be known in advance. For most practical applications one would like estimates for the mean and variance of ˜ : \u0016˜ = E[˜]and ˙2 ˜ = E[(˜ − \u0016˜) 2]: The mean represents the ideal outcome of the experiment while the variance measures the uncertainty in the measurements themselves. Let f˜ig denote a sequence of independent random variables which are all distributed according to (10.46). This is a model for inde- pendent trials of the experiment in question. If the experiment is performed N -times then the probability that the results lie in a rectangle [a1;b1] \u0002 \u0001\u0001\u0001 [aN ;bN ] is Prob(a1 <˜1b \u00141;::: ;aN <˜N \u0014 bN )= b1Z a1 \u0001\u0001\u0001 bNZ aN p˜(x1) \u0001\u0001\u0001 p˜(xN )dx1 \u0001\u0001\u0001 dxN : (10.47) Our discussion of experimental errors is strongly predicated on the assumption that the various trials of the experiment are independent. Let \u0016˜N = ˜1 + \u0001\u0001\u0001 + ˜N 470 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES denote the random variables de\fned as the average of the results of the \frst N -trials. Because the trials are independent, formula (10.26) gives the mean and variance of \u0016˜N : \u0016 \u0016˜N = \u0016 and ˙2 \u0016˜N = ˙2 10.4. STATISTICS AND MEASUREMENTS 471 Hence an estimate for ˙2 leads to an estimate for the error in asserting that m(x1;::: ;xN )= \u0016: For example Chebyshev's inequality gives Prob(jm(x) − \u0016j <\u000f) \u0015 1 − ˙2 472 CHAPTER 10. PROBABILITY AND RANDOM VARIABLES Chapter 11 Random processes To model noise in measurement and \fltering requires concepts more general than that of a random variable. This is because we need to discuss the results of passing a noisy signal through a linear \flter. As was the case in the chapter on \fltering, it is easier to present this material in terms of functions of a single variable, though the theory is easily adapted to functions of several variables. Our discussion of random processes is very brief, aimed squarely at the goal of analyzing the e\u000bects of noise in the \fltered backprojection algorithm. 11.1 Random processes in measurements To motivate this discussion we think of the familiar example of a radio signal. A radio station broadcasts a signal sb(t) as an electro-magnetic wave. A receiver detects a signal sr(t) which we model as a sum sr(t)= F (sb)(t)+ n(t): Here F is a \fltering operation which describes the propagation of the broadcast signal. For the purposes of this discussion we model F as attenuation, that is F (sb)= \u0015sb for a 0 <\u0015 < 1: The other term is \\noise.\" The noise is composed of several parts. On the one hand it records \\random\" aspects of the life history of the broadcast signal which are not modeled by F: On the other hand it is an accumulation of other signals which happen to be present at about the same carrier frequency as the signal of interest. The existence of the second part is easily veri\fed by tuning an (old) radio to a frequency for which there is no local station. Practically speaking it is not possible to give a formula for the noise. Because we cannot give an exact description of the noise we instead describe it in terms of its properties, beginning with the assumption that the noise is a bounded function of t: What else can be done to specify the noise? Recall the ensemble average de\fnition of probability: it is the average of the results of many \\identical experiments.\" In the radio example, imagine having many di\u000berent radios, labeled by a set A: For each \u000b 2A we let sr;\u000b(t) be the signal received by radio \u000b at time t: The collection of radios A is the sample space. The value of sr;\u000b at a time t is a function on the sample space, in other words, a random variable. From the form given above we see that sr;\u000b(t)= \u0015sb(t)+ n\u000b(t): 473 474 CHAPTER 11. RANDOM PROCESSES The noise can then be described in terms of the statistical properties of the random variables fn\u000b(t)g for di\u000berent values of t: We emphasize that the sample space is A; the collection of di\u000berent receivers, the time parameter simply labels di\u000berent random variables de\fned on the sample space. A family of random variables, de\fned on the same sample space, parametrized by a real variable is called a random process or more precisely a continuous parameter random process. Once the sample space, A is equipped with a ˙-algebra M and a probability measure, \u0017 we can discuss the statistics of the noise. For example at each time t the random variable n\u000b(t) has an expected value E[n\u000b(t)] = Z A n\u000b(t)d\u0017(\u000b): In many applications one assumes that the noise has mean zero, i.e. E[n\u000b(t)] = 0 for all t: This means that if we make many di\u000berent independent measurements fsr;\u000b(t)g and average them we should get a good approximation to \u0015sr(t): The correlation between the noise at one moment in time and another is given by E[n\u000b(t1)n\u000b(t2)] = Z A n\u000b(t1)n\u000b(t2)d\u0016(\u000b): How should the sample space be described mathematically? In an example like this, the usual thing is to use the space of all bounded functions as the index set. That is, any bounded function is a candidate for the noise in our received signal. In principle, the probabilistic component of the theory should then be encoded in the choice of a ˙-algebra and probability measure on the space of bounded continuous functions. These probability measures are rarely made explicit. Instead one speci\fes the cumulative joint distributions of the noise process at \fnite collections of times. This means that for any k 2N; any k-times (t1;::: ;tk)and any k values (s1;::: ;sk) the joint probability that n\u000b(tj) \u0014 sj for j =1;::: ;k is speci\fed. If the joint distributions satisfy the usual consistency conditions then a result of Kolmogorov states that there is a probability measure on A; inducing the joint distributions, with ˙-algebra chosen so that all the sets fn\u000b 2A : n\u000b(tj) \u0014 sjg are measurable. In this chapter we give a brief introduction to the basic concepts of random processes. Our treatment, though adequate for applications to imaging, is neither complete not rigorous. In particular we do not establish the existence of random processes as outlined above. Complete treatments can be found in [12] or [79]. 11.2 Basic deﬁnitions Let (X; M;\u0017) be a probability space, as noted above a random process is an indexed family of random variables de\fned on a \fxed probability space. There are two main types 11.2. BASIC DEFINITIONS 475 of random processes. If the index set is a subset of integers, e.g. natural numbersNthen the process is a discrete parameter random process. The random process is then a sequence of f˜1;˜2;::: ; g of random variables de\fned on X: A continuous parameter random process is a collection of random variables, ˜(t) indexed by a continuous parameter. Often the whole real line is used as the index set, though one can also use a \fnite interval or a half ray. For each t; ˜(t) is a random variable, that is a measurable function on X: We can think of ˜ as a function of the pair (w; t)where w 2 X: For a \fxed w 2 X the map t 7! ˜(w; t) is called a sample path for this random process. Depending on the context we use either the standard functional notation, ˜(t) or subscript notation ˜t; for sample paths. The dependence on the point in X is suppressed unless it is required for clarity. In the continuous case, a rigorous treatment of this subject requires hypotheses about the continuity properties of the random variables as functions of t; see [12] or [79]. It would appear that the \frst step in the discussion of a random process should be the de\fnition of the measure space and a probability measure de\fned on it. As noted above, this is rarely done. Instead the random process is de\fned in terms of the properties of the random variables themselves. In the continuous time case, for each k and every k-tuple of times t1 \u0014 \u0001\u0001\u0001 \u0014 tk the cumulative distributions Pt1;:::;tk (s1;::: ;sk)=Prob(˜(t1) \u0014 s1;˜(t2) \u0014 s2;::: ;˜(tk) \u0014 sk) are speci\fed. In favorable circumstances, these distributions are given by integrals of density functions. Pt1;:::;tk (s1;::: ;sk)= s1Z −1 \u0001\u0001\u0001 skZ −1 pt1;:::;tk(x1; \u0001\u0001\u0001 ;xk)dx1 \u0001\u0001\u0001 dxk; They must satisfy the usual consistency conditions: pt1;:::;tk(x1;::: ;xk)= 1Z −1 pt1;:::;tk+1(x1;::: ;xk;xk+1)dxk+1; ptτ (1);:::;tτ (k)(x˝ (1);::: ;x˝ (k))= pt1;:::;tk(x1;::: ;xk); (11.1) here ˝ is any permutation of f1;::: ;kg: In the discrete case the joint distribution functions are speci\fed for any \fnite subset of the random variables. That is for each k 2Nand each k-multi-index, i =(i1;::: ;ik)the cumulative distribution Pi(s1;::: ;sk)= Prob(˜i1 \u0014 s1;˜i2 \u0014 s2;::: ;˜ik \u0014 sk) is speci\fed. They also need to satisfy the consistency conditions for joint cumulative dis- tributions. If f˜ig is a discrete parameter random process, we say that the terms of the sequence are independent if for any choice of distinct indices fi1;::: ;ikg the random vari- ables f˜1i;::: ;˜ikg are independent. The cumulative distribution functions Pt1;:::;tk (s1;::: ;sk)(or Pi(s1;::: ;sk)) are called the \fnite dimensional distributions of the random process. A basic result of Kolmogorov states that if \fnite dimensional distributions are speci\fed which satisfy the compatibility conditions then there is a probability space (X; M;\u0017) and a random process ˜t de\fned on 476 CHAPTER 11. RANDOM PROCESSES it which induces the given \fnite dimensional distributions. We take this result for granted, for a proof see [12] or [79]. Some examples of random processes will serve to clarify these ideas. Example 11.2.1. Let X be the set of all bounded sequences of real numbers, i.e. X = fa =(a1;a2;:::);ai 2Rwith lim sup i!1 jaij < 1g: De\fne a discrete parameter, random process f˜1;˜2;:::g by setting ˜i(a)= ai: To describe the measure theoretic aspects of this process we choose a probability measure, \u0017 onR: For all i de\fne Prob(˜i \u0014 t)= tZ −1 d\u0017: Supposing further that the f˜ig are independent random variables, we can compute the joint distributions: for each k 2N; multi-index fi1;::: ;ikg and (s1;::: ;sk) 2Rk we have Prob(˜i1 \u0014 s1;::: ;˜ik \u0014 sk)= t1Z −1 d\u0017 \u0001\u0001\u0001 tkZ −1 d\u0017: These properties serve to characterize a random process, though the proof that a ˙-algebra and measure are de\fned on X inducing these joint distribution functions is by no means trivial. Example 11.2.2. Another example is the set of in\fnite sequences of coin ﬂips. The sample space X is a set of all possible in\fnite sequences of heads and tails. As above, de\fne ˜i(a)= (1if ai = H; 0if ai = T: The f˜ig are then taken to be independent random variables with Prob(˜i =0)=1 − p; Prob(˜i =1)= p: Such a process is called a Bernoulli random process because each ˜i is Bernoulli random variable. Example 11.2.3. An example of a continuous time random process is provided by setting X = C0(R+ ) the set of continuous functions onR+ which vanish at 0: For each t 2R+ we have the random variable ˜(t) de\fned at w 2 X by ˜(t; w)= w(t) ˜ is the evaluation of the function w at time t. As before, it is di\u000ecult to give a direct description of the ˙-algebra and measure on X: Instead the process is described in terms of its joint distribution functions. That is, we need to specify Prob(˜(t1) \u0014 s1;˜(t2) \u0014 s2;::: ;˜(tk) \u0014 sk); 11.2. BASIC DEFINITIONS 477 for all k 2Nand all pairs of real k-tuples, ((t1;::: ;tk); (s1;::: ;sk)): An important special case of this construction is given by Prob(˜(t1) \u0014 s1;˜(t2) \u0014 s2;::: ;˜(tk) \u0014 sk)= skZ −1 \u0001\u0001\u0001 s1Z −1 e − x2 1 478 CHAPTER 11. RANDOM PROCESSES Using the Cauchy-Schwartz inequality we deduce that, R˜(t1;t2)= ZZ xypt1;t2(x; y)dxdy \u0014 \u0002ZZ x2pt1;t2(x; y)dxdy\u00031=2\u0002ZZ y2pt1;t2(x; y)dxdy\u00031=2 = \u0002ZZ x2pt1(x)dx\u00031=2\u0002ZZ y2pt2(y)dy\u00031=2: Hence, we have the estimate jR˜(t1;t2)j\u0014 p 11.2. BASIC DEFINITIONS 479 De\fnition 11.2.2. If ˜t is a weak sense stationary random process and r˜ is integrable then its Fourier transform, S˜(˘)= 1Z −1 r˜(˝ )e −i˝ ˘d˝ is called the spectral density function for the process ˜: The autocorrelation function is not always integrable but, as shown below, it is a \\non- negative, de\fnite function.\" It then follows from a theorem of Herglotz that its Fourier transform is well de\fned as a measure onR: This means that, while S˜(˘)may notbewell de\fned at points, for any [a; b] the integral 1 480 CHAPTER 11. RANDOM PROCESSES The fourth fact is not obvious from the de\fnition. This follows because the autocorrelation function rχ(˝ )is a non-negative de\fnite function. This means that for any vectors (x1;:::;xN )and (˝1;::: ;˝N ), we have that NX i,j=1 rχ(˝i − ˝j )xixj \u0015 0: This is a consequence of the fact that the expected value of a non-negative, random variable is non-negative. If ˜(t) is any continuous time random process with \fnite mean and covariance then 0 \u0014 < j NX 1 xi˜(˝i))j 2 >= X i,j <xixj ˜i(˝i)˜j(˝j) >= X i,j xixjRχ(˝i;˝j): Hence, PN i,j=1 Rχ(˝i;˝j)xixj \u0015 0. For a weak sense stationary process, Rχ(˝1;˝2)= rχ(˝1 − ˝2)and thus NX i,j=1 rχ(˝i − ˝j )xixj \u0015 0: (*) The Herglotz theorem states that a function is the Fourier transform of a positive measure if and only if it is non-negative de\fnite. Hence, the fact that rχ is non-negative de\fnite implies that Sχd˘ is a non-negative measure, that is bZ a Sχ(˘)d˘ = bZ a 1Z −1 rχ(˝ )e−iτ ξd˝ \u0015 0; see [40]. Fact (5) follows from the Fourier inversion formula. 11.2. BASIC DEFINITIONS 481 We compute the expected value of jb˜T (˘)j2; E[jb˜T (˘)j 2]= E 2 4 TZ −T ˜(t)e −it˘dt TZ −T ˜(s)e is˘ds 3 5 = TZ −T TZ −T r˜(t − s)e −i(t−s)˘dtds: (11.7) Letting ˝ = t − s we obtain E[jb˜T (˘)j 2]= 2TZ −2T (2T −j˝ j)r˜(˝ )e −i˝ ˘d˝ =(2T ) 2TZ −2T \u00121 − j˝ j 482 CHAPTER 11. RANDOM PROCESSES Exercise 11.2.7. Suppose that ˜(t) is a random process so that E[j˜(t)j] is independent of t: Show either ˜(t) \u0011 0; with probability one or 1Z −1 j˜(t)jdt = 1; with probability one. 11.2.3 Independent and stationary increments Many processes encountered in imaging applications are not themselves stationary but satisfy the weaker hypothesis of having stationary increments. De\fnition 11.2.3. Let ˜t be a continuous parameter random process such that for any \fnite sequence of times t1 <t2 < \u0001\u0001\u0001 <tn;n \u0015 3 the random variables ˜t2 − ˜t1;˜t3 − ˜t2;::: ;˜tn − ˜tn−1 are independent. The process is said to have independent increments.If moreover Prob(˜t− ˜s \u0014 \u0015) depends only on t − s then the process has stationary increments. A weaker condition is that a process have uncorrelated increments, that is E[(˜t2 − ˜s2)(˜t1 − ˜s1)= E[(˜t2 − ˜s2)]E[(˜t1 − ˜s1)]; provided that [t2;s2] \\ [t1;s1]= ;: If E[j˜t − ˜sj2] only depends on t − s then the process is said to have wide sense stationary increments. Example 11.2.4. Brownian motion is a random process, parametrized by [0; 1) which de- scribes, among other things the motion of tiny particles in a ﬂuid. It is de\fned as a process ˜t with independent increments, such that for every s; t the increment ˜t − ˜s is a Gaussian random variable with E[˜t − ˜s]= 0 and E[(˜t − ˜s) 2]= ˙2jt − sj 2 This process is often normalized by \fxing ˜0 = a 2R; with probability 1. A very important fact about Brownian motion is that it is essentially the only process with independent increments whose sample paths are continuous, with probability one. Brownian motion is frequently called the Wiener process. Exercise 11.2.8. Show that if a Gaussian process has E[˜t] = 0 and uncorrelated incre- ments then it has independent increments. 11.3 Examples of random processes For many applications a small collection of special random processes su\u000ece. Several have already been de\fned, a few more are described in this section. 11.3. EXAMPLES OF RANDOM PROCESSES 483 11.3.1 Gaussian random process A Gaussian random process is a family, f˜(t)g or sequence of random variables f˜ig which, for each t (or i) is a Gaussian random variable. The \fnite dimensional distributions are also assumed to be Gaussian. As we saw in section 10.3.3, the joint distributions for Gaussian random variables are determined by their means and covariance matrix. This remains true of Gaussian processes and again the converse statement is also true. Suppose that T is the parameter space for a random process and that there are real valued functions \u0016(t) de\fned on T and r(s; t) de\fned on T \u0002 T: The function r is assumed to satisfy the conditions (1). For any pair s; t 2 Tr(s; t)= r(t; s) and, (2). If ft1;::: ;tmgˆ T then the matrix [r(ti;tj)] is non-negative de\fnite. There exists a Gaussian random process f˜t : t 2 T g such that E[˜t]= \u0016(t)and E[˜s˜t]= r(s; t): If one is only concerned with the second order statistics of a random process then one is free to assume that there process is Gaussian. Brownian motion, de\fned in example 11.2.4, is an important example of a Gaussian process. As remarked there, we can \fx ˜0 = 0 with probability one. Since E[˜t − ˜s]= 0 for all t; s it follows that E[˜t] = 0 for all t 2 [0; 1): The autocorrelation function can now be computed using the hypothesis E[(˜t − ˜s) 2]= ˙2jt − sj: (11.10) Let 0 <s <t then as ˜0 = 0 with probability one, E[˜s˜t]= E[(˜s − ˜0)(˜t − ˜0)]: This can be rewritten as E[˜s˜t]= E[(˜s − ˜0) 2]+ E[(˜s − ˜0)(˜t − ˜s)] = ˙2s2 = ˙2 minfjsj; jtjg: (11.11) In passing from the \frst line to the second we use the independence of the increments and (11.10). Thus Brownian motion is not a weak sense stationary process. 11.3.2 The Poisson counting process The Poisson counting process is another example of a process with independent, stationary increments. This process is a family of random variables f˜(t)g de\fned for t \u0015 0, which take values in f0; 1; 2;:::g: The Poisson counting process has a nice axiomatic characterization. For convenience let: P (k; t)=Prob(˜(t)= k): Here are the axioms phrased in terms of counting \\emitted particles:\" 484 CHAPTER 11. RANDOM PROCESSES Independent increments: The number of particles emitted in [t1;t2] is independent of the number in [t3;t4] if [t1;t2] \\ [t3;t4]= ;. Short time behavior: The probability that one particle is emitted in a very short time interval is given by P (1; \u0001t)= \u0015\u0001t + o(\u0001t) for some constant \u0015 where o(\u0001t) denotes a term such that lim\u0001t!0 o(\u0001t)=\u0001t =0: As a consequence P (1; 0) = 0: Stationary increments: The process has stationary increments. Prob(˜(t) − ˜(s)= k)=Prob(˜(t + ˝ ) − ˜(s + ˝ )= k); 8˝ \u0015 0; 0 \u0014 s \u0014 t: We can now estimate the probability that two particles are emitted in a short interval [0; \u0001t] : In order for this to happen there must be a 0 <˝ < \u0001t such that one particle is emitted in [0;˝ ] and one is emitted in (˝; \u0001]: The hypothesis of independent, stationary increments implies that P (2; \u0001t) \u0014 max ˝ 2(0;\u0001t) P (1;˝ )P (1; \u0001t − ˝ )= O((\u0001t) 2): From the independent, stationary increments axiom, we have that P (0;t +\u0001t)= P (0;t)P (0; \u0001t): For any time \u0001t it is clear that P (k; \u0001t) \u0014 P (k +1; \u0001t) \u0014 ::: and that 1X k=0 P (k; \u0001t)=1 In fact arguing as above one can show that P (k; \u0001t) \u0014 [P (1; \u0001t)]k; combining these obser- vation leads to P (0; \u0001t)+ P (1; \u0001t)=1 + o(\u0001t): (11.12) Hence P (0; \u0001t)= 1 − \u0015\u0001t + o(\u0001t); (11.13) P (0;t +\u0001t)= P (0;t)P (0; \u0001t)= P (0;t)[1 − \u0015\u0001t + o(\u0001t)]: (11.14) Letting \u0001t ! 0, we have lim \u0001t!0 P (0;t +\u0001t) − P (0;t) 11.3. EXAMPLES OF RANDOM PROCESSES 485 The solution of this equation is P (0;t)= e −\u0015t: The probabilities fP (k; t)g for k> 1 are obtained recursively. For each t \u0015 0and j \u0014 k suppose that P (j; t)= (\u0015t)j 486 CHAPTER 11. RANDOM PROCESSES This says that each time in [0;T ] is equally probable. The Poisson counting process is used to describe radioactive decay. If it is known that one decay was observed in certain interval then, the time of decay is uniformly distributed over the interval. This is why it is said that the time of decay is \\completely random.\" Next we compute the autocorrelation function, E[˜(t)˜(s)]. It follows from the identity identity: E[(˜(t) − ˜(s)) 2]= E[˜(t) 2 − 2˜(t)˜(s)+ ˜(s) 2] = E[˜(t) 2] − 2E[˜(t)˜(s)] + E[˜(s) 2]: From the stationary increments property, and ˜(0) = 0, it follows that E[(˜(t) − ˜(s)) 2]= E[˜(t − s) − ˜(0)) 2]= E[˜(t − s) 2]: Assume that t \u0015 s; then E[˜(t − s) 2]= 1X k=0 k2 Prob(˜(t − s)= k) = 1X k=0 1 11.3. EXAMPLES OF RANDOM PROCESSES 487 The hypothesis that the original process f˜(t)g has independent increments implies that fZig are independent random variables. They are identically distributed because the counting process has stationary increments. The original process is a function of a continuous parameter which takes integer values. The arrival process and its increments are sequences of random variables indexed by positive integers taking continuous values. We now work out the distribution function for these two processes. The probability that the \frst particle arrives after time t equals the probability that ˜(t) is zero: Prob(Z1 >t)=Prob(˜(t)=0) = e −\u0015t: Hence, Prob(Z1 \u0014 t)=1 − e −\u0015t = tZ −1 \u0015e −\u0015t˜[0;1](t)dt: (11.19) The density function of Z1, hence that of Zi for each i,is \u0015e−\u0015t˜[0;1](t): The expected value of Z1 is E[Z1]= 1Z 0 t\u0015e −\u0015tdt = 1 488 CHAPTER 11. RANDOM PROCESSES The probability distribution for Tn is therefore: Prob(Tn \u0014 t)= tZ 0 \u0015e−\u0015t(\u0015t)n−1 11.3. EXAMPLES OF RANDOM PROCESSES 489 Proposition 11.3.1. If ˜t is a mean square T -periodic, weak sense stationary random process then r˜(˝ ) is T -periodic. Proof. We need to show that rχ(˝ + T )= rχ(˝ ) for any ˝: The proof is a simple computation: rχ(˝ + T )= E[˜0˜τ +T ] = E[˜T ˜τ +T ]+ E[(˜0 − ˜T )˜τ +T ] = rχ(˝ )+ E[(˜0 − ˜T )˜τ +T ]: (11.20) The Cauchy-Schwarz inequality, (10.2.2) gives the estimate E[(˜0 − ˜T )˜τ +T ] \u0014 p 490 CHAPTER 11. RANDOM PROCESSES Proof. Once again the proof is a simple computation interchanging an expectation with integrals over [0;T ]: E[\u0010k 11.3. EXAMPLES OF RANDOM PROCESSES 491 Another way to state the conclusion of the proposition is that the series in (11.24) represents ˜t; in the mean, with probability one if and only if r˜ is represented pointwise by its Fourier series at 0: This in turn depends on the regularity of r˜(˝ )for ˝ near to zero. Remark 11.3.2. This material is adapted from [13] where a thorough treatment of eigen- function expansions for random processes can be found. Exercise 11.3.3. Prove (11.21). 11.3.5 White noise 492 CHAPTER 11. RANDOM PROCESSES makes perfect sense. Similarly, one can give as precise meaning to time averages of white noise. If f (t) is a continuously di\u000berentiable function, then Wf = bZ a f (t)Wtdt makes sense as a random variable; it has the \\expected\" mean and variance: E[Wf ]= bZ a f (t)E[Wt]dt =0; and E[W 2 f ]= ˙2 bZ a f 2(t)dt: In a similar way it makes sense to pass white noise through an su\u000eciently smoothing, linear \flter. It is much more complicated to make sense of non-linear operations involving white noise. The sample paths for a white noise process are usually described as the derivatives of the sample paths of an ordinary continuous time random process. Of course the sample paths of a random process are essentially never classically di\u000berentiable, so these derivatives must be interpreted as weak derivatives. We close this section by explaining, formally why white noise can be thought of in this way. Let f˜t : t \u0015 0g denote Brownian motion and recall that E[˜t]=0 and E[˜s˜t]= ˙2 minfs; tg: Formally we set Wt = @t˜t: Commuting the derivative and the integral de\fning the expec- tation gives E[Wt]= E[@t˜t]= @tE[˜t]= 0; hence Wt has mean zero. To compute the variance we again commute the derivatives and the expectation to obtain E[WtWs]= @t@sE[˜t˜s]= @t@s˙2 minfs; tg: The right most expression is well de\fned as the weak derivative of a function of two vari- ables: @t@s˙2 minfs; tg =2˙2\u000e(t − s): (11.27) Let '(t; s) be a smooth function with bounded support in [0; 1)\u0002[0; 1); the weak derivative in (11.27) is de\fned by the condition 1Z 0 1Z 0 @t@s minfs; tg'(s; t)dsdt = 1Z 0 1Z 0 minfs; tg@t@s'(s; t)dsdt; 11.4. RANDOM INPUTS TO LINEAR SYSTEMS 493 for every test function ': Writing out the integral on the right hand side gives 1Z 0 1Z 0 minfs; tg@t@s'(s; t)dsdt = 1Z 0 1Z s s@t@s'(s; t)dtds + 1Z 0 1Z t t@s@t'(s; t)dsdt = − 1Z 0 s@s'(s; s)ds − 1Z 0 t@t'(t; t)dt =2 1Z 0 '(s; s)ds: (11.28) The last line follows by integration by parts, using the bounded support of ' to eliminate the boundary terms. At least formally, this shows that the \frst (weak) derivative of Brownian motion is white noise. Exercise 11.3.4. Give a detailed justi\fcation for the computations in (11.28). Exercise 11.3.5. Show that the \frst derivative of the Poisson arrival process is also, formally a white noise process. What do the sample paths for this process look like? 11.4 Random inputs to linear systems In the analysis of linear \flters, we often interpret the input and output as a deterministic part plus noise. The noise is modeled as a random process and therefore the output is also a random process. One often wishes to understand the statistical properties of the output in terms of those of the input. In this connection is is useful to think of a continuous parameter, random process as a function of two variables ˜(t; w); with w apoint in the sample space X and t atime. Recall that for w a point in the sample space X; t 7! ˜(t; w) is called a sample path. For a shift invariant \flter H with the impulse response function h, the output for such a random input is again a random process on the same sample space given formally by \u0007(t; w)= H(˜(t; w)) = 1Z −1 h(t − s)˜(s; w)ds: When writing such an expression we are asserting that it makes sense with probability one. The statistics of the output process \u0007 are determined by the impulse response and the statistics of the input process. The expected value of the output of a linear system is an iterated integral: E[\u0007(t)] = Z X \u0007(t; w)d\u0017(w) = Z X 1Z −1 h(t − s)˜(s; w)dsd\u0017(w): 494 CHAPTER 11. RANDOM PROCESSES Under reasonable hypotheses, e.g. ˜ is bounded and h is integrable, the order of the two integrations can be interchanged. Though this exchange of order may not be trivial because X is usually an in\fnite dimensional space. Interchanging the order of the integrations gives E[\u0007(t)] = 1Z −1 Z X h(t − s)˜(s; w)d\u0017(w)ds = 1Z −1 h(t − s)E[˜(s)]ds: The expected output of a linear \flter applied to a random process is the result of applying the linear \flter to the expected value of the random process. Some care is necessary, even at this stage. If E[˜(s)] is a constant, \u0016˜; then the expected value of \u0007 is E[\u0007(t)] = \u0016˜ 1Z −1 h(t − s)ds = \u0016˜ 1Z −1 h(s)ds = \u0016˜^h(0): For this to make sense, we should assume that R jh(s)j < 1. If the input random process ˜ is stationary then the output process H˜ is as well. Exercise 11.4.1. Suppose that ˜t is a stationary random process and H is a linear shift invariant \flter for which H˜ makes sense (with probability one). Show that (H˜)t is also a stationary process. 11.4.1 The autocorrelation of the output To analyze shift invariant, linear systems we used the Fourier transform. In this case, it cannot be used directly since noise does not usually have a Fourier transform in the ordinary sense. Observe that E[ 1Z −1 j˜(s)jds]= 1Z −1 E[j˜(s)j]ds: For a stationary process the integral diverges unless E[j˜(s)j]=0; which would imply that the process equals zero, with probability one! For a non-trivial, stationary process the Fourier transform does not exist as an absolutely convergent integral. To get around this di\u000eculty we consider the autocorrelation function. It turns out that the autocorrelation function for a stationary process is frequently square integrable. For a non-stationary process, the autocorrelation function R˜is de\fned by R˜(t1;t2)= E[˜(t1)˜(t2)]: The process is weak sense stationary if there is a function r˜ so that the autocorrelation function is R˜(t1;t2)= r˜(t1 − t2): 11.4. RANDOM INPUTS TO LINEAR SYSTEMS 495 Given two random processes, ˜(t); \u0007(t) on the same underlying probability space the cross- correlation function is de\fned to be R˜;\u0007(t1;t2)= E[˜(t1)\u0007(t2)]: For two stationary processes R˜;\u0007 is only a function of t2 − t1; we de\fne r˜;\u0007(˝ )= E[˜(t)\u0007(t + ˝ )]: Now suppose that H is a linear shift \flter, with impulse response h and that ˜t is a random process for which H˜t makes sense. The autocorrelation of the output process is RH˜(t1;t2)= E[H˜(t1)H˜(t2)] = E[ 1Z −1 h(t1 − s1)˜(s1)ds1 1Z −1 h(t2 − s2)˜(s2)ds2]: (11.29) The expected value is itself an integral, interchanging the order of the integrations leads to RH˜(t1;t2)= E[ 1Z −1 h(t1 − s1)˜(s1; w)ds1 1Z −1 h(t2 − s2)˜(s2; w)ds2] = 1Z −1 1Z −1 h(t1 − s1)h(t2 − s2)E[˜(s1; w)˜(s2; w)]ds1ds2 = 1Z −1 1Z −1 h(t1 − s1)h(t2 − s2)R˜(s1;s2)ds1ds2 =[h (2) \u0003 R˜](t1;t2): where h(2)(x; y):= h(x)h(y). Hence, RH˜ = h(2) \u0003 R˜ is expressible as a two dimensional convolution with R˜: For the case of a weak sense stationary process the result is simpler, recall that R˜(t1;t2)= r˜(t1 − t2): Letting ˝i = ti − si;i =1; 2weobtain RH˜(t1;t2)= 1Z −1 1Z −1 h(˝1)h(˝2)r˜(˝1 − ˝2 + t2 − t1):d˝1d˝2: Thus the output is also weak sense stationary with rH˜(˝ )= 1Z −1 1Z −1 h(s + t)h(t)r˜(s − ˝ )dtds: 496 CHAPTER 11. RANDOM PROCESSES In Proposition 11.2.1 the properties of the power spectral density of a stationary random process are enumerated. Using the formula for rH˜ we compute the spectral power density of the output in terms of the spectral power density of the input obtaining SH˜(˘)= j^h(˘)j 2S˜(˘): (11.30) This is consistent with the \\determinate\" case for if x is a \fnite energy signal, with y = Hx and ^y = ^h^x we have j^y(˘)j 2 = j^h(˘)j 2j^x(˘)j 2: (11.31) If h(˘) is large for large values of ˘, then the linear \flter ampli\fes the noise. Note that the total power of the output is given by E[(H˜) 2]= rH˜(0) = 1 11.4. RANDOM INPUTS TO LINEAR SYSTEMS 497 understand this dependence we examine the result of using a white noise voltage source as the input to the simple electrical circuit shown in \fgure 11.1. R V 1 (t) V 2 (t) V(t) L Figure 11.1: An RL-circuit. Thermodynamic considerations show that the expected power through the circuit, due to the thermal ﬂuctuations of the electrons is E[ LI 2 498 CHAPTER 11. RANDOM PROCESSES This result and its generalizations are also known as Nyquist's theorem. At room temper- ature (about 300\u000eK) with a resistance R =106 Ohms, the intensity of the Johnson noise process is ˙2 ˇ 4 \u0002 10 −15(volt)2sec: Of course, in a real physical system the spectrum of the thermal noise cannot be ﬂat, for this would imply that the noise process contains an in\fnite amount of energy. It is an empirical fact that the spectrum is essentially ﬂat up to a fairly high frequency. Indeed Johnson noise is sometimes describes as a random process, ˜ with S˜(˘)= ˙2˜[0;B](j˘j); or brieﬂy, as bandlimited white noise. The integral above from −1 to 1 is then replaced by an integral from −B to B: If B is reasonably large then the result is nearly the same. The total power of the (bandlimited) Johnson noise is therefore Stot = RkT B 11.4. RANDOM INPUTS TO LINEAR SYSTEMS 499 Expanding the square and di\u000berentiating in t gives E[((h \u0003 x − s)k \u0003 x)(t1)] = 0 for any k and t1: (11.36) Given that the various convolutions make sense, the derivation up to this point has been fairly rigorous. Choose a smooth, non-negative function '(t) with bounded support and total integral 1. For any t2; taking k\u000f(t)= 1 500 CHAPTER 11. RANDOM PROCESSES Exercise 11.4.4. Prove that if H de\fnes the optimal \flter then E[jHxj 2]= E[sHx]: (11.38) Exercise 11.4.5. Using (11.38), compute the expected mean squared error for the optimal \flter, H E[jHx − sj 2]= rs(0) − 1Z −1 h(t)rs(t)dt: (11.39) Exercise 11.4.6. Using the Parseval formula and (11.39) prove that E[jHx − sj 2]= 1 Chapter 12 Resolution and noise in the ﬁltered backprojection algorithm In Chapter 8 we determined the point spread function of the measurement and reconstruc- tion process for a parallel beam scanner. It is shown in examples that if there is at least one sample per beam width (as de\fned by the beam pro\fle function) then the resolution in the reconstructed image is determined by the beam width. Using a rectangular pro\fle, the resolution is about half the beam width. A second conclusion of that analysis is that the e\u000bects of aliasing, resulting from ray sampling are well controlled by using the Shepp- Logan \flter and a Gaussian focal spot. Decreased sample spacing sharpens the peak of the PSF and does not produce oscillatory side lobes. Finally, the e\u000bect of view sampling is an oscillatory artifact, appearing at a de\fnite distance from a hard object. The distance is proportional to \u0001\u0012−1: This analysis, and considerable empirical evidence shows that, by decreasing \u0001\u0012 one can obtain an artifact free region of any desired size. In this chapter we consider how noise in the measurements obtained in a CT scanner propagates through a reconstruction algorithm. The principal source of noise in CT-imaging is quantum noise. This is a consequence of the fact that X-rays \\beams\" are really com- posed of discrete photons, whose number ﬂuctuates in a random way. An X-ray source is usually modeled as Poisson random process, with the intensity equal to the expected number of photons per unit time. Recall that the signal-to-noise ratio in a Poisson random variable, ˜ equals p 502 CHAPTER 12. RESOLUTION AND NOISE Here f is the absorption coe\u000ecient of the object and RW f is the Radon transform averaged with the beam pro\fle. The signal-to-noise ratio provides a measure of the useful information in the reconstructed image. If ˜ is a random variable then recall that the signal-to-noise ratio is de\fned by SNR = E[˜] 12.1. THE CONTINUOUS CASE 503 Its Fourier transform Si(˘) is the power spectral density in the input noise process. The power spectral density of the output is given by (11.30), So(˘)= Si(˘)j ^Ψ(˘)j 2: The total noise power in the output is therefore Stot = 1 504 CHAPTER 12. RESOLUTION AND NOISE where, strictly speaking \u0012 should be understood in this formula as \u0012 mod 2ˇ: That the errors from ray to ray are weakly correlated is not an unreasonable hypothesis, however the analysis in section 10.3.7, particularly equation (10.44), shows that the variance is unlikely to be constant. These assumptions give E[nm˚(x; y)nm˚(0; 0)] = ˙2 12.2. SAMPLED DATA 505 The sample spacing in the a\u000ene parameter is denoted by d: Given the normalization of the spatial coordinates N = 2 506 CHAPTER 12. RESOLUTION AND NOISE Remark 12.2.1 (Important notational remark). In the remainder of this chapter, the notation \u0015f˚ refers to a reconstruction using noisy data, as in (12.12). This allows us to distinguish, such approximate reconstructions from the approximate reconstruction, ~f˚ made with \\exact\" data. Expanding the square in the reconstruction formula gives \u0015f 2 ˚ =( d 12.3. A COMPUTATION OF THE VARIANCE 507 If ˚ satis\fes (12.10), then Z 1 −1 ˚ 2(t)dt ˇ 1 508 CHAPTER 12. RESOLUTION AND NOISE Let \u0012 denote the direction, !(\u0012)and N\u0012(kd) the number of photons measured for the ray lkd;!(\u0012): Let ideal result would give P\u0012(kd)= Z lkd,ω(θ) fds: For each ray the number of measured photons is a Poisson random variable. Let \u0016N\u0012(kd) denote the expected value E[N\u0012(kd)]. For simplicity we assume that, Nin the number of incident photons in each beam is a deterministic \fxed, large number. Beer's law is the statement that \u0016N\u0012(kd)= Nine −Pθ(kd): Because N\u0012(kd) is a Poisson random variable its probability distribution is determined by its expected value, Prob(N\u0012(kd)= l)= [ \u0016N\u0012(kd)]le− \u0016Nθ(kd) 12.3. A COMPUTATION OF THE VARIANCE 509 Using Taylor's formula we derive an expression for the di\u000berence, E[log N\u0012(kd)] − log E[N\u0012(kd)]: Let y be a non-negative random variable with density function p(y); for which \u0016y = 1Z −1 yp(y)dy and ˙2 = 1Z −1 (y − \u0016y) 2p(y)dy: Assuming that \u0016y is a large number and that p is sharply peaked around its mean, E[log y]= Z 1 0 (log y)p(y)dy = Z 1 −\u0016y log(x +\u0016y)p(x +\u0016y)dx = Z 1 −\u0016y \u0014log \u0016y +log(1 + x 510 CHAPTER 12. RESOLUTION AND NOISE The variance is Var(P m \u0012 (kd)) = E[(P m \u0012 (kd) − P\u0012(kd)) 2] = E[(log \u0012 N\u0012(kd) 12.3. A COMPUTATION OF THE VARIANCE 511 Using the formula \u0016N\u0012i(kd)= Nine −Pθ(kd); the variance can be rewritten Var( \u0015f˚(x; y)) = ( ˇd 512 CHAPTER 12. RESOLUTION AND NOISE Exercise 12.3.2. Find a formula for Var( \u0015f˚(x; y)) for other points in the disk. Graph the result, for \fxed values of (d; m): Exercise 12.3.3. Find a formula for Cov( \u0015f˚(0; 0); \u0015f˚(x; y)): Graph the result, for \fxed values of (d; m): 12.3.3 Signal-to-noise ratio, dosage and contrast We now compute the signal-to-noise ratio at the center of a homogeneous disk of radius R and absorption coe\u000ecient m. From the previous section we have \u0016N0 = Nine −2mR; E[ \u0015f˚(0; 0)] ˇ ~f˚(0; 0): Approximating the integral in (12.22) gives Var( ~f˚(0; 0)) ˇ ˇ2d 12.3. A COMPUTATION OF THE VARIANCE 513 values between -1000 (air) and 1000 (bone), see table 2.1. The structures of interest are usually soft tissues and these occupy a tiny part of this range, about -50 to 60, or %5. The signal-to-noise ratio in the measurements determines the numerical resolution or accuracy in the reconstructed absorption coe\u000ecient. In imaging this is called contrast.Noise in the measurements interferes with the discrimination of low contrast objects, that is, contiguous objects with very similar absorption coe\u000ecients. In clinical applications the image is usually viewed on a monitor and the range of grays or colors available in the display is mapped to a certain part of the dynamic range of the reconstructed absorption coe\u000ecient. If, for example the features of interest lie between 0 and 100 Houns\feld units then everything below 0 is mapped to white and everything above 100 to black. If the absorption coe\u000ecient is reconstructed with an accuracy of 1=2% then a di\u000berence of 10 Houns\feld unit is meaningful and, by scaling the range of displayed values, should be discernible in the output. If on the other hand, the measured values are only accurate to %2 or 40 Houns\feld, then the scaled image will have a mottled appearance and contain little useful information. The accuracy of the reconstruction should not be confused with the spatial resolution. In prosaic terms the accuracy is the number of signi\fcant digits in the values of the recon- structed absorption coe\u000ecient. The reconstructed values approximate spatial averages of the actual absorption coe\u000ecient over a pixel, or if the slice thickness is included, voxel of a certain size. The spatial resolution is a function of the dimensions of a voxel. This in turn, is largely determined by the beam width, sample spacing in the a\u000ene parameter and FWHM of the reconstruction algorithm. Increasing the resolution is essentially the same thing as decreasing the parameter \u000e in (12.23). If the dosage is \fxed then this leads to a decrease in the SNR and a consequent decrease in in the contrast available in the recon- structed image. Joseph and Stockham give an interesting discussion of the relationship of contrast and resolution in CT images, see [38]. Remark 12.3.1. Our discussion of SNR is adapted from [4]. 514 CHAPTER 12. RESOLUTION AND NOISE Appendix A Background material In applied subjects mathematics needs to be appreciated in three rather distinct ways: 1. In the abstract context of perfect and complete knowledge generally employed in mathematics itself, 2. In a less abstract context of fully speci\fed, but incompletely known functions, this is the world of mathematical approximation. 3. In a realistic context of partially known functions and noisy, approximate data, this is closer to the real world of measurements. With these di\u000berent perspectives in mind, we introduce some of the mathematical concepts underlying image reconstruction and signal processing. The bulk of this material is usually presented in undergraduate courses in linear algebra, analysis and functional analysis. In- stead of a giving the usual development, which emphasizes mathematical rigor and proof techniques, we present this material from an engineering perspective. Many of the results are proved in exercises and examples are given to illustrate general phenomena. This mate- rial is intended to \fll in background material and recast familiar material in a more applied framework; it should be referred to as needed. A.1 Numbers We begin by discussing numbers, beginning with the abstract concept of numbers and their arithmetic properties. Representations of number are then considered, leading to a com- parison, between abstract numbers and the way numbers are actually used in computation. A.1.1 Integers Mathematicians think of numbers as a set which has two operations, addition and mul- tiplication, which satisfy certain properties. The mathematical discussion of this subject always begins with the integers. We denote the set of integers byZand the set of positive integers (the whole or natural numbers) byN: There are two operations de\fned on the in- tegers addition, + and multiplication, \u0002: Associated to each of these operations is a special number: For addition that number is 0 it is de\fned by the property n +0 = 0 + n = n for every integer n: For multiplication that number is 1 and it is de\fned by the property n \u0002 1=1 \u0002 n = n for every integer n: 515 516 APPENDIX A. BACKGROUND MATERIAL The important axiomatic properties of addition and multiplication are Commutative law: n + m = m + n; n \u0002 m = m \u0002 n; for every m; n 2Z; Associative law: (n + m)+ p = n +(m + p); (n \u0002 m) \u0002 p = n \u0002 (m \u0002 p); for every m; n; p 2Z; Distributive law: (m + n) \u0002 p = m \u0002 p + n \u0002 p; for every m; n; p 2Z: These rules are familiar from grade school and we use them all the time when we do computations by hand. In mathematics numbers are treated in an axiomatic way. Neither a representation of numbers nor an algorithm to perform addition and multiplication has yet to be considered. We normally use the decimal representation, when working with numbers \\by hand.\" To de\fne a representation of numbers we \frst require some special symbols; for the decimal representation we use the symbols 0; 1; 2; 3; 4; 5; 6; 7; 8; 9 which represent the numbers zero through nine. We also introduce an additional symbol − to indicate that a number is smaller than zero. The decimal representation of an integer is a string of numbers amam−1 ::: a1a0 where 0 \u0014 aj \u0014 9; for j =0;::: ;m: What does this string of numbers mean? By de\fnition amam−1 ::: a1a0 = mX j=0 aj10 j : What appears on the right hand side of this formula is a mathematical number, what appears on the left is its decimal or base 10 representation. A negative number is represented by prepending the minus sign −am ::: a0: For each positive integer k> 1there is an analogous representation for integers called the base-k or k-ary expansion. The usual algorithms for adding and multiplying numbers revolve around the decimal representation. To do addition we need to know how to do the sums a + b for 0 \u0014 a; b \u0014 9; then we use \\carrying\" to add larger numbers. To do multiplication we need to know how to do the products a \u0002 b for 0 \u0014 a; b \u0014 9: The algorithms for these operations require an addition and multiplication table. The normal human mind has no di\u000eculty remembering these base 10 addition and multiplication tables. Especially in the early days, this was a large burden to place on a machine. It was found to be much easier to build a machine that uses a base 2 or binary representation to store and manipulate numbers. In a binary representation an integer is represented as a string of zeros and ones. By de\fnition bmbm−1 ::: b1b0 = mX j=0 bj2 j where bj 2f0; 1g for j =0;::: ;m: The analogous algorithms for adding and multiplying in base 2 only require a knowledge of a + b; a \u0002 b for 0 \u0014 a; b \u0014 1: That is a lot less to remember. On the other hand you need to do a lot more carrying, to add or multiply numbers of a given size. A.1. NUMBERS 517 Even in this very simply example we see that there is a trade o\u000b in e\u000eciency of com- putation between the amount of memory utilized and the number of steps needed to do a certain computation. There is a second reason why binary representations are preferred for machine computation. For a machine to evaluate a binary digit, it only needs to distinguish between two possible states. This is easy to do, even with inexpensive hardware. To eval- uate a decimal digit, a machine would need to distinguish between ten di\u000berent possible states. This would require a much more expensive machine. Finally there is the issue of tradition. It might be cheaper and more e\u000ecient to use base 3 for machine computation, but the mere fact that so many base 2 machines already exist make it highly unlikely that we will soon have to start to learn to do arithmetic in base 3. Because we have a conceptual basis for numbers, there is no limit to size of the numbers we can work with. Could a given number N be the largest number we can \\handle?\" It would be hard to see why, because if we could handle N then we could certainly N +1: In fact this is essentially the mathematical proof that there is no largest integer. The same cannot be said of a normally programmed computer, it has numbers of maximum size with which it can work. Exercise A.1.1. Write algorithms to do addition and multiplication using the decimal representation of numbers. Exercise A.1.2. Adding the symbols A; B; C; D; E to represent the decimal numbers 10; 11; 12; 13; 14; 15 leads to the hexadecimal of base-16 representation of numbers. Work out the relationship between the binary and hexadecimal representations. Write out the addition and multiplication tables in hexadecimal. A.1.2 Rational numbers The addition operation also has an inverse operation which we call subtraction: given a number n there is a number −n which has the property n +(−n)=0: We are so used to this that it is di\u000ecult to see this as a \\property,\" but note that, if we are only permitted to use integers then the multiplication operation does not have an inverse. This can be thought of in terms of solving equations: any equation of the form x + m = n where m; n 2Zhas an integer solution x = n − m: On the other hand, for many choices of m; n 2Zthe equation n \u0002 x = m (A.1) does not have an integer solution. Again we learned in grade school how to handle this problem: we introduce fractions and then (A.1) has the solution x = m 518 APPENDIX A. BACKGROUND MATERIAL is the same as the solution to (A.1). This means that the number represented by the symbol p\u0002m A.1. NUMBERS 519 have such a \fnite decimal representation. For some purposes the representation as fractions is more useful, it is certainly more e\u000ecient. For example using a fraction we have an exact representation of the number 1=3; using long division we \fnd that 1 520 APPENDIX A. BACKGROUND MATERIAL constant throughout the range of representable numbers. On the other hand it places subtle constraints on the kinds of computations that can accurately be done. For examples, subtracting numbers of vastly di\u000berent sizes does not usually give a meaningful result. Since we only have \fnitely many digits, computations done in a computer are essen- tially never exact. It is therefore very important to use algorithms that are not sensitive to repeatedly making small errors of approximation. In image reconstruction this is an impor- tant issue as the number of computations used to reconstruct a single image is usually in the millions. For a thorough discussion of treatment of numbers in machine computation see [78]. Exercise A.1.3. Show that the condition in (A.3) is the correct condition to capture the elementary concept that two fractions represent the same number. Exercise A.1.4. Show that formula (A.4) de\fnes an operation on rational numbers. That is if m A.1. NUMBERS 521 The distance between two numbers x and y is de\fned to be d(x; y) d = jx − yj: It is easy to see that this has the basic property of a distance, the triangle inequality d(x; y) \u0014 d(x; z)+ d(z; y): (A.5) This relation is called the triangle inequality by analogy with the familiar fact from Eu- clidean geometry: the shortest route between two points is the line segment between them, visiting a third point only makes the trip longer. Sequences A sequence of real numbers is an in\fnite, ordered list of numbers. Frequently the terms of a sequence are labeled or indexed by the positive integers x1;x2;x3;::: The notation <xn > refers to a sequence indexed by n: A sequence is bounded if there is a number M so that jxnj\u0014 M for all choices of the index n: It is monotone increasing if xn \u0014 xn+1 for all n: The de\fnition of limits and the completeness axiom for the real numbers are Limits: If <xn > is a sequence of real numbers then we say that <xn > converges to x if the distances, d(xn;x) can be made arbitrarily small by taking the index su\u000eciently large. More technically, given a positive number \u000f> 0we can \fnd an integer N so that d(xn;x) <\u000f provided that n>N: In this case we say the \\limit of the sequence <xn > is x\" and write lim n!1 xn = x: Completeness Axiom: If <xn > is a monotone increasing, bounded sequence of real numbers then <xn > converges to limit, that is there exists an x 2Rsuch that limn!1 xn = x: From the completeness axiom it is easy to show that bounded, monotone decreasing se- quences also converge. The completeness axiom is what distinguishes the real numbers from the rational numbers. It is, for example, not di\u000ecult to construct a bounded, monotone sequence of rational numbers <xn > which get closer and closer to p 522 APPENDIX A. BACKGROUND MATERIAL Using the completeness axiom it is not di\u000ecult to show that every real number has a decimal expansion. That is, given a positive real number x we can \fnd a (possibly in\fnite) sequence <am;am−1; \u0001\u0001\u0001 > of numbers such that 0 \u0014 aj \u0014 9and x = lim N !1 2 4 mX j=−N aj10 j 3 5 : In this context the index for the sequence <aj > is decreasing and tends to −1: If x has only \fnitely many non-zero terms in its decimal expansion then, by convention we set all the remaining digits to zero. To study such in\fnite decimal expansions it is useful to have a formula for the sum of a geometric series. Proposition A.1.1. If r 2Rand N 2Nthen NX j=0 rj = rN +1 − 1 A.1. NUMBERS 523 Remark A.1.1. Notational remark As is serves no further pedagogical purpose to use \u0002 to indicate multiplication of numbers we henceforth follow the standard notation of indicating multiplication of numbers by juxtaposition: If a; b are numbers then ab is the product of a and b: Exercise A.1.9. De\fne the sequence by letting x0 =2 and xj = 1 524 APPENDIX A. BACKGROUND MATERIAL If one imagines \\observing\" a sequence of numbers, then it seems unlikely the one could directly observe its limit, if it exists. On the other hand, the clustering described in the Cauchy criterion is something which is readily observed. Example A.1.1. Let xn = n−1; if n<m then jxn − xmj\u0014 1 A.2. VECTOR SPACES 525 Recall the de\fnition of the derivative, a function f (x)has derivative f 0(x)at x provided that f (x + h) − f (x)= f 0(x)h + e(h) (A.10) where e(h) goes to zero faster that jhj as h ! 0: This formula tells us that replacing f (x + h) − f (x)by the linear function f 0(x)h leads to an error of size smaller than jhj: If f 0(x) 6= 0 then (A.10) gives a complete qualitative picture of f for arguments near to x; which is increasingly accurate as h ! 0: Of course if f 0(x) = 0 then (A.10) says little beyond that f is not well approximated by a linear function near to x: GeometricallyRis usually represented by a straight line, the numbers are coordinates for this line. One can specify coordinates on a plane by choosing two, intersecting straight lines and coordinates in space are determined by choosing three lines which intersect in a point. Of course one can continue in this way. We denote the set of ordered pairs of real numbers byR2 = f(x; y) j x; y 2Rg and the set of ordered triples byR3 = f(x; y; z) j x; y; z 2Rg: These are known as the Euclidean 2-space and 3-space respectively. From a mathematical perspective there is no reason to stop at 3, for each n 2Nwe letRn denote the set of ordered n-tuples (x1;x2;::: ;xn) of real numbers. This is called the Euclidean n-space or just n-space for short. From a physical perspective we can think of n-space as giving (local) coordinates for a system with n-degrees of freedom. The physical space we occupy is 3- space, if we include time then this gives us 4-space. If we are studying the weather then we would want to know the temperature, humidity and barometric pressure at each point in space-time, so this requires 7 parameters (x; y; z; t; T; H; P ): The more complicated the physical model the more dimensions one requires to describe it. A.2.1 Euclidean n-space All the Euclidean n-spaces have the structure of linear or vector spaces. This means that we know how to add two n-tuples of real numbers (x1;::: ;xn)+ (y1;::: ;yn)=(x1 + y1;::: ;xn + yn) and multiply an n-tuple of real numbers by a real number a \u0001 (x1;::: ;xn)= (ax1;::: axn): These two operations are compatible in that a \u0001 (x1;::: ;xn)+ a \u0001 (y1;::: ;yn)= a \u0001 (x1 + y1;::: ;xn + yn) =(a(x1 + y1);::: ;a(xn + yn)): An ordered n-tuple of numbers is called an n-vector or vector. The \frst operation is called vector addition (or just addition) and the second operation is called scalar multiplication. For most values of n there is no way to de\fne a compatible notion of \\vector multiplication,\" 526 APPENDIX A. BACKGROUND MATERIAL however there are some special cases where this can be done (if n=2(complex numbers), n=3(cross product), n=4(quaternions), n=8( Cayley numbers)). It is often convenient to use a single letter to denote an n-tuple of numbers. In this book bold-face, Roman letters are used to denote vectors, that is x =(x1;::: ;xn): For the moment we also use a \u0001 x to denote scalar multiplication. The compatibility of vector addition and scalar multiplication is then written as a \u0001 (x + y)= a \u0001 x + a \u0001 y: There is a special vector all of whose entries are zero denoted by 0 =(0;::: ; 0); it satis\fes x + 0 = x = 0 + x for any vector x: It is also useful to single out a collection of n coordinate vectors.Let ej 2Rn denote the vector with all entries zero but for the jth-entry which equals one. For example if n = 3 then the coordinate vectors are e1 =(1; 0; 0); e2 =(0; 1; 0); e3 =(0; 0; 1): These are called coordinate vectors because we can express any vector as a sum of these vectors, if x 2Rn then x = x1 \u0001 e1 + \u0001\u0001\u0001 + xn \u0001 en = nX j=1 xj \u0001 ej: (A.11) The n-tuple of numbers (x1;::: ;xn) are then the coordinates for the vector x: The set of vectors fe1;::: ; eng is also called the standard basis forRn : As before, the linear structure singles out a special collection of functions f :Rn !R; De\fnition A.2.2. A function f :Rn !Ris linear if it satis\fes the following conditions: for any pair of vectors x; y 2Rn and a 2R f (x + y)= f (x)+ f (y); f (a \u0001 x)= af (x): (A.12) In light of (A.11) it is clear that a linear function onRn is completely determined by the n values ff (e1);::: ;f (en)g: For an arbitrary x 2Rn (A.11) and (A.12) imply f (x)= nX j=1 xjf (ej): On the other hand it is easy to see that given n-numbers fa1;::: ;ang we can de\fne a linear function onRn by setting f (x)= nX j=1 ajxj: A.2. VECTOR SPACES 527 As in the one-dimensional case we therefore have an explicit knowledge of the collection of linear functions. What measurements are required to determine a linear function? While it su\u000eces, it is not actually necessary to measure ff (e1);::: ;f (en)g: To describes what is needed, requires a de\fnition. De\fnition A.2.3. If fv1;::: ; vng is a collection of n vectors inRn with the property that every vector x can be represented as x = nX j=1 aj \u0001 vj; (A.13) for a collection of scalars fa1;::: ;ang then we say that these vectors are a basis forRn : The coe\u000ecients are called the coordinates of x with respect to this basis. Note that the standard bases de\fned above satisfy (A.13). Example A.2.1. The standard basis forR2 is e1 =(1; 0); e2 =(0; 1): The vectors v1 = (1; 1); v2 =(0; 1) also de\fne a basis forR2 : To see this we observe that e1 = v1 − v2 and e2 = v2; therefore if x = x1 \u0001 e1 + x2 \u0001 e2 then x = x1 \u0001 (v1 − v2)+ x2 \u0001 v2 = x1 \u0001 v1 +(x2 − x1) \u0001 v2: Proposition A.2.1. Acollection of n vectors inRn fv1;::: ; vng de\fnes a basis if and only if the only n-tuple for which nX j=1 aj \u0001 vj =0 is the zero vector. This implies that the scalars appearing in (A.13) are uniquely determined by x: From the Proposition it is clear that the values, ff (v1);::: ;f (vn)g; for any basis fv1;::: ; vng; su\u000ece to determine a linear function, f: On the other hand, given numbers fa1;::: ;ang we can de\fne a linear function f by setting f (vj)= aj for 1 \u0014 j \u0014 n (A.14) and extending by linearity. This means that if x = X j=1 bj \u0001 vj then f (x)= nX j=1 bjaj: (A.15) 528 APPENDIX A. BACKGROUND MATERIAL From the standpoint of measurement, how are vectors inRn distinguished from one another? Linear functions provide an answer to this question. Let fv1;::: ; vng be a basis and for each 1 \u0014 j \u0014 n we de\fne the linear function fj by the conditions fj(vj)=1;fj(vi)=1 for i 6= j: Suppose that we can build a machine whose output is fj(x): Two vectors x and y are equal if and only if fj(x)= fj(y)for 1 \u0014 j \u0014 n: Linear functions are very useful in higher dimensions and play the same role in multi-variate calculus as they play in the single variable case. Exercise A.2.1. Prove Proposition A.2.1. Exercise A.2.2. Show that the function de\fned in (A.14) and (A.15) is well de\fned and linear. Exercise A.2.3. Let f :Rn !Rbe a non-zero linear function. Show that there is a basis fv1;::: ; vng forRn such that f (v1)=1 and f (vj)= 0 for 2 \u0014 j \u0014 n: A.2.2 General vector spaces As is often the case in mathematics it is useful to introduce an abstract concept which encompasses many special cases. De\fnition A.2.4. Let V be a set, it is a real vector space if it has two operations: Addition: Addition is a map from V \u0002 V ! V: If (v1; v2)is an element of V \u0002 V then we denote this by (v1; v2) 7! v1 + v2: Scalar multiplication: Scalar multiplication is a map fromR\u0002 V ! V: If a 2Rand v 2 V then we denote this by (a; v) 7! a \u0001 v: The operations have the following properties: Commutative law: v1 + v2 = v2 + v1; Associative law: (v1 + v2)+ v3 = v1 +(v2 + v3); Distributive law: a \u0001 (v1 + v2)= a \u0001 v1 + a \u0001 v2: Finally there is a special element 0 2 V such that v + 0 = v = 0 + v and 0 =0 \u0001 v; this vector is called the zero vector. A.2. VECTOR SPACES 529 Example A.2.2. For each n 2Nthe spaceRn with the addition and scalar multiplication de\fned above is a vector space. Example A.2.3. The set real valued functions de\fned onRis a vector space. We de\fne addition by the rule (f + g)(x)= f (x)+ g(x); scalar multiplication is de\fned by (a \u0001 f )(x)= af (x): We denote the space of functions onRwith these operations by F: Example A.2.4. If f1 and f2 are linear functions onRn then de\fne f1 + f2 as above: (f1 + f2)(x)= f1(x)+ f2(x) for all x 2Rn and (a \u0001 f )(x)= af (x): A sum of linear functions is a linear function, as is a scalar multiple. Thus the set of linear functions onRn is also a vector space. This vector space is called the dual vector space, it is denoted by (Rn )0: Example A.2.5. For each n 2N[f0g let Pn denote the set of polynomial functions onRof degree at most n: Since the sum of two polynomials of degree at most n is again a polynomial of degree at most n; as is a scalar multiple, it follows that Pn is a vector space. Many natural mathematical objects have a vector space structure. Often a vector space is subset of a larger vector space. De\fnition A.2.5. Let V be a vector space, a subset U ˆ V is a subspace if whenever u1; u2 2 U then u1 + u2 2 U and for every a 2R;a \u0001 u1 2 U as well. Brieﬂy, a subset U is a subspace if it is a vector space with the addition and scalar multiplication it inherits from V: Example A.2.6. The subset ofR2 consisting of the vectors f(x; 0) j x 2Rg is a subspace. Example A.2.7. Let f :Rn !Rbe a linear function, the set of vectors fv 2Rn j f (v)=0g is a subspace. This subspace is called the null-space of the linear function f: Example A.2.8. The set of vectors fv 2Rn j f (v)=1g in not a subspace. If g :(x; y) !Ris de\fned by g(x; y)= x2 − y then the set of vectors f(x; y) 2R2 j g(x; y)= 0g is not a subspace. Example A.2.9. The set of polynomials of degree at most 2 is a subspace of the set of polynomials ofdegreeatmost3: De\fnition A.2.6. Let fv1;::: ; vmg be a collection of vectors in a vector space V: A vector of the form v = a1 \u0001 v1 + \u0001\u0001\u0001 + am \u0001 vm is called a linear combination of the vectors fv1;::: ; vmg: The linear span of these vectors is the set of all linear combinations span(v1;::: ; vm) d = fa1 \u0001 v1 + \u0001\u0001\u0001 + am \u0001 vm j a1;::: am 2Rg: Example A.2.10. The linear span of a collection of vectors fv1;::: ; vmgˆ V is a subspace of V: A basic feature of a vector space is its dimension. This is a precise mathematical formulation of the number of degrees of freedom. The vector spaceRn has dimension n: The general concept of a basis is needed to de\fne the dimension. 530 APPENDIX A. BACKGROUND MATERIAL De\fnition A.2.7. Let V be a vector space, a set of vectors fv1;::: ; vngˆ V is said to be linearly independent if nX j=1 aj \u0001 vj =0 implies that aj =0 for j =1;::: ;n: This is another way of saying that it is not possible to write one of these vectors as a linear combination of the others. A \fnite set of vectors fv1;::: ; vngˆ V is a basis for V if (1). The vectors are linearly independent, (2). Every vector in V is a linear combination of these vectors, that is span(v1;::: ; vn)= V: The de\fnition of a basis given earlier for the vector spacesRn is a special case of this de\fnition. If a vector space V has a basis then every basis for V has the same number of elements. This fact allows makes it possible to de\fne the dimension of a vector space. De\fnition A.2.8. If a vector space V has a basis consisting of n vectors then the dimension of V is n: We write dim V = n: If fv1;::: ; vng is a basis for V then for every vector v 2 V there is a unique point (x1;::: ;xn) 2Rn such that v = x1 \u0001 v1 + \u0001\u0001\u0001 + xn \u0001 vn: (A.16) A vector space V of dimension n has exactly the same number of degrees of freedom asRn : In fact, by choosing a basis we de\fne an isomorphism between V andRn : This is because if v $ (x1;::: ;x2)and v0 $ (y1;::: ;yn) in (A.16) then v + v0 =(x1 + y1) \u0001 v1 + \u0001\u0001\u0001 +(xn + yn) \u0001 vn and for a 2Ra \u0001 v =(ax1) \u0001 v1 + \u0001\u0001\u0001 +(axn) \u0001 vn: From this point of view, all vector spaces of dimension n are \\the same.\" The abstract concept is still useful. Vector spaces often do not come with a natural choice of basis. Indeed the possibility of changing the basis, that is changing the identi\fcation of V withRn is a very powerful tool. In applications one tries to choose a basis that is well adapted to the problem at hand. It is important to note that many properties of vector spaces are independent of the choice of basis. Example A.2.11. The vector space F of all functions onRdoes not have a basis, that is we cannot \fnd a \fnite collection of functions such that any function is a linear combination of these functions. The vector space F is in\fnite dimensional. The study of in\fnite dimen- sional vector spaces is called functional analysis, we return to this subject in section A.3. Example A.2.12. For each n the set f1;x;::: ;xng is a basis for the Pn: Thus the dim Pn = n +1: A.2. VECTOR SPACES 531 Exercise A.2.4. Show that F; de\fned in example A.2.3 is a vector space. Exercise A.2.5. Show that the set of polynomials fxj(1 − x)n−j j 0 \u0014 j \u0014 ng is a basis for Pn: Exercise A.2.6. Show that if a vector space V has a basis then any basis for V has the same number of vectors. Exercise A.2.7. Let V be a vector space with dim V = n and let V 0 denote the set of linear functions on V: Show that V 0 is also a vector space with dim V 0 = n: Exercise A.2.8. Exercises on bases, dimensions, and vector spaces in general. A.2.3 Linear Transformations and matrices The fact that bothRn andRm have linear structures allows us to single out a special class of maps between these spaces. De\fnition A.2.9. Amap F :Rn !Rm is called a linear transformation if for all pairs x; y 2Rn and a 2Rwe have F (x + y)= F (x)+ F (y); F (a \u0001 x)= a \u0001 F (x): (A.17) Comparing the de\fnitions we see that a linear function is just the m =1case of a linear transformation. For each n 2Nthere is a special linear transformation ofRn to itself, called the identity map. It is de\fned by x 7! x and denoted by Idn : If fv1;::: ; vng is a basis forRn then a linear transformation is determined by the values fF (v1);::: ;F (vn)g: If x = aj \u0001 v1 + \u0001\u0001\u0001 + an \u0001 vn then (A.17) implies that F (x)= nX j=1 aj \u0001 F (vj): In this section, linear transformations are denoted by bold, upper case, Roman letters, e.g. A; B: The action of a linear Connected to a linear transformation A :Rn !Rm are two natural subspaces. De\fnition A.2.10. The set of vectors fx 2Rn j Ax =0g is called the kernel or null space of the linear transformation A; we denote this subspace by ker A: De\fnition A.2.11. The set of vectors fAx 2Rm j x 2Rn g is called the image of the linear transformation A;wedenotethisIm A: The kernel and image of a linear transformation are basic examples of subspaces of a vector space which are de\fned without reference to a basis. There is, in general no natural choice of a basis for either subspace. As above let fv1;::: ; vng be a basis forRn ; if we also choose a basis fu1;::: ; umg forRm then there is a collection of mn-numbers faijg so that for each j A(vj)= mX i=1 aijui: 532 APPENDIX A. BACKGROUND MATERIAL Such a collection of numbers, labeled with two indices is called a matrix.Once bases for the domain and range of A are \fxed, the matrix determines and is determined by the linear transformation. If A :Rn !Rn then one usually selects a single basis fvjg and uses it to represent vectors in both the domain and range of A: Often times it is implicitly understood that the bases are the standard bases. Example A.2.13. If fv1;::: ; vng is a basis forRn then Idn(vj)= vj: The matrix for Idn; with respect to any basis is denoted by \u000eij = ( 1if i = j; 0\f i 6= j: Once a pair of bases is \fxed then one can identify the set of linear transformations fromRn toRm with the collection of m \u0002 n-arrays (read m by n) of numbers. If we think of (aij) as a rectangular array of numbers then the \frst index, i labels the rows and the second index, j labels the columns. 0 B @ a11 ::: a1n ... ... am1 ::: amn 1 C A (A.18) A vector inRn can be thought of as either a row vector, that is an 1 \u0002 n-matrix or a column vector, that is an n \u0002 1-matrix. An m \u0002 n-matrix has n columns consisting of m \u0002 1 vectors, a =(a1 ::: an) or m rows consisting of 1 \u0002 n vectors a = 0 B @ a1 ... am 1 C A : Precisely how one wishes to think about a matrix depends on the situation at hand. We can de\fne a notion of multiplication between column vectors and matrices. De\fnition A.2.12. Let a be an m \u0002 n matrix with entries aij; 1 \u0014 i \u0014 m; 1 \u0014 j \u0014 n and x be an n-vector with entries xj; 1 \u0014 j \u0014 n then we de\fne the product a \u0001 x to be the m-vector y with entries yi = nX j=1 aijxj;i =1;::: ;m: Concisely this is written y = a \u0001 x: In this section we use lower case, bold Roman letters to denote matrices, e.g. a; b: Proposition A.2.2. Let a be an m \u0002 n-matrix, x1; x2n-vectors and a 2Rthen a \u0001 (x1 + x2)= a \u0001 x1 + a \u0001 x2 and a \u0001 (a \u0001 x)= a \u0001 (a \u0001 x): A.2. VECTOR SPACES 533 These conditions show that the map x 7! a \u0001 x is a linear transformation ofRn toRm : We can also de\fne multiplication between matrices with compatible dimensions. Let a be an m \u0002 n matrix and b be an l \u0002 m matrix. If x is an n-vector then a \u0001 x is an m-vector so b \u0001 (a \u0001 x) is de\fned. As the composition is a linear transformation fromRn toRl ; this de\fnes c = b \u0001 a as an l \u0002 n-matrix. If (aij) are the entries of a and (bpq); the entries of b then the entries of the product c are given by cpj = mX i=1 bpiaij: This shows that we can multiply an m \u0002 n matrix by an l \u0002 m matrix and the result is an l \u0002 n matrix. If a and b are both n \u0002 n-matrices then both products a \u0001 b; b \u0001 a are de\fned. In general they are not equal. One says that matrix multiplication is non-commutative. The product of an m \u0002 n-matrix and an n-vector is the special case of multiplying an n \u0002 1 matrix by a m \u0002 n-matrix, as expected the result if an m \u0002 1-matrix or an m-column vector. Suppose that fv1;::: ; vng and fu1;::: ; ung are both bases forRn : The de\fnition of a basis implies that there are n \u0002 n-matrices a =(aij)and b =(bij)so that vi = nX j=1 aji \u0001 uj and ui = nX j=1 bji \u0001 vj: These are called change of basis matrices. If x 2Rn then there are vectors (a1;::: ;an)and (b1;::: ;bn)so that x = nX j=1 aj \u0001 vj and also x = nX j=1 bj \u0001 uj: Substituting our expression for the fvj g in terms of the fujg gives x = nX j=1 aj \u0001 \" nX k=1 akj \u0001 uk # nX k=1 2 4 nX j=1 akjaj 3 5 \u0001 uk: (A.19) Comparing (A.19) with our earlier formula we see that bk = nX j=1 akjaj for k =1;::: ;n: This explains a is called the change of basis matrix. Suppose that A :Rn !Rm is a linear transformation and we select bases fv1;::: ; vng and fu1;::: ; umg forRn andRm respectively. Let (aij) denote the matrix of this linear transformation with respect to this choice of bases. How does the matrix change if these bases are replaced by a di\u000berent pair of bases? We consider what it means for \\(aij )tobe the matrix representing A with respect to the bases fvjg and fuig\" by putting into words 534 APPENDIX A. BACKGROUND MATERIAL the computations performed above: Suppose that x is a vector inRn with coordinates (x1;::: ;xn) with respect to the basis fvjg; then the coordinates of y = Ax with respect to fuig are yi = nX j=1 aijxj;i =1;::: ;m: The fact to keep in mind is that we are dealing with di\u000berent representations of \fxed (abstract) vectors x and Ax: Suppose that fv0 jg and fu0 ig are new bases forRn andRm respectively and let (blj)and (cki) be change of basis matrices, that is v0 j = nX l=1 blj \u0001 vl and ui = mX k=1 cki \u0001 u 0 k: Let a0 ij be the matrix of A with respect to fv0 jg and fu0 ig: If (x0 1;::: ;x0 n) are the coordinates of x with respect to fv0 jg and (y0 1;::: ;y0 m) the coordinates of Ax with respect to fu0 ig then y0 i = nX j=1 a 0 ijx0 j: Formula (A.19) tells us that xj = nX l=1 bjlx0 l and therefore yi = nX j=1 aij \" nX l=1 bjlx0 l # = nX l=1 2 4 nX j=1 aijbjl 3 5 x0 l (A.20) gives the expression for Ax with respect to the fuig: To complete our computation we only need to re-express Ax with respect to the basis fu0 ig: To that end we apply (A.19) one more time to obtain that y0 i = mX k=1 cikyk: Putting this into (A.20) and reordering the sums we obtain that y0 i = nX j=1 \" mX k=1 nX l=1 cikaklblj # x0 j: This shows that a 0 ij = mX k=1 nX l=1 cikaklblj: A.2. VECTOR SPACES 535 Using a; a0; b; c to denote the matrices de\fned above and x; x0; y; y0; the column vectors of coordinates, we can rewrite these expressions more concisely as x = b \u0001 x 0; y0 = c \u0001 y; a0 = c \u0001 a \u0001 b: (A.21) The reader should be aware that this formula di\u000bers slightly from that usually given in textbooks, this is because b changes from x0 to x whereas c changes from y to y0: trans- formation A on a vector x is often denoted by Ax: Exercise A.2.9. Show that that if A :Rn !Rm and B :Rm !Rl are linear transfor- mations then the composition B \u000e A(x) d = B(A(x)) is a linear transformation fromRn toRl : Exercise A.2.10. Let A :Rn !Rm be a linear transformation, show that ker A is a subspace ofRn : Exercise A.2.11. Let A :Rn !Rm be a linear transformation, show that Im A is a subspace ofRm : Exercise A.2.12. Suppose that we use a basis fv1;::: ; vng for the domain and fu1;::: ; ung for the range, what is the matrix for Idn? Exercise A.2.13. Prove Proposition A.2.2. Exercise A.2.14. If a = \u001200 10 \u0013 and b = \u001201 00 \u0013 then show that a \u0001 b 6= b \u0001 a: Exercise A.2.15. Show that if a is the matrix of a linear transformation A :Rn !Rm and b is the matrix of a linear transformation B :Rm !Rl then b \u0001 a is the matrix of their composition B \u000e A :Rn !Rl : Exercise A.2.16. Show that @x : Pn !Pn is a linear transformation. It is de\fned without reference to a basis. Find the basis for @x in terms of the basis f1;x;::: ;xng: Find bases for ker @x and Im @x: Exercise A.2.17. Show that the space of linear transformations fromRn toRm is a vector space with addition de\fned by (A + B)x d = Ax + Bx for all x 2Rn and scalar multiplication de\fned by (a \u0001 A)(x) d = a \u0001 (Ax): Let fv1;::: ; vng and fu1;::: ; umg be bases forRn andRm respectively. For 1 \u0014 i \u0014 m and 1 \u0014 j \u0014 n de\fne the linear transformations lij by letting lij(vj)= ui and lij(vk)=0 if k 6= j: Show that the flij j 1 \u0014 i \u0014 m; 1 \u0014 j \u0014 ng are a basis for this vector space. This shows that the space of linear transformations fromRn toRm is isomorphic toRmn : 536 APPENDIX A. BACKGROUND MATERIAL A.2.4 Norms and Metrics In the previous section we concentrated on algebraic properties of vector spaces. In appli- cations of linear algebra to physical problems it is also important to have a way to measure distances. In part, this is because measurements and computations are inaccurate and one needs a way to quantify the errors. Measurement of distance in a vector space usually begins with a notion of length. The distance between two vectors x and y is then de\fned as the length of x − y; again taking advantage of the underlying linear structure. There are many reasonable ways to measure length in a vector space. Let us consider thecaseofRn : The usual way to measure length is to de\fne the length of (x1;::: ;xn)= v u u t A.2. VECTOR SPACES 537 Using a norm k\u0001 k we can de\fne a notion of distance between two vectors by setting d(x; y)= kx − yk: For any choice of norm this function has the following properties Non-degeneracy: d(x; y) \u0015 0 with equality if and only if x = y: Symmetry: d(x; y)= d(y; x): Triangle inequality: For any 3 points x; y; z we have that d(x; z) \u0014 d(x; y)+ d(y; z): Any function d :Rn \u0002Rn !Rwith these properties is called a metric. There are metrics onRn which are not de\fne by norms. A metric gives a way to measure distances and therefore a way to de\fne the convergence of sequences. De\fnition A.2.14. Suppose that d(\u0001; \u0001) is a metric de\fned by a norm and that < xj >ˆRn is a sequence of vectors. The sequence converges to x in the d-sense if lim j!1 d(xj; x)= 0: Proposition A.2.3. Suppose that k\u0001 k and k\u0001 k0 are two norms onRn with associated metrics d and d0 then there is a positive constant C so that C −1kxk 0 \u0014kxk\u0014 Ckxk 0 for all x 2Rn : The proof is left as an exercise. Given a notion of distance it is also possible to de\fne Cauchy sequences. De\fnition A.2.15. Suppose that d is a metric onRn and < xn > is a sequence. It is a Cauchy sequence with respect to d if, for any \u000f> 0; there exists an N so that d(xn; xm) <\u000f provided that m and n>N: The importance of this concept is contained in the following theorem. Theorem A.2.1. Let d be a metric onRn : A sequence < xn > converges in the d-sense if and only if it is a Cauchy sequence. The choice of which norm to use in a practical problem is often dictated by physical considerations. For example if we have a system whose state is described by a point inRn and we allow the same uncertainty in each of our measurements then it would be reasonable to use the sup-norm ,i.e. k\u0001 k1: If on the other hand we can only tolerate a certain \fxed 538 APPENDIX A. BACKGROUND MATERIAL aggregate error, but it is not important how this error is distributed among the various measurements, then it would be reasonable to use k\u0001k1 to de\fne the norm. If the errors are expected to follow a Gaussian distribution then one would usually use the Euclidean norm. There are also computational considerations which can dictate the choice of a norm. If a is an m \u0002 n matrix with m> n then the system of linear equations ax = y is over- determined. For most choices of y it has no solution. A way to handle such equations is to look for a vector such that the \\size\" of the error ax − y is minimized. To do this, one needs to choose a norm onRm to measure the size of the error. It turns out that among all possible choices the Euclidean norm leads to the simplest minimization problems. The vector \u0016x such that ka\u0016x − yk2 is minimal is called the least squares solution. In exercise A.2.17 it is shown that the space of linear transformations fromRn toRm is a vector space. When discussing numerical methods for solving linear equations it is very useful to have a way to measure the size of linear transformation which is connected to its geometric properties as a map. We can use norms on the domain and range to de\fne a notion of size for a linear transformation A :Rn !Rm : Let k\u0001k be a norm onRn and k\u0001k0 be a norm onRm : The operator norm of A is de\fned by setting jjjAjjj =max x2Rnnf0g kAxk0 A.2. VECTOR SPACES 539 Exercise A.2.18. Suppose that x; y; z 2Rn and that d(x; y)= kx − yk2: If d(x; z)= d(x; y)+ d(y; z): then show that the three points lie along a line in the indicated order. Is this true if we use k\u0001 kp with p 6= 2 to de\fne the metric? Exercise A.2.19. Prove Proposition A.2.3. Hint: use the fact that kaxk = jajkxk: Exercise A.2.20. Let k\u0001k and k\u0001k0 be two norms onRn and d; d0 the corresponding metrics. Show that a sequence < xj > converges to x in the d-sense if and only if it converges in the d0-sense. This shows that the notion of limits on Euclidean spaces is independent of the choice of norm. Hint: Use Proposition A.2.3. Exercise A.2.21. Let k\u0001k and k\u0001k0 be two norms onRn and d; d0 the corresponding metrics. Show that a sequence < xj > converges to x in the d-sense if and only if it converges in the d0-sense. This shows that the notion of limits on Euclidean spaces is independent of the choice of norm. Hint: Use Proposition A.2.3. Exercise A.2.22. Suppose that w1;w2 are positive numbers show that lw((x1;x2)) = q 540 APPENDIX A. BACKGROUND MATERIAL A.2.5 Inner product structure The notions of distance considered in the previous section do not allow for the measurement of angles between vectors. Recall the formula for the dot product inR2 x \u0001 y = x1y1 + x2y2 = kxk2kyk2 cos \u0012; where \u0012 is the angle between x and y: We can generalize the notion of the dot-product to n-dimensions by setting x \u0001 y = nX j=1 xjyj: This is sometimes denoted by hx; yi; it is also called an inner product. Proposition A.2.4. If x; y; z are vectors inRn and a 2Rthen hx; yi = hy; xi; h(x + y); zi =hx; zi + hy; zi and hax; yi =ahx; yi = hx;ayi: (A.28) The inner product is connected with the Euclidean norm by the relation hx; xi = kxk 2 2: Most of the special properties of the Euclidean norm stem from this fact. There is a very important estimate which also connects these two objects called the Cauchy-Schwarz inequality jhx; yij \u0014 kxk2kyk2: (A.29) It is proved in exercise A.2.31. It implies that −1 \u0014 hx; yi A.2. VECTOR SPACES 541 Expressing the basis vectors and x in terms of the standard basis, x =(x1;::: ;xn)and vj =(v1j;::: ;vnj) this can be re-expressed as a system of linear equations, nX j=1 vijaj = xi for i =1;::: ;n: In general this system can be quite di\u000ecult to solve, however there is a special case when it is very easy to write down a formula for the solution. Suppose that the basis vectors are of Euclidean length one and pairwise orthogonal, that is kvjk2 =1 for j =1;::: ;n and hvi; vji =0 if i 6= j: Such a basis is called an orthonormal basis. The standard basis is an orthonormal basis. In this case aj = hx; vji; that is, the coordinates of x with respect to fvjg can be computing by simply evaluating these inner products, hence x = nX j=1hx; vjivj: (A.31) A consequence of (A.31) is that, in any orthonormal basis fvjg; the Pythagorean theorem holds kxk 2 2 = nX j=1 jhx; vj ij 2 (A.32) An immediate consequence of (A.32) is that the individual coordinates of a vector, with respect to an orthonormal basis are bounded by the Euclidean length of the vector. Orthonormal bases are often preferred in applications because they display stability properties not shared by arbitrary bases. Example A.2.15. If \u000f 6= 0 then the vectors v =(1; 0) and u\u000f =(1;\u000f)are a basis forR2 : If \u000f is small then the angle between these vectors is very close to zero. The representation of (0; 1) with respect to fv; u\u000fg is (0; 1) = 1 542 APPENDIX A. BACKGROUND MATERIAL Step 1 Replace u1 with the vector v1 = u1 A.2. VECTOR SPACES 543 Proposition A.2.6. If A :Rn !Rm has matrix (aij) with respect to the standard bases then At has matrix (aji) with respect to the standard bases, (Aty)i = mX j=1 ajiyj: The linear transformation from At :Rm !Rn is called the transpose (or adjoint )of A: Note that while the matrices representing A and its transpose are simply related, the transpose is de\fned without reference to a particular basis, for by de\fnition hAx; yim = hx; Atyin for all x 2Rn and y 2Rm : In order to avoid confusion, we have used h\u0001; \u0001in (resp. h\u0001; \u0001im) to denote the inner product onRn (resp.Rm ). We close this section by placing these considerations in a slightly more abstract frame- work. De\fnition A.2.16. Let V be a vector space, a function b : V \u0002 V !Rwhich satis\fes the conditions b(v; v) \u0015 0with b(v; v) = 0 if and only if v = 0 and for all v; w; z 2 V and a 2R b(v; w)= b(w; v); b(v + w; z)=b(v; z)+ b(w; z)and b(av; w)=ab(v; w)= b(v;aw) (A.34) de\fnes an inner product on V: A function with the properties in (A.34) is called a bilinear function. Example A.2.16. Let A :Rn !Rn be a linear transformation with ker A = f0g then hx; yiA = hAx; Ayi de\fnes an inner product onRn : Example A.2.17. Let Pn be the real valued polynomials of degree at most n then bn(p; q)= 1Z −1 p(x)q(x)dx de\fnes an inner product on Pn: Exercise A.2.26. Let b be an inner product on a vector space V: Using the same idea as used above to prove the Cauchy-Schwarz inequality, show that jb(v1; v2)j\u0014 p 544 APPENDIX A. BACKGROUND MATERIAL Exercise A.2.28. Apply the Gram-Schmidt process to the basis f1;x;x2g with the inner product given in example A.2.17 to \fnd an orthonormal basis for P2: Exercise A.2.29. Prove Proposition A.2.4. Exercise A.2.30. If a is an m \u0002 n-matrix and x 2Rn then we can use the inner product to express the matrix product ax: Show that if we write a in terms of its rows a = 0 B @ a1 ... am 1 C A then ax = 0 B @ ha1; xi ... ham; xi 1 C A : (A.35) Exercise A.2.31. Calculus can be used to proved (A.29). Let x and y be vectors inRn and de\fne the function f (t)= hx + ty; x + tyi = kx + tyk 2 2: This function satis\fes f (t) \u0015 0 for all t 2R: Use calculus to locate the value of t where f assumes it minimum. By evaluating f at its minimum and using the fact that f (t) \u0015 0 show that (A.29) holds. Exercise A.2.32. Prove formula (A.31). Exercise A.2.33. Prove Proposition A.2.5. Exercise A.2.34. Prove Proposition A.2.6. Exercise A.2.35. Suppose that A :Rn !Rm and B :Rm !Rl show that (B \u000e A) t = At \u000e B t: Express this relation in terms of the matrices for these transformations with respect to the standard bases. Exercise A.2.36. Show that hx; yiA is an inner product. Why do we need to assume that ker A = f0g? Exercise A.2.37. Prove that bn de\fned in example A.2.17 is an inner product. A.2. VECTOR SPACES 545 A.2.6 Linear transformations and linear equations Linear transformations give a geometric way to think about linear equations. Suppose that A :Rn !Rm is a linear transformation. The kernel of A is nothing more that the set of solutions to the equation Ax = 0: This is sometimes called the homogeneous equation. The system of equations Ax = y has a solution if and only if y belongs to the image of A: Theorem 1.3.2 relates the dimen- sions of the kernel and image of a linear transformation A :Rn !Rm ; they satisfy the relation dim ker A +dim Im A = n: (A.36) If A :Rn !Rn and dim ker A =0; then formula (A.36) implies that dim Im A = n and therefore for every y 2Rn there is a unique x 2Rn such that Ax = y: A linear transformation with this property is called invertible, we let A−1 denote the inverse of A: It is also a linear transformation. A linear transformation and its inverse satisfy the relations A −1A =Idn = AA−1: If (aij)is the matrix of A with respect to a basis and (bij)is the matrix for A−1 then these relations imply that nX j=1 bijajk = \u000eik = nX j=1 aijbjk: (A.37) From a purely mathematical standpoint the problem of solving the linear equation Ax = y is simply a matter of computing A−1y: Cramer's rule gives an explicit formula for A−1; though it is very unusual to solve linear equations this way. The direct computation of A−1 is enormously expensive, computationally and also unstable. Less direct, computationally more stable and e\u000ecient methods are usually employed. De\fnition A.2.17. An n \u0002 n matrix (aij) is called upper triangular if aij =0 if j< i: A system of equations is upper triangular if its matrix of coe\u000ecients is upper triangular. Upper triangular systems are very easy to solve. Suppose that (aij) is an upper trian- gular matrix with all of its diagonal entries faiig non-zero. The system of equations ax = y becomes nX j=i aijxj = yi for i =1;::: ;n: It is easily solved using the back substitution algorithm: 546 APPENDIX A. BACKGROUND MATERIAL Step 1 Let xn = a−1 nn yn: Step 2 For a 1 <j <n assume we know (xj+1;::: ;xn)and let xj+1 = yj+1 − Pn k=j+1 a(j+1)kxk A.2. VECTOR SPACES 547 A.2.7 Linear algebra with uncertainties In applications of linear algebra a vector x often represents the (unknown) state of a system, amatrix a models a measurement process and y = ax are the (known) results of the measurements. The simple minded problem is then to solve this system of linear equations. In reality things are more involved. The model for the measurements is only an approximation and therefore it is perhaps more reasonable to think of the measurement matrix as a + \u000ea: Here \u000ea represents an aggregation of errors in the model. The measurements are themselves subject to error and therefore should also be considered to have the form y + \u000ey: A more realistic problem is therefore to solve the system of equations (a + \u000ea)x = y + \u000ey: (A.38) But what does this mean? We consider only the simplest case where a is an n \u0002 n; invertible matrix. Let k\u0001 k denote a norm onRn and jjj \u0001 jjj the operator norm de\fned as in (A.25). Suppose that we can bound the uncertainty in both the model and the measurements in the sense that we have constants \u000f> 0and \u0011> 0 such that k\u000eyk <\u000f and jjj\u000eajjj <\u0011: In the absence of more detailed information about the systematic errors, \\the solution\" to (A.38) should be de\fned as the set of vectors fx : j (a + \u000ea)x = y + \u000ey for some choice of \u000ea;\u000ey with k\u000eyk <\u000f; jjj\u000eajjj <\u0011g: This is a little cumbersome. In practice one \fnds a vector which satis\fes ax = y and a bound for the error one makes in asserting that the actual state of the system is x: To proceed with this analysis we assume that all the possible model matrices, a + \u000ea are invertible. If jjj\u000eajjj is su\u000eciently small then this condition is satis\fed. As a is invertible the number \u0016 =min x6=0 kaxk 548 APPENDIX A. BACKGROUND MATERIAL Because (\u0016 − jjj\u000eajjj)and v were assumed to be positive, the \frst and last lines are in contradiction. This shows that if a is an invertible matrix then so is a + \u000ea; for su\u000eciently small perturbations \u000ea: Note that the de\fnition of small depends on a: In the remainder of this discussion we assume that \u0011; the bound on the uncertainty in the model is smaller than \u0016: An estimate on the error in x is found in two steps. First, \fx the model and consider only errors in measurement. Suppose that ax = y and a(x + \u000ex)= y + \u000ey: Taking the di\u000berence of these two equations gives a\u000ex = \u000ey and therefore \u000ex = a−1\u000ey: Using (A.26) again we see that k\u000exk \u0014 jjja−1jjjk\u000eyk: This is a bound on the absolute error; it is more meaningful to bound the relative error k\u000exk=kxk: To that end observe that kyk \u0014 jjjajjjkxk and therefore k\u000exk A.2. VECTOR SPACES 549 Ignoring quadratic error terms this gives the estimate k\u000exk 550 APPENDIX A. BACKGROUND MATERIAL Computing f 0(0) we \fnd 0= f 0(0) = 2hav; (a\u0016x − y)i =2hv; (ata\u0016x − aty)i: Since this must hold for all vectors v 2Rn it follows that atax = aty: (A.46) This is called the system of normal equations.The matrix ata is n \u0002 n and under our assumptions it is invertible. If it were not invertible then there would be a vector x0 6=0 so that atax0 = 0: This would imply that 0= hatax0; x0i = hax0; ax0i: Hence ax0 = 0; but this means that the columns of a are not linearly independent. In terms of measurements, the state x0 6= 0 cannot be distinguished from 0: This shows that the normal equations have a unique solution for any set of measurements y: The matrix ata is a special type of matrix, called a positive de\fnite, symmetric matrix. This means that hatax; xi > 0if x 6= 0 and (ata) t = ata: There are many special algorithms for solving a system of equations with a positive de\fnite coe\u000ecient matrix, for example steepest descent or the conjugate gradient method, see [43] or [19]. These considerations explain, in part why the Euclidean norm is usually chosen to measure the error in an over determined linear system. A.2.9 Complex numbers and the Euclidean plane Thus far we have only considered real numbers and vector spaces over the real numbers. While the real numbers are complete in the sense that there are no holes, they are not complete from an algebraic standpoint. There is no real number which solves the algebraic equation x2 = −1: To remedy this we simply introduce a new symbol i which is de\fned by the condition that i 2 = −1: It is called the imaginary unit. De\fnition A.2.19. The complex numbers are de\fned to be the collection of symbols fx + iy j x; y 2Rg with the addition operation de\fned by (x1 + iy1)+(x2 + iy2) d =(x1 + x2)+ i(y1 + y2) and multiplication de\fned by (x1 + iy1)(x2 + iy2) d =(x1x2 − y1y2)+ i(x1y2 + x2y1): The set of complex numbers is denoted byC: A.2. VECTOR SPACES 551 Note that addition and multiplication are de\fned in terms of the addition and multipli- cation operations on real numbers. The complex number 0 = 0 + i0satis\fes0 + (x + iy)= x + iy and the complex number 1 = 1 + i0satis\fes(1 + i0)(x + iy)= x + iy: We often use the letters z or w to denote complex numbers. The sum is denoted by z + w and the product by zw: Proposition A.2.7. The addition and multiplication de\fned for complex numbers satisfy the commutative, associative and distributive laws. That is, if z1;z2;z3 are three complex numbers then z1 + z2 = z2 + z1 (z1 + z2)+ z3 = z1 +(z2 + z3); z1z2 = z2z1 (z1z2)z3 = z1(z2z3); z1(z2 + z3)= z1z2 + z1z3: (A.47) De\fnition A.2.20. If z = x + iy then the real part of z is x and the imaginary part of z is y: These functions are written symbolically as Re z = x; Im z = y: The set underlying the complex numbers isR2 ; x + iy $ (x; y); addition of complex numbers is the same as vector addition. Often the set of complex numbers is called the complex plane. The Euclidean norm is used to de\fne the absolute value of a complex number jx + iyj = p 552 APPENDIX A. BACKGROUND MATERIAL do not. If x and y are real numbers then we say that x< y if y − x> 0: This relation has many familiar properties: if x<y and s is another real number then x + s< y + s;if s> 0 then xs < ys as well. In other words, the order relation is compatible with the arithmetic operations. It is not di\u000ecult to show that no such compatible order relation exists on the complex numbers. It is useful to understand the multiplication of complex numbers geometrically. For this purpose we represent points in the complex plane using polar coordinates. The radial coordinate is simply r = jzj: The ratio ! = zjzj−1 is a number of unit length and therefore has a representation as ! =cos \u0012 + i sin \u0012 so that z = r(cos \u0012 + i sin \u0012) (A.48) The angle \u0012 is called the argument of z; which is denoted by arg(z): It is only determined up to multiples of 2ˇ: If z and w are two complex numbers then they can be expressed in polar form as z = r(cos \u0012 + i sin \u0012);w = ˆ(cos ˚ + i sin ˚): Computing their product we \fnd that zw =rˆ([cos \u0012 cos ˚ − sin \u0012 sin ˚]+ i(cos \u0012 sin ˚ +sin \u0012 cos ˚]) =rˆ(cos(\u0012 + ˚)+ i sin(\u0012 + ˚)): (A.49) In the second line we used the sum formulˆ for the sine and cosine. This shows us that complex multiplication of w by z can be understood geometrically as scaling the length of w by jzj and rotating it in the plane through an angle arg(z): r(cos θ, sin θ ) R(cos ϕ, sin ϕ) rR(cos(θ+ϕ), sin( θ+ϕ)) Figure A.1: Multiplication of complex numbers Using the notion of distance onCde\fned above we can de\fne the concept of convergence for sequences of complex numbers. A.2. VECTOR SPACES 553 De\fnition A.2.21. Let <zn > be a sequence of complex numbers. The sequence con- verges to z\u0003 if lim n!1 jzn − z\u0003j =0: In this case case z\u0003 is called the limit of <zn > and we write lim n!1 zn = z\u0003: Exercise A.2.43. Prove Proposition A.2.7. Exercise A.2.44. Let z and w be complex numbers show that 554 APPENDIX A. BACKGROUND MATERIAL Proposition A.2.8. If w 2Cand z 2Cn then kwzk = jwjkzk: Moreover, if z; w 2Cn then hz; wi = A.3. FUNCTIONS, THEORY AND PRACTICE 555 De\fnition A.3.1. A function f is a real computable function if its value can be determined for any x 2Rby doing a \fnite number of feasible operations. We refer to these functions as computable functions, for more on this concept see [85]. What are the \\feasible operations?\" Provisionally the feasible operations are those which only require the ability to do arithmetic and to determine if a number is non-negative. These are the operations which can be done approximately by a computer. One can give an analogous de\fnition for computable functions de\fned onRn or on subsets ofRn ;n \u0015 1: Rational functions are evidently computable functions, but there are other types of computable functions. If [a; b] ˆRis an interval,thatis [a; b]= fx 2Rj a \u0014 x \u0014 bg then we de\fne the characteristic function of an interval by the rule ˜[a;b](x)= ( 1if x 2 [a; b]; 0if x=2 [a; b]: Again if we know the exact value of x then to compute ˜[a;b](x) we only need to perform feasible operations to check if 0 \u0014 x − a and 0 \u0014 b − x: Proposition A.3.1. Suppose that f (x) and g(x) are two computable functions then f + g; f g; f − g; f =g and f \u000e g are also computable functions. The set of computable functions is, in essence the set of functions that are actually available for computational purposes. They are the functional analogue of ﬂoating point numbers. However it is very easy to de\fne functions, quite explicitly which do not fall into this class. The function f (x)= x3 is a computable function and it is one-to-one and onto. That is f (x1)= f (x2) implies that x1 = x2: For every y 2Rthere is an x (evidently unique) so that f (x)= y: This means there is a function g(y)which inverts f (x); that is g(f (x)) = x: Of course this is just the cube root function. Much less evident is how to compute g(y) for an arbitrary value of y: A function can also be de\fned implicitly via a functional relation. For example we think of y as a function of x de\fned by the computable relation x2 + y2 =1: Evaluating y as a function of x entails solving this equation, formally we can write y\u0006(x)= \u0006p 556 APPENDIX A. BACKGROUND MATERIAL Both of the functions x(t)and y(t) are computable and so we see that, at the expense of expressing both x and y in terms of an auxiliary variable, t we areableto solve x2 + y2 =1: For only slightly more complicated equations in two variables it is known that no such trick exists. So the problem of solving non-linear equations in one or several variables leads to non- computable functions. Probably the most important examples of non- computable functions are the solutions of linear, ordinary di\u000berential equations. For example the sin x; cos x; exp x all arise in this context as well as the Bessel functions, Legendre functions, etc. Such functions are called transcendental functions. For many purposes these functions are regarded as completely innocuous. They are however not computable, except for very special values of x: The reason that these functions are not greeted with horror is that they are all well approximated by computable functions in a precise sense: For each of these functions there are computable approximations and estimates for the di\u000berences between the actual functions and their approximations. As machine computation is always approximate, it is not necessary (or even possible) to evaluate functions exactly. It is only necessary to be able to evaluate functions to within a speci\fed error. Exercise A.3.1. Prove Proposition A.3.1. Exercise A.3.2. Give a de\fnition for computable functions of several variables. Show that linear functions are always computable functions. Show moreover that the solution x of a system of linear equations ax = y is a computable function of y: A.3.1 Inﬁnite series Most of the functions that one encounters can be represented as in\fnite sums. Elementary arithmetic de\fnes any \fnite sum of numbers but a further de\fnition is needed to de\fne an in\fnite sum. This is clear because not every in\fnite sum makes sense, for example what value should assigned to the sums, 1X j=1(−1) j or 1X j=1 1 A.3. FUNCTIONS, THEORY AND PRACTICE 557 If the sequence <sn > has a limit then the sum in (A.54) converges, otherwise the sum diverges. If the sum converges then by de\fnition 1X j=1 aj d = lim n!1 sn: In the \frst example above the partial sums are given by sn =(−1)n; so this series diverges; in the second example one can show that sn > log n and therefore this series also diverges. In the last case, the terms are decreasing, alternating in sign and converge to zero, so the alternating series test (B.4.7) applies to show that the sum converges. A sum can diverge in two di\u000berent ways: the partial sums can tend to \u00061 or simply fail to approach a \fnite limit. In the former case we sometimes write P1 j=1 aj = 1: Note that if the order of terms in the third sum is changed then the value of the sum can also be changed. Perhaps the most dramatic way to see this is to \frst add up the positive terms and then the negative terms. But observe that 1X j=1 (−1)j 558 APPENDIX A. BACKGROUND MATERIAL This distinction is important to understand because conditionally convergent series arise frequently in imaging applications. As might be expected, conditionally convergent series require more care to approximate than absolutely convergent sums. There are two very useful tests for convergence that can often be applied in practice. These tests are called the integral test and the alternating series test. The \frst applies to show that certain series are absolutely convergent while the second can be used to study special, conditionally convergent series. Proposition A.3.2 (The integral test). Suppose that f (x) is a function de\fned on [1; 1) which is non-negative and monotone decreasing, that is 0 \u0014 f (x) \u0014 f (y) if x>y: If an = f (n) then the series 1X n=1 an converges if and only if lim R!1 RZ 1 f (x)dx < 1: If the series converges then \f \f \f \f \f 1X n=1 an − NX n=1 an \f \f \f \f \f \u0014 1Z N f (x)dx: (A.56) The test which can be applied to series with non-positive terms is Proposition A.3.3 (Alternating series test). Let <an > be a sequence that satis\fes the three conditions lim n!1 an =0; anan+1 < 0 and jan+1j\u0014 janj for all n: (A.57) Then the series 1X n=1 an converges and \f \f \f \f \f 1X n=1 an − NX n=1 an \f \f \f \f \f \u0014jaN +1j: (A.58) These tests not only give criteria for certain in\fnite series to converge, but also give estimates for the errors made by replacing the in\fnite sum by sN for any value of N: Exercise A.3.3. Suppose that the series 1X j=1 aj converges. Show that the limj!1 aj =0: Note the converse statement is false: the P j−1 = 1 even though limj!1 j−1 =0: A.3. FUNCTIONS, THEORY AND PRACTICE 559 Exercise A.3.4. Show that aj = j−p converges if p> 1 and diverges if p \u0015 1: Exercise A.3.5. Prove the alternating series test. A.3.2 Partial summation One of the most useful tools for working with integrals is the integration by parts formula. If f and g are di\u000berentiable functions on an interval [a; b]then bZ a f 0(x)g(x)dx = f (b)g(b) − f (a)g(a) − bZ a f (x)g0(x)dx: There is an analogue of this formula which is very important in the study of non-absolutely convergent series. It is called the summation by parts formula. Proposition A.3.4 (Summation by Parts Formula). Let <an > and <bn > be sequences of numbers. For each n let Bn = nX k=1 bk then NX n=1 anbn = aN BN − N −1X n=1 (an+1 − an)Bn: (A.59) Using this formula it is often possible to replace a conditionally convergent sum by an absolutely convergent sum. Example A.3.1. Let \u000b = e2ˇix where x=2Z; so that \u000b 6=1: For any such \u000b; the series 1X n=1 \u000bn 560 APPENDIX A. BACKGROUND MATERIAL A.3.3 Power series A special sub-class of in\fnite series are called power series, it is the in\fnite series general- ization of a polynomial and involves the powers of a variable. De\fnition A.3.3. Let <aj > be a sequence of complex numbers. The power series with these coe\u000ecients is the in\fnite series 1X j=0 ajzj; (A.60) z is a complex number. As it stands a power series is a formal expression. The theory of convergence of power series is relatively simple. Roughly speaking, a power series converges for a complex argu- ment z provided that limj!1 jajzjj =0: The exact result is given in the following theorem. Theorem A.3.1. Suppose that r \u0015 0 and lim j!1 jajjrj = 0 (A.61) then the power series (A.60) converges absolutely for all complex numbers z with jzj <r: The supremum of the numbers which satisfy (A.61) is called the radius of convergence of the power series; we denote it by rconv: For values of z with jzj <rconv the power series converges absolutely, if jzj = rconv then the question of convergence or divergence of the series is again quite subtle. Example A.3.2. If aj = j−1 then rconv =1: For jzj < 1the series 1X j=1 zj A.3. FUNCTIONS, THEORY AND PRACTICE 561 and suppose that the radius of convergence is rconv > 0: Formally di\u000berentiating gives a new power series, f1(z)= 1X j=1 jajzj−1: It is not di\u000ecult to show that the radius of convergence of this series is also rconv andinfact f 0(z)= f1(z); see [1]. This can of course be repeated over and over. These observations are summarized in the following theorem. Theorem A.3.2. Suppose that the radius of convergence of the power series (A.62) is rconv > 0; the function, f (z) it de\fnes in Dconv is in\fnitely di\u000berentiable. For each k \u0015 0 f [k](z)= 1X j=k ajj(j − 1) ::: (j − k +1)zj−k also has radius of convergence rconv: Note in particular that f [k](0) = k!ak: Example A.3.4. The functions sin(z); cos(z); exp(z) are de\fned as the solutions of di\u000ber- ential equations. The sine and cosine satisfy f 00 + f =0 while the exponential solves f 0 − f =0: Assuming that these functions have power series expansions, we \fnd by substituting into the di\u000berential equations that sin(x)= − 1X j=0 (−x)2j+1 562 APPENDIX A. BACKGROUND MATERIAL The exponential function is given as an in\fnite series with positive coe\u000ecients and therefore x> 0 implies that exp(x) > 0: Since exp(x) exp(−x) = 1 this holds for any real number. Combining this observation with the fact that @x exp(x)=exp(x)shows that the exponential is strictly monotone increasing on the real line. Thus exp has an inverse function l(y); de\fned for positive real numbers y, which satis\fes exp(l(y)) = y and l(exp(x)) = 0: Note that l(1) = 0: This function is called the logarithm (or natural logarithm). Following standard practice we use the notation log(y)for l(y): As the derivative of exp is non- vanishing its inverse is also di\u000berentiable. Using the chain rule we obtain that log0(y)= 1 A.3. FUNCTIONS, THEORY AND PRACTICE 563564 APPENDIX A. BACKGROUND MATERIAL The coe\u000ecients are the binomial coe\u000ecients given by \u0012n j \u0013 = n! A.3. FUNCTIONS, THEORY AND PRACTICE 565 Note that the numbers which appear in these two sums are identical, only the signs are di\u000berent. The \frst sum is a very small positive number and the second a very large positive number. This means that there is a lot of rather subtle cancelation occurring in the \frst sum. Using ﬂoating point arithmetic it is very di\u000ecult to compute such a sum accurately. A much more accurate computation of exp(−x) is obtained by \frst computing an approx- imation y ' exp(x) and then setting exp(−x) ' y−1: We can compute the relative error, \frst suppose y = e x + \u000f then a calculation shows that 1 566 APPENDIX A. BACKGROUND MATERIAL We give an outline of the derivation of Stirling's formula. It is a special case of Laplace's method for obtaining asymptotics for functions of the form f (x)= Z e x˚(s) (s)ds: The idea is very simple: only the global maxima of the exponent ˚(s) contribute, asymptotically to f (x)as x tends to in\fnity. A fuller account of this method can be found in [58]. We begin by setting t = s(x − 1) in (A.75) to obtain, Γ(x)= (x − 1) x 1Z 0 e (x−1)(log s−s)ds: The function in the exponent log s − s has a unique maximum at s = 1 where it assumes the value −1: This implies that for any small \u000e> 0 we have the asymptotic formula 1Z 0 e (x−1)(log s−s)ds = 1+\u000eZ 1−\u000e e (x−1)(log s−s)ds + O(e −(x−1)(1+ δ2 A.3. FUNCTIONS, THEORY AND PRACTICE 567 Exercise A.3.11. Prove the functional equation (A.76). Deduce that Γ(z + n)= z(z +1) \u0001\u0001\u0001 (z + n − 1)Γ(z): Exercise A.3.12. Show that for a positive integer n; Γ(n +1) = n! Exercise A.3.13. For m a non-positive integer compute the limit lim z!m (z − m)Γ(z): Exercise A.3.14. Prove formula A.78. Exercise A.3.15. Prove that v is a smooth, invertible function of u for u in an interval about 0 and that if u = h(v)then h0(0) = 1: Exercise A.3.16. Fill in the details of the last step in the derivation of Stirling's formula. Exercise A.3.17. Provethatif x and y are positive real numbers then 1Z 0 tx−1(1 − t) y−1dt = Γ(x)Γ(y) 568 APPENDIX A. BACKGROUND MATERIAL –0.4 –0.2 0 0.2 0.4 0.6 0.8 1 10 20 30 40 50x (a) J0: –0.2 0 0.2 0.4 0.6 10 20 30 40 50x (b) J 1 A.4. SPACES OF FUNCTIONS 569 Exercise A.3.21. Derive the power series expansion, (A.83). from (A.85). Hint: Write the exponential as a power series and use (A.81). Exercise A.3.22. Bessel functions with half integral order can be expressed in terms of trigonometric functions. Show that (1). J 1 570 APPENDIX A. BACKGROUND MATERIAL A sequence of functions <fn > converges in this norm to a function f provided lim n!1 kfn − f kC0 =0: It is a non-trivial result in analysis that if <fn > converges to f; in this sense, then f is also a continuous function. This norm is sometimes called the uniform norm and convergence in this norm is called uniform convergence. The vector space C0([0; 1]) is complete with respect to this norm. For each k 2Nwe let Ck([0; 1]) denote the space of functions de\fned on [0; 1] with k continuous derivatives. We de\fne a norm on this space by setting kf kCk = kX j=0 kf [j]kC0: (A.87) This norm de\fnes a notion of convergence for k-times di\u000berentiable functions. As before, if a sequence <fn > converges to f in this sense then f is also a function with k continuous derivatives. The basic result in analysis used to study these function spaces is: Theorem A.4.1. Let <fn > be a sequence of k-times di\u000berentiable functions de\fned on [a; b]: If <fn > converges uniformly to f and the sequences of derivatives <f [j] n > ;j =1;::: ;k converge uniformly to functions g1;::: ;gk then f is k-times, continuously di\u000berentiable and f [j] = gj for j =1;::: ;k: A proof of this theorem is given in [67]. Let C1([0; 1]) denote the vector space of functions, de\fned on [0; 1] with in\fnitely many continuous derivatives. The expression on the right hand side of (A.87) makes no sense if k = 1: In fact there is no way to de\fne a norm on the vector space C1([0; 1]): We can however de\fne a metric on C1([0; 1]) by setting d(f; g)= 1X j=0 2 −j kf − gkCj A.4. SPACES OF FUNCTIONS 571 Such a function is said to be square integrable. It is not necessary for f to be continuous in order for it to belong to L2([0; 1]); only that this integral makes sense. The function jx − 1 572 APPENDIX A. BACKGROUND MATERIAL and therefore limn!1 kfn − 0k2 =0: So the sequence <fn > does converge in the L2-norm to the function which is identically zero. Note that the pointwise limit, f (x) cannot be distinguished from the zero function by the L2-norm. Note also the related fact: the L2- convergence of a sequence <fn > to a function f does not require that limn!1 fn(x)= f (x) for all x: Example A.4.3. De\fne a sequence of functions fn(x)= 8 >< >: 0for x 2 [0; n−1 A.4. SPACES OF FUNCTIONS 573 Theorem A.4.2 (H¨older's inequality). Let 1 \u0014 p \u00141 and de\fne q to be q = 8 >< >: p 574 APPENDIX A. BACKGROUND MATERIAL De\fnition A.4.1. Let (V; k\u0001 k) be a normed vector space. A sequence <vn >ˆ V is a Cauchy sequence if, for any \u000f> 0; there exists an N so that kvn − vmk <\u000f provided that m and n>N: Reasoning by analogy, the Cauchy sequences are the ones which \\should converge.\" However, because there are many di\u000berent norms which can be used on an in\fnite dimen- sional space, this is quite a subtle question. Example A.4.5. Let V be the continuous functions on [0; 1] and use for a norm kf k = 1Z 0 jf (x)jdx: De\fne a sequence <fn >ˆ V by setting fn(x)= 8 >< >: 0for 0 \u0014 x \u0014 1 A.4. SPACES OF FUNCTIONS 575 A.4.3 Linear functionals For \fnite dimensional vector spaces the concept of a linear function is given by the purely algebraic conditions (A.12). For in\fnite dimensional vector more care is required because linear functions may not be continuous. Example A.4.6. Let V be the set of once di\u000berentiable functions on [0; 1]: Instead of using the the usual C1-norm we use the C0-norm. With this choice of norm a sequence of functions <fn >ˆ V converges to f 2 V if lim n!1 kfn − f kC0 =0: Suppose <fn > is a sequence which converges to 0 in this sense and that l : V !Ris a linear function. If l is continuous then lim n!1 l(fn)=0: De\fne a function on V by setting l(f )= f 0( 1 576 APPENDIX A. BACKGROUND MATERIAL We denote the set of linear functionals by V 0; as before it is a vector space called the dual vector space. It has a naturally de\fned norm given by klk 0 =sup V 3f 6=0 jl(f )j A.4. SPACES OF FUNCTIONS 577 Example A.4.7. The space l1 consists of sequences which de\fne absolutely convergent sums, that is <aj >2 l1 if and only if 1X j=1 jajj < 1: If p< p0 then it is clear that lp ˆ lp0: There is also a version of the H¨older inequality. Let 1 \u0014 p \u00141 and q by given by (A.90), for <aj >2 lp and <bj >2 lq the sequence <ajbj >2 l1 and 1X j=1 jajbjj\u0014k <aj > kpk <bj > kq: (A.95) This inequality shows that if b =<bj >2 lq then we can de\fne a bounded linear functional on lp by setting lb(a)= 1X j=1 ajbj: This again gives all bounded functionals provided p is \fnite. Theorem A.4.5 (Riesz Representation Theorem 2). If 1 \u0014 p< 1 and q is given by (A.90) then lq is the dual space to lp: That is every continuous linear function on lp is given by lb for some b 2 lq: Exercise A.4.5. Prove that lp ˆ lp0: A.4.4 Measurement, linear functionals and weak convergence Suppose that the state of a system is described by a function f 2 Lp([0; 1]): In this case the measurements that one can make are often modeled as the evaluation of linear functionals. That is we have a collection of functions fg1;::: ;gkgˆ Lq([0; 1]) and our measurements are given by mj(f )= 1Z 0 f (x)gj(x)dx; j =1;::: ;k: From the point of view of measurement this suggests a di\u000berent, perhaps more reasonable, notion of convergence. In so far as these measurements are concerned, a sequence of states <fn > would appear to converge to a state f if lim n!1 1Z 0 fn(x)gj (x)dx = 1Z 0 f (x)gj(x)dx; for j =1;::: k: Since we are only considering \fnitely many measurements on an in\fnite dimensional state space this is clearly a much weaker condition than the condition that <fn > converge to f in the Lp-sense. 578 APPENDIX A. BACKGROUND MATERIAL Of course if <fn > converges to f in the Lp-sense then, for any g 2 Lq([0; 1]) limn!1 lg(fn)= lg(f ): However the Lp-convergence is not required for these conditions to hold. It is a very important observation that the condition lim n!1 1Z 0 fn(x)g(x)dx = 1Z 0 f (x)g(x)dx for every function g 2 Lq([0; 1]) is a much weaker condition than Lp-convergence. De\fnition A.4.5. Suppose that (V; k\u0001k) is a normed vector space and < vn > is a sequence of vectors in V: If there exists a vector v 2 V such that for every continuous linear function l we have that lim n!1 l(vn)= l(v) then we say that vn converges weakly to v: This is sometimes denoted by vn + v: From the point of view of measurement, weak convergence is often the appropriate notion. Unfortunately it cannot be de\fned by a norm and a sequence does not exert very much control over the properties of its weak limit. For example it is not in general true that lim n!1 kvnk = kvk for a weakly convergent sequence. This is replaced by the statement If vn + v then lim sup n!1 kvnk\u0015kvk: (A.96) Example A.4.8. The sequence of functions <fn > de\fned in example A.4.4 is a sequence with kfnkL2 =1 for all n: On the other hand if x 2 (0; 1] then lim n!1 fn(x)=0: These two facts allow the application of standard results from measure theory to conclude that lim n!1 1Z 0 fn(x)g(x)dx =0; for every function g 2 L2([0; 1]): In other words the sequence <fn > converges weakly to zero even though it does not converge to anything in the L2-sense. A.4. SPACES OF FUNCTIONS 579 Example A.4.9. Let < an >ˆ l2 be the sequence de\fned by an(j)= ( 1if j = n; 0if j 6= n: Since an(j)= 0 if j< n it is clear that if < an > were to converge to a; in the l2-sense then a = 0: On the other hand kankl2 =1 for all n and this shows that an cannot converge in the l2-sense. On the other hand if b 2 l2 then han; bil2 = b(n): Because kbkl2 < 1 it is clear that lim n!1 b(n)= 0 and therefore an converges weakly to 0: Exercise A.4.6. Suppose that <fn >ˆ L2([0; 1]) and <fn > has a weak limit, show that it is unique. A.4.5 The L 2-case Of particular note is the case p =2; for in this case (and only this case) p = q: That is L2([0; 1]) is its own dual vector space. This distinction is already familiar from the \fnite dimensional case. The space L2([0; 1]) has an inner product which de\fnes its metric. It is given by hf; giL2 = 1Z 0 f (x)g(x)dx: H¨older's inequality in this case is just the in\fnite dimensional analogue of the Cauchy- Schwarz inequality, jhf; giL2 j\u0014 kf kL2kgkL2 : (A.97) An inner product can be de\fned on every \fnite dimensional vector space. This is false in in\fnite dimensions. Among the Lp-spaces, L2 is the only space which has an inner product de\fning its norm As before (A.97) leads to a de\fnition of the angle, \u0012 between two vectors, f; g by de\fning cos \u0012 = hf; giL2 580 APPENDIX A. BACKGROUND MATERIAL The set fejg is an orthonormal basis if for every vector f 2 L2([0; 1]) there is a sequence of numbers <aj > so that lim N !1 kf − NX j=1 ajejkL2 =0: (A.99) In this case we write f = 1X j=1 ajej: (A.100) It follows from (A.98) and (A.99) that kf k 2 L2 = 1X j=1 jajj 2: (A.101) This shows that L2([0; 1]) is a reasonable, in\fnite dimensional analogue of Euclidean space with its Euclidean norm. Example A.4.10. To prove that a set of functions de\fnes an orthonormal basis for L2([0; 1]) is a highly non-trivial matter. The functions f1g[f p A.4. SPACES OF FUNCTIONS 581 Since the sum in (A.101) is \fnite, this sum can be made as small as one likes by choosing N su\u000eciently large. How large N must be clearly depends on f; in our study of Fourier series we examine this question carefully. A.4.6 Generalized functions onR Within mathematics and also in its applications the fact that many functions are not di\u000berentiable can be a serious di\u000eculty. Within the context of linear analysis, generalized functions or distributions provides a very comprehensive solution to this problem. Though it is more common in the mathematics literature, we avoid the term \\distribution,\" because there are so many other things in imaging that go by this name. In this section we outline the theory of generalized functions and give many examples. The reader wishing to attain a degree of comfort with these ideas is strongly urged to do the exercises at the end of the section. Let C1 c (R) denote in\fnitely di\u000berentiable functions de\fned onRwhich vanish outside of bounded sets. These are sometimes called test functions. De\fnition A.4.7. A generalized function onRis a linear function, l de\fned on the set of test functions such that there is a constant C and an integer k so that, for every f 2C1 c (R) we have the estimate jl(f )j\u0014 C sup x2R 2 4(1 + jxj) k kX j=0 j@j xf (x)j 3 5 (A.104) These are linear functions on C1 c (R) which are, in a certain sense continuous. The constants C and k in (A.104) depend on l but do not depend on f: The expression on the right hand side de\fnes a norm on C1 c (R); for convenience we let kf kk =sup x2R 2 4(1 + jxj) k kX j=0 j@j xf (x)j 3 5 : If f 2C1 c (R) then it easy to show that kf kk is \fnite for every k 2N[f0g: A few examples of generalized function should help clarify the de\fnition. Example A.4.11. The most famous generalized function of all is the Dirac \u000e-function. If is de\fned by \u000e(f )= f (0): It is immediate from the de\fnition that f 7! \u000e(f ) is linear and j\u000e(f )j\u0014 kf k0; so the \u000e-function is a generalized function. For j 2Nde\fne \u000e(j)(f )= @j xf (0): Since di\u000berentiation is linear, these also de\fne linear functions on C1 c (R) which satisfy the estimates j\u000e(j)(f )j\u0014kf kj: Hence these are also generalized functions. 582 APPENDIX A. BACKGROUND MATERIAL Example A.4.12. Let '(x) be a function which is integrable on any \fnite interval and such that C' = 1Z −1 j'(x)j(1 + jxj) −k \u00141 for some non-negative integer k: Any such function de\fnes a generalized function l'(f )= 1Z −1 f (x)'(x)dx: Because f has bounded support the integral converges absolutely. The linearity of the integral implies that f 7! l'(f ) is linear. To prove the estimate we observe that jf (x)j\u0014 kf kk A.4. SPACES OF FUNCTIONS 583 therefore the limit exists as \u000f ! 0: This shows that l1=x(f )= 1Z −1 (f (x) − f (0))dx 584 APPENDIX A. BACKGROUND MATERIAL The proof is completed by showing that k@xf kk \u0014kf kk+1: This is left as an exercise. A.4. SPACES OF FUNCTIONS 585 The set of generalized functions is a vector space. If l and k are generalized functions then so is the sum (l + k)(f ) d = l(f )+ k(f ) as well as scalar multiples (al)(f ) d = a(l(f )) for a 2R: Di\u000berentiation is a linear operation with respect to this vector space structure, i.e. (l + k) [1] = l[1] + k[1] and (al) [1] = al[1]: The notion of weak convergence is perfectly adapted to generalized functions. De\fnition A.4.9. A sequence flng of generalized functions converges weakly to a gener- alized function l if, for every test function f; lim n!1 ln(f )= l(f ): Weak derivatives of generalized functions behave very nicely under weak limits. Proposition A.4.2. If <ln > is a sequence of generalized functions which converge weakly to a generalized function l then for every j 2Nthe sequence of generalized functions <l[j] n > converges weakly to l[j]: Generalized functions seem to have many nice properties and they provide a system- atic way to de\fne derivatives of all functions, though the derivatives are, in general not functions. Multiplication is the one basic operation that cannot be done with generalized functions. Indeed, it is a theorem in mathematics that there is no way to de\fne a product on generalized functions so that lf \u0001 lg = lfg: However if f is a test function and l is a generalized function then the product f \u0001 l is de\fned, it is (f \u0001 l)(g) d = l(fg): This product satis\fes the usual Leibniz formula (f \u0001 l) [1] = f \u0001 l[1] + @xf \u0001 l: (A.113) This is generalized slightly in exercise A.4.16. We close this brief introduction to the idea of a generalized function with a proposition that gives a fairly concrete picture of the \\general\" generalized function in term of easier to imagine examples. Proposition A.4.3. If l is a generalized function then there is a sequence of test functions <fn > such that l is the weak limit of the sequence of generalized functions <lfn >: In other words, any generalized function is the weak limit of generalized functions de\fned by integration. 586 APPENDIX A. BACKGROUND MATERIAL Example A.4.17. Let '(x) be a smooth non-negative function with support in (−1; 1) nor- malized so that 1Z −1 '(x)dx =1: For each n 2Nde\fne 'n(x)= n'(nx); then \u000e is the limit of l'n: The generalized functions considered in this section are usually called tempered distri- butions in the mathematics literature. This is because they have \\tempered growth\" at in\fnity. A more systematic development and proofs of the results in this section can be found in [5]. The theory of generalized functions extends essentially verbatim toRn : Avery complete treatment of this subject including its higher dimensional generalizations is given in [28]. Exercise A.4.7. Show that if ' = ejxj then l' is not a generalized function. Exercise A.4.8. Prove (A.105). Exercise A.4.9. Suppose that f 2C1 c (R) show that k@xf kk \u0014kf kk+1: Exercise A.4.10. Prove (A.110). Exercise A.4.11. Compute the derivative of l1=x: Exercise A.4.12. Let '(x)=(1 −jxj)˜[−1;1](x) and, for n 2Nset 'n(x)= n'(nx): Prove that l'n converges to \u000e: Show by direct computation that l[1] 'n converges to \u000e[1]: Exercise A.4.13. Prove (A.112). Exercise A.4.14. Prove Proposition A.4.2. Exercise A.4.15. Prove (A.113). Exercise A.4.16. Let l be a generalized function and f C1(R) a function with tempered growth. This means that there is a k 2Nand constants fCjg so that j@j xf (x)j\u0014 Cj(1 + jxj) k: Show that (f \u0001 l)(g) d = l(fg) de\fnes a generalized function. Exercise A.4.17. Show that any polynomial is a function of tempered growth. Show that a smooth periodic function is a function of tempered growth. A.4. SPACES OF FUNCTIONS 587 A.4.7 Generalized functions onRn . The theory of generalized function extends essentially verbatim to functions of several variables. We give a very brief sketch. For each non-negative integer k de\fne a semi-norm on CIc(Rn ) by setting kf kk =sup x2Rn 2 4(j1+ kxk) k X \u000b\u000b\u000b j\u000b\u000b\u000bj\u0014k j@\u000b\u000b\u000b x f (x)j 3 5 : Here \u000b\u000b\u000b is an n-multi-index, i.e. an n-tuple of non-negative integers, \u000b\u000b\u000b =(\u000b1;::: ;\u000bn)with @\u000b\u000b\u000b x d = @\u000b1 x1 \u0001\u0001\u0001 @\u000bn xn and j\u000b\u000b\u000bj d = \u000b1 + \u0001\u0001\u0001 + \u000bn: De\fnition A.4.10. A linear function l : C1 c (Rn ) !Ris a generalized function if there exists a k 2N[f0g and a constant C so that jl(f )j\u0014 Ckf kk: As before the set of generalized functions is a vector space. A sequence of generalized functions, <ln > converges weakly to a generalized function l provided that lim n!1 ln(f )= l(f ) for every f 2C1 c (Rn ): Example A.4.18. The Dirac \u000e-function is de\fned in n-dimensions by \u000e(f )= f (0): It satis\fes the estimate j\u000e(f )j\u0014 kf k0 and is therefore a generalized function. Example A.4.19. If ' is a locally integrable function of tempered growth, i.e. there is a k \u0015 0and aconstantsothat j'(x)j\u0014 C(1 + kxk) k then l'(f )= ZRn '(x)f (x)dx satis\fes jl'(f )j\u0014 C 0kf kn+1+k: (A.114) This shows that l' is a generalized function. If \u000b\u000b\u000b is an n-multi-index and f 2C1 c (Rn )then @\u000b\u000b\u000b x f is also in C1 c (Rn ) and satis\fes the estimates k@\u000b\u000b\u000b x f kk \u0014kf kk+j\u000b\u000b\u000bj: As before this allows us to extend the notion of partial derivatives to generalized functions. De\fnition A.4.11. If l is a generalized function then, for 1 \u0014 j \u0014 n the weak jth-partial derivative of l is the generalized function de\fned by [@xj l](f )= (−1)l(@xj f ): (A.115) 588 APPENDIX A. BACKGROUND MATERIAL Since @xj l is a generalized function as well, it also partial derivatives. To make a long story short, for an arbitrary multi-index \u000b\u000b\u000b the weak \u000b\u000b\u000bth-partial derivative of the generalized function l is de\fned by [@\u000b\u000b\u000b x l](f )=(−1) j\u000b\u000b\u000bjl(@\u000b\u000b\u000b x f ): (A.116) If f; g 2C1 c (Rn ) then the n-dimensional integration by parts formula states that ZRn [@xj f (x)]g(x)dx = − ZRn [@xj g(x)]f (x)dx: (A.117) Applying this formula recursively gives the integration by parts for higher order derivatives ZRn [@\u000b\u000b\u000b x f (x)]g(x)dx =(−1) j\u000b\u000b\u000bj ZRn [@\u000b\u000b\u000b x g(x)]f (x)dx: (A.118) It therefore follows that if f 2C1 c (Rn ) then the de\fnition of the weak partial derivatives of lf is consistent with the classical de\fnition of the derivatives of f in that @\u000b\u000b\u000b x lf = l@ααα x f for all \u000b\u000b\u000b: (A.119) Finally we remark that every generalized function onRn is a weak limit of \\nice\" generalized functions. Proposition A.4.4. If l is a generalized function onRn then there is a sequence of func- tions <fn >ˆC1 c (Rn ) so that l(g) = lim n!1 lfn(g) for all g 2C1 c (Rn ): Exercise A.4.18. Provethatif f 2C1 c (Rn ) then and j \u0014 k then j@\u000b\u000b\u000b x f (x)j\u0014 kf kk A.5. BOUNDED LINEAR OPERATORS 589 Exercise A.4.23. By writing the integrals overRn as iterated 1-dimensional integrals, prove (A.117). Deduce (A.118). Exercise A.4.24. Prove (A.119). Exercise A.4.25. Let '(x; y)= ˜[0;1)(x) \u0001 ˜[0;1)(y): Show that @x@yl' = \u000e: Exercise A.4.26. Let '(x; y)= 1 590 APPENDIX A. BACKGROUND MATERIAL From the example we see that if the state of the system is described by a function then an idealized, model measurement is described by a function as well. The measurement should be thought of as a function of the state. The measurement process should therefore be thought of as a map. To determine the state of the system from measurements we need to invert this map. From our experience with \fnite dimensional problems we know that the easiest case to study is that of a linear mapping. In the \fnite dimensional case we gave a complete theory for solvability of linear equations. This theory is entirely algebraic which means that it does not require a way to measure distances. In in\fnite dimensions it remains true that linear maps are the simplest to analyze. The most important di\u000berence between the \fnite and in\fnite dimensional cases is that there is no e\u000bective way to study the behavior of linear maps without \frst introducing norms. This is quite similar to what we have already seen for linear functions. De\fnition A.5.1. Let (X; k\u0001 k)and (Y; k\u0001k0) be normed linear spaces. A linear map A : X ! Y is bounded if there is a constant M such that, for all x 2 X we have kAxk 0 \u0014 M kxk: Such maps are often called bounded linear operators. As in the \fnite dimensional case this estimate implies that the map is continuous, for kAx1 − Ax2k 0 = kA(x1 − x2)k 0 \u0014 M kx1 − x2k: If (X; k\u0001k)and (Y; k\u0001k0)are complete normed linear spaces then continuity is equivalent to boundedness, see [16] or [66]. In the \fnite dimensional case a linear map is invertible if it is onto and its null-space consists of the zero vector. This of course remains true in the in\fnite dimensional case as well. But in this case, we also need to ask if the inverse is continuous. For the case of complete normed linear spaces this question has a very satisfactory answer. Theorem A.5.1 (Open Mapping Theorem). Suppose that (X; k\u0001 k) and (Y; k\u0001k0) are complete normed linear spaces and A : X ! Y is a continuous linear map. Then A has a continuous inverse if and only if it is both one-to-one and onto. Suppose that A : X ! Y is an invertible linear map between complete normed linear spaces and let B denote its inverse. Because B is continuous we see that there is a constant M so that kByk\u0014 M kyk 0: For y = Ax this can be rewritten kAxk 0 \u0015 1 A.5. BOUNDED LINEAR OPERATORS 591 We now consider examples which illustrate some di\u000berences between the \fnite and in\fnite dimensional cases. Example A.5.2. Let X = l2 and Y = l2; we de\fne a linear map by setting A<aj >=< aj 592 APPENDIX A. BACKGROUND MATERIAL Example A.5.4. The operator A in example A.5.2 is not invertible because it does not satisfy the estimate (A.121) for any M: For each N de\fne the operator BN <aj >= (jaj for j \u0014 N; 0for j> N: This is an approximate left inverse for A in the sense that BN \u000e A<aj >= ( aj for j \u0014 N; 0for j> N: Thus we have the identity ka − BN \u000e Aak 2 2 = 1X j=N +1 jajj 2: In applications, the coe\u000ecients (aN +1;aN +2;::: ) would represent the \\high frequency\" information in <aj > which is attenuated by the measurement process and corrupted by noise. As such it cannot be measured reliably. For an appropriate choice of N; BN provides an approximate inverse to our measurement process A which captures all the reliable data that is present in the measurements and discards the parts of the measurement that are not usable. The choice of N depends on the resolution and accuracy of the measuring device. Example A.5.5. We consider another bounded operator de\fned on lp for any p \u0015 1: It is de\fned by setting S :(a1;a2;a3;::: ) 7! (0;a1;a2;a3;::: ): This is called a shift operator. For any p \u0015 1 we see that kSakp = kakp: Clearly Sa = 0 if an only if a = 0: However we see that the image of S is not all of lp: A vector b =(b1;b2;b3;::: ) is in the image of S if and only if b1 =0: This is di\u000berent way that a linear transformation of in\fnite dimensional space having only the zero vector in its null space can fail to be invertible. Linear transformations of function spaces are often written as integrals. For example if k(x; y) is a function de\fned on [0; 1] \u0002 [0; 1] then Kf (x)= 1Z 0 k(x; y)f (y)dy de\fnes a linear transformation. It can be subtle to decide whether or not this is a bounded operator. Here is a simple criterion which implies that K : L2([0; 1]) ! L2([0; 1]) is bounded. A.5. BOUNDED LINEAR OPERATORS 593 Proposition A.5.1. Suppose that k(x; y) satis\fes ZZ [0;1]\u0002[0;1] jk(x; y)j 2dxdy < 1; then the linear operator K : L2([0; 1]) ! L2([0; 1]) is bounded. Proof. The proof is an application of the H¨older inequality. We need to show that there is a constant M so that 1Z 0 jKf (x)j 2dx \u0014 M 1Z 0 jf (x)j 2dx for every f 2 L2([0; 1]): If we write out the left hand side and use the H¨older inequality we see that 1Z 0 jKf (x)j 2dx = 1Z 0 j 1Z 0 k(x; y)f (y)dyj 2dx \u0014 1Z 0 2 4 1Z 0 jk(x; y)j 2dy 1Z 0 jf (y)j 2dy 3 5 dx = 2 4 1Z 0 jf (y)j 2dy 3 5 1Z 0 1Z 0 jk(x; y)j 2dydx: (A.122) From (A.122) we see that 1Z 0 jKf (x)j 2dx \u0014 2 4 1Z 0 1Z 0 jk(x; y)j 2dydx 3 5 1Z 0 jf (x)j 2dx; which establishes the needed estimate with M = v u u u t 594 APPENDIX A. BACKGROUND MATERIAL Exercise A.5.2. Let k(x; y) be a function de\fned on [0; 1] \u0002 [0; 1] for which there is a constant M such that max x2[0;1] 1Z 0 jk(x; y)jdy \u0014 M and max y2[0;1] 1Z 0 jk(x; y)jdx \u0014 M: Show that the operator f 7! Kf de\fned by k(x; y) is a bounded operator from L2([0; 1]) ! L2([0; 1]): Exercise A.5.3. De\fne an operator A : L1(R) !C0(R) by letting Af (x)= x+1Z x f (s)ds: Prove that A is bounded and that Af = 0 implies that f =0: Is A invertible? Why or why not? Exercise A.5.4. In example A.5.2 show directly that there is no constant M> 0sothat A satis\fes the estimate (A.121) for all a 2 L2: A.6 Functions in the real world In section A.4 we considered functions from the point of view of a mathematician. In this approach, functions are described by abstract properties such as di\u000berentiability or integrability. Using these properties functions are grouped together into normed vector spaces. The principal reason for doing this is to study the mapping properties of linear transformations. It is a very abstract situation because we do not, even in principle, have a way to compute most of the functions under consideration: they are described by their properties and not de\fned by rules or formulˆ. This level of abstraction even leads to di\u000eculties in the mathematical development of the subject. In practice we can only approximately measure a function, in most circumstances, at a \fnite collection of values. What mediates between these two, very di\u000berent views of functions? This is a question with many di\u000berent answers, but the basic ideas involve the concepts of approximation, sampling and interpolation. A.6.1 Approximation The basic problem of approximation theory is to begin with a function from an abstract class, for example continuous functions and approximate it, in an appropriate sense, by functions from a more concrete class, for example polynomials. We begin with the basic theorem in this subject. Theorem A.6.1 (The Weierstrass Approximation Theorem). Given a function f 2 C0([0; 1]) and an \u000f> 0 there is a polynomial p such that kf − pkC0 <\u000f: (A.123) A.6. FUNCTIONS IN THE REAL WORLD 595 The set of polynomial functions has an analogous relationship to the set of continuous functions as the set of \fnite decimal expansions has to the set of real numbers. For the purposes of approximate computations (even with a speci\fed error) it su\u000eces to work with polynomials. This theorem is the prototype for many other such results. A very useful result for Lp-spaces, uses approximation by step functions. Recall that if E is an subset ofRthen its characteristic function is de\fned by ˜E(x)= ( 1if x 2 E; 0if x=2 E: De\fnition A.6.1. A function f is called a step function if there is a \fnite collection of intervals f[ai;bi): i =1;::: ;N g and constants fcig so that f (x)= NX i=1 ci˜[ai;bi)(x): Step functions are computable functions. Theorem A.6.2 (Lp-approximation Theorem). Suppose that 1 \u0014 p< 1;f 2 Lp(R) and \u000f> 0 is given. There exists a step function F such that kf − F kLp <\u000f: (A.124) Note that p = 1 is excluded, the theorem is false in this case. The proof of this theorem uses the de\fnition of the Lebesgue integral and the structure of Lebesgue measurable sets. It is beyond the scope of this text but can be found in [16]. It has a very useful corollary. Corollary A.6.1. Suppose that 1 \u0014 p< 1;f 2 Lp(R) and \u000f> 0 is given. There exists a continuous function G such that kf − GkLp <\u000f: (A.125) Proof. Theorem A.6.2 gives the existence of a step function F so that kf − F kLp <\u000f=2: This means that it su\u000eces to \fnd a continuous function G so that kF − GkLp < \u000f 596 APPENDIX A. BACKGROUND MATERIAL For j =0 or m we let l0(x)= (0if jx − a0j >\u0011; cj x−(a0−η) A.6. FUNCTIONS IN THE REAL WORLD 597 Theorem A.6.3 (Jackson's Theorem). If f 2Ck([0; 1]) for a k 2Nthen En(f ) \u0014 C 598 APPENDIX A. BACKGROUND MATERIAL Let h\u0001; \u0001iw denote the inner product associated with this weighted L2-norm hf; giw = 1Z 0 f (x)g(x)w(x)dx: We use the linearly independent functions f1;x;x2;::: g to de\fne a sequence of polynomials fPng by the properties (1). deg Pn = n; (2). hPn;Pmiw = \u000emn; The algorithm is exactly the same as the \fnite dimensional case: Step 1 Let P0 =[h1; 1iw] − 1 A.6. FUNCTIONS IN THE REAL WORLD 599 This function is called the best, weighted, least squares approximation to f of degree n: Thus the best polynomial approximations with respect to these weighted L2-norms are easy to \fnd. If f is continuous how does kf − pnkC0 behave? Do these give better approximations if the function f is smoother? The answers to these questions depend, in part on the weight function. For the case w(x)=[x(1 − x)] − 1 600 APPENDIX A. BACKGROUND MATERIAL A.6.2 Sampling and Interpolation Suppose that we have a system whose state is described by a function f of a variable t; f (t): The simplest way to model a measurement is as evaluation of this function. That is we have a sequence of \\times\" <tj > and the measurement consists in evaluating f (tj): The sequence of numbers <f (tj) > are called the samples of f at the times <tj >: The sample times are usually labeled in a monotone fashion, that is tj <tj+1: The di\u000berences \u0001tj = tj −tj−1 are called the sample spacings. If they are all equal to a single value \u0001t then we say f is uniformly sampled with sample spacing \u0001t: In a real application we can measure at most \fnitely many samples and of course we can only measure them with \fnite precision. In analyzing measurement processes it is often useful to assume that we can evaluate f along an in\fnite sequence and that the measurements are exact. A question of primary interest is to decide what the samples <f (tj) > tell us about the value of f (t)for times t not in our sample set. The answer of course depends on how close together the sample times are and apriori knowledge of the smoothness of f: Such information is usually incorporated implicitly into a model. Suppose that we sample a di\u000berentiable function f (t)at the points ftjg: Let t lie between tj and tj+1 then the mean value theorem implies that there is a point ˝ 2 (tj;tj+1) such that f (t)= f (tj)+ f 0(˝ )(t − tj): If the points are close together and the derivative is continuous then f 0(˝ ) ' f (tj) − f (tj+1) A.6. FUNCTIONS IN THE REAL WORLD 601 lines. Because of the very random nature of this type of measurement, neither scheme gives very good results. Indeed, if one could \fnd a reliable way to predict the price of a stock, even ten minutes hence, he or she could easily become a very rich person! If we know that f (t) is a polynomial function of t then a \fnite number of samples determines f completely. If f (t) is a polynomial of degree 0, in other words a constant, then a single sample determines f: If the degree is 1 then 2 samples are required and if the degree is n then n + 1-samples su\u000ece. Indeed there are simple explicit formulˆ to reconstruct a polynomial from such data. For example, if the sample points are ft1;t2g then a linear polynomial is reconstructed as follows f (t)= f (t1) t − t2 602 APPENDIX A. BACKGROUND MATERIAL 0 0.2 0.4 0.6 0.8 1 1.2 0.2 0.4 0.6 0.8 1x Figure A.3: Polynomial interpolants for jx − 1 A.7. NUMERICAL TECHNIQUES FOR DIFFERENTIATION AND INTEGRATION603 Evaluating this function gives ci(ti+1)= ci+1(ti+1)= fi+1; c 0 i(ti+1)= c 0 i+1(ti+1)= ai+1: (A.132) In other words, for any choice of the values of fa1;::: ;an−1g these functions piece together to de\fne a continuously di\u000berentiable function, interpolating the values of f: To \fnd the spline with these properties we need to select these coe\u000ecients so that the resultant function also has a continuous second derivative. Evaluating the second derivatives and comparing at the adjacent endpoints we derive the relations ai +4ai+1 + ai+2 = 3 604 APPENDIX A. BACKGROUND MATERIAL If f (t) is a continuous function and we set mN =max 0\u0014j\u0014N −1 max t2[ j A.7. NUMERICAL TECHNIQUES FOR DIFFERENTIATION AND INTEGRATION605 A.7.1 Numerical integration In addition to the (right) Riemann sum formula, RN (f )= 1 606 APPENDIX A. BACKGROUND MATERIALA.7. NUMERICAL TECHNIQUES FOR DIFFERENTIATION AND INTEGRATION607 –1 –0.5 0 0.5 1 0.2 0.4 0.6 0.8 1x Figure A.4: A random piecewise linear function. 608 APPENDIX A. BACKGROUND MATERIAL to obtain approximations to f 0( j A.7. NUMERICAL TECHNIQUES FOR DIFFERENTIATION AND INTEGRATION609610 APPENDIX A. BACKGROUND MATERIAL of evaluating a function at the arguments f j A.7. NUMERICAL TECHNIQUES FOR DIFFERENTIATION AND INTEGRATION611612 APPENDIX A. BACKGROUND MATERIAL Appendix B Basic analysis This appendix contains some of the facts from analysis that are used in this book. Many good treatments of this material are available, for example [67] or [73]. De\fnition B.0.1. A subset S of the real numbers is bounded from below if there is some number m so that m \u0014 x; 8x 2 S; and bounded from below if there is a number M such that M \u0015 x; 8x 2 S: If a set is bounded from above and below, then we say it is bounded. If a set is bounded from below then we de\fne inf S as the largest number m such that m \u0014 x 8x 2 S and if S is bounded from above we de\fne sup S to be the smallest number such that x \u0014 M 8x 2 S: B.1 Sequences The most important idea in introductory analysis is the concept of a sequence. A sequence is a function from the positive integers (natural numbers =N:) to some set. The simplest examples are sequences of real numbers. Using standard functional notation we could denote such a function as x :N−!R; then the nth term of the sequence would be denoted x(n): It is customary not to use functional notation but rather to use subscripts, so that the nth term is denoted by xn.We also consider sequences of functions, for example we could consider a sequence of functions de\fned on [0; 1];fn(x) denotes the value of the nth term in the sequence at the point x 2 [0; 1]: Almost as important as the concept of a sequence is the concept of a subsequence. Given a sequence fxng; a subsequence is de\fned by selecting a subset of fxng and keeping them in the same order as they appear in fxng: In practice this amounts to de\fning a function fromNto itself. We denote this function by nj. It must have the following properties nj <nj+1: The jth term of the subsequence is given by xnj : As an example, consider the sequence xn =(−1)nn; the mapping nj =2j de\fnes the subsequence xnj = (−1)2j 2j: 613 614 APPENDIX B. BASIC ANALYSIS De\fnition B.1.1. A sequence of real numbers, fxng has a limit if there is a number L such that given any \u000f> 0 there exists a N> 0 such that jxn − Lj <\u000f whenever n>N: A sequence with a limit is called a convergent sequence, we then write lim n!1 xn = L: The limit, when it exists is unique. A sequence may fail to have limit but it may have a subsequence which does. In this case the sequence is said to have a convergent subsequence. For example xn =(−1)n 1 B.4. SERIES 615 Theorem B.3.1 (Bolzano{Weierstrass Theorem). A bounded sequence of real num- bers has a convergent subsequence. Note that this does not assert that any bounded sequence converges but only that any bounded sequence has a convergent subsequence. De\fnition B.3.1. In general, if S ˆRthen the set of points which can be obtained as limits of sequences fxngˆ S is called the set of accumulation points of S: A subset S is dense in an interval I; if I is a subset of the set of accumulation points of S: For example the rational numbersQare dense in every interval. The following two lemmas are very useful Lemma B.3.1. If xn;yn;zn are sequences of real numbers such that xn \u0014 yn \u0014 zn and xn and zn are convergent with L = lim n!1 xn = lim n!1 zn then yn converges with lim n!1 yn = L: Lemma B.3.2. If xn \u0015 0 is convergent then lim n!1 xn \u0015 0: In the above discussion of limits we always assumed that the limit is known in advance. There is a criterion, due to Cauchy which implies that a given sequence has a limit but makes no reference to the limit itself. Theorem B.3.2 (Cauchy Criterion for Sequences). If fxng is a sequence of real numbers such that given \u000f> 0 there exists an N for which jxn − xmj <\u000f whenever both n and m are greater than N; then the sequence is convergent. A sequence satisfying this condition is called a Cauchy sequence. B.4 Series A series is the sum of a sequence, it is denoted by 1X n=1 xn: 616 APPENDIX B. BASIC ANALYSIS De\fnition B.4.1. A series converges if the sequence of partial sums sk = kX n=1 xn; converges. In this case 1X n=1 xn d = lim k!1 sk: If a series does not converge then it diverges. De\fnition B.4.2. A series converges absolutely if the sum of the absolute values 1X n=1 jxnj converges. The following theorem describes the elementary properties of series. Theorem B.4.1 (Theorem on Series). Suppose that xn;yn are sequences. Suppose that 1P n=1 xn; 1P n=1 yn converge then 1X n=1 (xn + yn) converges and 1X n=1 (xn + yn)= 1X n=1 xn + 1X n=1 yn; If a 2R 1X n=1 axn = a 1X n=1 xn; If xn \u0015 0 for all n; then 1X n=1 xn \u0015 0: (B.2) There are many criterion that are used to determine if a given series converges. The most important is the comparison test Theorem B.4.2 (Comparison Test). Suppose that xn;yn are sequences such that jxnj\u0014 yn if 1P n=1 yn converges then so does 1P n=1 xn: If 0 \u0014 yn \u0014 xn and 1P n=1 yn diverges then so does 1P n=1 xn: To apply this test we need to have some series which we know converge or diverge. The simplest case is a geometric series. This is because there is a formula for the partial sums: kX n=0 a k = ak+1 − 1 B.4. SERIES 617 Theorem B.4.3 (Convergence of Geometric Series). A geometric converges if and only if jaj < 1: The root and ratio tests are really special cases of the comparison test where the series are comparable to geometric series. Theorem B.4.4 (Ratio Test). If xn is a sequence with lim sup n!1 \f \f \f \f xn+1 618 APPENDIX B. BASIC ANALYSIS Theorem B.4.7 (Alternating Series Test). Suppose that xn is a sequence such that the sign alternates, the limn!1 xn =0 and jxn+1j\u0014jxnj then 1X n=1 xn converges. Note that this test requires that the signs alternate, the absolute value of the sequence is monotonely decreasing, and the sequence tends to zero. If any of these conditions are not met the series may fail to converge. B.5 Limits of Functions and Continuity The next thing to consider is the behavior of functions de\fned on intervals inR: Suppose that f (x) is de\fned for x 2 (a; c) [ (c; b): This is called a punctured neighborhood of c. De\fnition B.5.1. We say that f (x) has a limit, L as x approaches c if given \u000f> 0there exists \u000e> 0 such that jf (x) − Lj <\u000f provided 0 < jx − cj <\u000e and we write lim x!c f (x)= L: Note that in this de\fnition nothing is said about the value of f (x)at x = c.This has no bearing at all on whether the limit exists. De\fnition B.5.2. If f (c) is de\fned and we have that lim x!c f (x)= f (c) then we say that f (x) is continuous at x = c.If f (x) is continuous for all x 2 (a; b)then we say that f (x) is continuous on (a; b): In addition to the ordinary limit, we also de\fne one sided limits. If f (x) is de\fned in (a; b) and there exists an L such that given \u000f> 0 there exists \u000e such that jf (x) − Lj <\u000f provided 0 <x − a< \u000e then lim x!a+ f (x)= L: If instead jf (x) − Lj <\u000f provided 0 <b − x<\u000e then lim x!b− f (x)= L: The rules for dealing with limits of functions are very similar to the rules for handling limits of sequences B.5. LIMITS OF FUNCTIONS AND CONTINUITY 619 Theorem B.5.1 (Algebraic Rules for Limits of Functions). Suppose that f (x);g(x) are de\fned in a punctured neighborhood of c and that lim x!c f (x)= L; lim x!c g(x)= M: Then lim x!c (af (x)) exists and equals aL for all a 2R; lim x!c (f (x)+ g(x)) exists and equals L + M; lim x!c (f (x)g(x)) exists and equals LM; lim x!c f (x) 620 APPENDIX B. BASIC ANALYSIS B.6 Diﬀerentiability A function de\fned in a neighborhood of a point c is said to be di\u000berentiable at c if the function g(x)= f (x) − f (c) B.7. HIGHER ORDER DERIVATIVES AND TAYLOR’S THEOREM 621 B.7 Higher Order Derivatives and Taylor’s Theorem If the \frst derivative of function, f 0(x) happens itself to be di\u000berentiable then we say that f (x) is twice di\u000berentiable. The second derivative is denoted by f 00(x): Inductively if the kth derivative happens to be di\u000berentiable then we say that f is (k + 1){times di\u000berentiable. We denote the kth derivative by f [k](x): For a function which has n derivatives we can \fnd a polynomial which agrees with f (x)to order n − 1atapoint. Theorem B.7.1 (Taylor's Theorem). Suppose that f (x) has n derivatives at a point c then f (x) − n−1X j=0 f [j](c)(x − c)j 622 APPENDIX B. BASIC ANALYSIS These are called the upper and lower Riemann sums, observe that U (f; P ) \u0015 L(f; P ): (B.6) If P and P 0 are partitions with the property that every point in P is also a point in P 0 then we say that P 0 is a re\fnement of P and write P< P 0: If P1 and P2 are two partitions then, by using the union of the points in the two underlying sets, we can de\fne a new partition P3 with the property that P1 <P3 and P2 <P3: A partition with this property is called a common re\fnement of P1 and P2: From the de\fnitions it is clear that if P< P 0 then U (f; P ) \u0015 U (f; P 0)and L(f; P ) \u0014 L(f; P 0): (B.7) We de\fne the upper Riemann integral of f to be −Z b af (x)dx =inf P U (f; P ); and the lower Riemann integral to be Z − b a f (x)dx =sup P L(f; P ): In light of (B.6) it is not hard to show that −Z b af (x)dx \u0015 Z − b a f (x)dx: De\fnition B.8.1. A bounded function f de\fned on an interval [a; b]is Riemann integrable if −Z b af (x)dx Z − b a f (x)dx: In this case we denote the common value by Z b a f (x)dx: Most \\nice\" functions are Riemann integrable. For example we have the following basic result B.8. INTEGRATION 623 Theorem B.8.1. Suppose that f is a piecewise continuous function de\fned on [a; b] then f is Riemann integrable and Z b a f (x)dx = lim N !1 NX j=1 f (a + j 624 APPENDIX B. BASIC ANALYSIS Theorem B.8.5 (The Fundamental Theorem of Calculus). If f is a continuous function on [a; b] then F 0(x)= f (x): If f 2C1([a; b]) then Z b a f 0(x)dx = f (b) − f (a): There are two further basics tools needed to compute and manipulate integrals. The \frst is called integration by parts, it is a consequence of the product rule for derivatives, see Proposition B.6.1. Proposition B.8.1 (Integration by parts). If f; g 2C1([a; b]) then Z b a f 0(x)g(x)dx = f (b)g(b) − f (a)g(a) − Z b a f (x)g0(x)dx: The other formula follows from the chain rule, Proposition B.6.2. Proposition B.8.2 (Change of variable). Let g be a monotone increasing, di\u000berentiable function de\fned [a; b] with g(a)= c; g(b)= d and let f be a Riemann integrable function on [c; d]: The following formula holds Z d c f (y)dy = Z b a f (g(x))g0(x)dx: There is analogous treatment for the integration of functions of several variables. A result which is especially important in applications is Fubini's theorem. Only a very special case is usually required. The statement of the special case requires only the de\fnition of the one-dimensional Riemann integral. Suppose that f (x; y) is a continuous function on the rectangle [a; b] \u0002 [c; d] then for each \fxed valued of y; f (\u0001;y) is an integrable function on [a; b] and similarly for each \fxed x; f (x; \u0001) is an integrable function on [c; d]: Performing these integrals leads to two new functions g(y)= Z b a f (x; y)dx; h(x)= Z d c f (x; y)dy which are themselves integrable on the appropriate intervals. Theorem B.8.6 (Fubini's theorem). Let f (x; y) be a continuous function on [a; b]\u0002[c; d] then Z d c \u0012Z b a f (x; y)dx \u0013 dy = Z b a \u0012Z d c f (x; y)dy\u0013 dx: More colloquially we say that we can change the order of the integrations. B.9 Improper integrals In the previous section we de\fned the Riemann integral for bounded functions on bounded intervals. In applications both of these restrictions need to be removed. This leads to various notions of improper integrals. The simplest situation is that of a function f (x) B.9. IMPROPER INTEGRALS 625 de\fned on [0; 1) and integrable on [0;R] for every R> 0: We say that the improper integral, Z 1 0 f (x)dx exists if the limit, lim R!1 Z R 0 f (x)dx (B.9) exists. In this case the improper integral is given by the limiting value. By analogy with the theory of in\fnite series there are two distinct situations in which the improper integral exists. If the improper integral of jf j exists then we say that f is absolutely integrable on [0; 1): Example B.9.1. The function (1 + x2)−1 is absolutely integrable on [0; 1): Indeed we see that if R< R0 then 0 \u0014 Z R0 0 dx 626 APPENDIX B. BASIC ANALYSIS There are similar de\fnitions for the improper integrals Z 0 −1 f (x)dx and Z 1 −1 f (x)dx: The only small subtlety is that we say that the improper integral exists in the second case only when both the improper integrals Z 0 −1 f (x)dx; Z 1 0 f (x)dx exist separately. Similar de\fnitions apply to functions de\fned on bounded intervals (a; b) which are integrable on any subinterval [c; d]: We say that the improper integral Z b a f (x)dx exists if the limits lim c!a+ Z e c f (x)dx and lim c!b− Z c e f (x)dx both exist. Here e is any point in (a; b); the existence or non-existence of these limits is clearly independent of which (\fxed) point we use. Because the improper integrals are de\fned by limits of proper integrals they have the same linearity properties as integrals. For example: Proposition B.9.1. Suppose that f and g are improperly integrable on [0; 1) then f + g is as well and Z 1 0 (f (x)+ g(x))dx = Z 1 0 f (x)dx + Z 1 0 g(x)dx; for a 2R;af is improperly integrable and Z 1 0 af (x)dx = a Z 1 0 f (x)dx: The \fnal case that requires consideration is that of a function f de\fned on a deleted interval [a; b) [ (b; c] and integrable on subintervals of the form [a; e]and [f; c]where a \u0014 e<b and b<f \u0014 c: If both limits lim e!b− Z e a f (x)dx and lim f !b+ Z c f f (x)dx existthen wesay that f is improperly integrable on [a; b]: For example the function f (x)= x − 1 B.9. IMPROPER INTEGRALS 627 There is however a further extension of the notion of integrability that allows us to assign a meaning to Z 1 −1 dx 628 APPENDIX B. BASIC ANALYSIS The function f is continuously di\u000berentiable and therefore the function g(x)= ( f (x)−f (0) Bibliography [1] Lars V. Ahlfors, Complex analysis, McGraw-Hill, New York, 1979. [2] R.E. Alvarez and A. Macovski, Energy selective reconstructions in x-ray computerized tomography, Phys. Med. Biol. 21 (1976), 733{744. [3] R.J. Barlow, Statistics, a guide to the use of statistical methods in the physical sciences, The Manchester Physics Series, John Wiley& Sons, 1989. [4] Harrison H. Barrett and William Swindell, Radiological imaging, Academic Press, 1981. [5] R. Beals, Advanced mathematical analysis, Graduate Texts in Mathematics, vol. 119, Springer Verlag, 1988. [6] George B. Benedek and Felix M.H. Villars, Physics with Illustrative Examples from Medicine and Biology, Electricity and Magnetism, 2nd ed., AIP Press and Springer Verlag, New York, 2000. [7] William E. Boyce and Richard C. DiPrima, Elementary di\u000berential equations, 6th ed., Wiley, New York, 1997. [8] Robert Grover Brown, Introduction to random signal analysis and Kalman \fltering, John Wiley & Sons, New York, 1983. [9] Yair Censor and Gabor T. Herman, On some optimization techniques in image recon- struction from projections, Appl. Numer. Math. 3 (1987), 365{391. [10] A.M.J. Cormack, Representation of a function by its line integrals, with some radio- logical applications i., ii.,J.Applied Physics 34,35 (1963,1964), 2722{2727, 195{207. [11] John D'Angelo and Douglas West, Mathematical thinking, problem-solving and proofs, 2nd ed., Prentice Hall, Upper Saddle River, NJ, 2000. [12] J.L. Doob, Stochastic processes, John Wiley & Sons, Inc., New York, 1953. [13] Edward R. Dougherty, Random processes for image and signal processing, SPIE/IEEE series on imaging science and engineering, IEEE press, Piscataway, NJ, 1999. [14] Charles L. Epstein and Bruce Kleiner, Spherical means in annular regions,CPAM 44 (1993), 441{451. 629 630 BIBLIOGRAPHY [15] W. Feller, Introduction to probability theory and its applications, I and II, John Wiley & Sons, New York, 1968, 1971. [16] G.B. Folland, Real analysis, modern techniques and their applications, John Wiley and sons, New York, NY, 1984. [17] BIBLIOGRAPHY 631 [33] Peter M. Joseph, Image noise and smoothing in computed tomography (CT) scanners, SPIE { Optical Instrumentation in Medicine VI 127 (1977), 43{49. [34] 632 BIBLIOGRAPHY [51] Frank Natterer and Frank W¨ubbelling, Mathematical methods in image reconstruction, SIAM, Philadelphia, 2001. [52] Zeev Nehari, Conformal mapping, Dover, New York, 1952. [53] Alan V. Oppenheim and Ronald W. Schafer, Digital signal processing, Prentice Hall, 1975. [54] Sidney C. Port Paul G. Hoel and Charles J. Stone, Introduction to Stochastic Processes, Houghton Mi\u000fin, Boston, Ma, 1972. [55] Isaac Pesenson, A sampling theorem on homogeneous manifolds, Trans. of the Amer. Math. Soc. 352 (2000), 4257{4269. [56] Robin D. Spital Peter M. Joseph and Charles D. Stockham, The e\u000bects of sampling on CT-images, Computerized Tomography 4 (1980), 189{206. [57] Mark A. Pinsky and Michael E. Taylor, Pointwise Fourier inversion: a wave equation approach, The Journal of Fourier Analysis and Applications 3 (1997), 647{703. [58] G. P\u0013olya and G. Szeg}o, Problems and theorems in analysis, I, Springer-Verlag, New York, 1972. [59] Johan Radon, ¨uber die bestimmung von funktionen durch ihre integralwerte l¨angs gewisser mannigfaltigkeiten, Ber. Sachs.Akad. Wiss., Leipzig 69 (1917), 262{267. [60] Matthew Sands Richard P. Feynman, Robert B. Leighton, The Feynman Lectures on Physics,vol.2, Addison-Wesley, Reading, Mass., 1964. [61] F. Riesz and B. Sz.-Nagy, Functional analysis, Fredrick Ungar, New York, 1955. [62] Theodore J. Rivlin, The Chebyshev polynomials, New York, NY, Wiley, 1974. [63] BIBLIOGRAPHY 633 [70] L.A. Shepp and B.F. Logan, The Fourier reconstruction of a head section, IEEE Trans. Nuc. Sci. NS-21 (1990), 21{43. [71] L.A. Shepp and J.A. Stein, Simulated reconstruction artifacts in computerized X-ray tomography, In Ter-Pergossian [77], pp. 33{48. [72] Elias M. Stein and Guido Weiss, Introduction to Fourier analysis on Euclidean spaces, Princeton Press, Princeton, NJ, 1971. [73] Robert Strichartz, The way of analysis, Jones and Bartlett, Boston,MA, 1995. [74] Kunio Tanabe, Projection method for solving a singular system of linear equations and its applications, Num. Math. 17 (1971), 203{214. [75] M.E. Taylor, Pseudodi\u000berential Operators, Princeton Mathematical Series, vol. 34, Princeton University Press, Princeton, NJ, 1981. [76] Index L2-derivative higher derivatives, 99 higher dimensions, 127 one dimension, 98 periodic case, 193 periodic case, higher dimensions, 216 L2([0; 1), 580 L2([0; 1]), 570 C0([0; 1]), 569 Cj(R), 74 Ck([0; 1]), 570 \u000eij, 532 ˙-algebra, 425 lp spaces, 576 Abel transform de\fnition, 56 inversion formula, 59 absolute convergence, 558 absolute value, 551 absolutely convergent series, 616 accumulation points, 615 addition vector, 526 aliasing, 229 alternating series test, 558, 618 amplitude, 249, 253 apodizing \flter, 258 function, 258, 350 approximation step function, 595 approximation problems, 192 back substitution, 545 back-projection formula, 56 bandwidth e\u000bective, 282 basis, 527, 530 Bayes' law, 438 Bayesian, 417 beam hardening, 45 beam pro\fle, 367 Beer's law, 36 Bernoulli detector, 466 Bessel function, 567 asymptotic expansion, 568 integral formula, 568 power series expansion, 567 Bessel's inequality, 192 bilinear function, 543 binary representation, 516 binary string, 519 binomial formula elementary, 564 general, 564 Borel sets, 426 bounded linear operator, 590 Brownian motion, 482 capacitor, 267 carrier frequency, 234 Cauchy criterion, 523 Cauchy sequence, 523, 615 normed vector space, 574 onRn , 537 Cauchy-Schwarz inequality L2(R), 80Rn , 540 centered moment, 443 Central limit theorem, 459 Central slice theorem, 135 higher dimensions, 171 change of variable formula, 624 characteristic function, 445 characteristic polynomial, 78 Chebyshev inequality, 443 collimator, 332 common re\fnement, 622 comparison test, 616 completeness, 521, 574 axiom, 615 complex conjugation, 551 complex exponential, 68 higher dimensions, 114 complex numbers, 550 634 INDEX 635 complex plane, 551 condition number, 548 conditional convergence, 558 convergence generalized functions, 585 in the mean, 571 uniform, 570 with respect to a metric, 537 convergent sequence, 614 convergent subsequence, 614 convex region, 8 convolution and Fourier series, 198 de\fnitionRn , 122 de\fnition in higher dimensional periodic case, 215 de\fnition in periodic case, 197 derivatives and, 124 Fourier transform of, 85 of sequences, 198 one dimension, 84 coordinate vectors, 526 correlation coe\u000ecient, 448 matrix, 453 covariance, 448 matrix, 453 cross-correlation function, 495 cumulative distribution, 441 decay rate of, 74 decimal representation, 516, 522 \u000e-function, 92 density probability, 442 derivative classical, 620 generalized function, 584 di\u000berentiation rules of computation, 620 dimension, 530 Dirichlet kernel, 199 disk of radius r,48 distance function, 537 distribution function, 442 dot product, 540 dual vector space, 529 dynamic range, 34 e\u000bectively bandlimited, 232, 282 empty set, 426 ensemble average, 434 equivalent width, 261 Euclidean n-space, 525 event, 424 allowable, 424 expected value, 440 exponential polynomials, 191 extend by linearity, 528 fan beam scanner, 342 Fejer kernel, 206 Fejer means, 206 Fejer's theorem, 207 \flter, 243 bandpass, 257 cascade, 263 causal, 256 comb, 279 commuting, 248 high pass, 257 input, 243 inverse, 281 isotropic, 288 linear, 245 low pass, 232, 257 multiplication, 248 non-linear, 245 output, 243 passive linear, 267 shift invariant, 248 \flter mask, 320 \fltered backprojection, 143, 145 \fnite di\u000berence, 603, 608 \fnite dimensional distributions, 476 \fnite Fourier transform, 293 Fourier coe\u000ecients, 178 Fourier series higher dimensional, 213 inversion formula, 179, 197 inversion formula in higher dimensions, 213 localization principle, 211 partial sums, 179 partial sums in higher dimensions, 214 Fourier transform de\fnitionR1 ,69 de\fnitionRn -case, 113 derivatives, 75 di\u000berential equations, 78 functional notation F ,70 generalized functions, 108 inversion formulaR1 ,69 636 INDEX inversion formula, higher dimensions, 115 on L2,81 fractional derivative, 62 L2, 100 classical, 100 fractional integral, 62 frequency space description, 252 Fubini's theorem, 624 full width half maximum, 94 higher dimensions, 289 function L-periodic, 197 rect, 257 absolutely integrable, 69 bandlimited, 220 continuous, 618 di\u000berentiable, 620 Riemann integrable, 622 fundamental theorem of algebra, 551 Fundamental Theorem of Calculus, 624 FWHM, see full width half maximum Gamma function, 565 Gaussian focal spot, 368 Fourier transform, 71 Gaussian focal spot, 331 generalized function, 106, 581 n-dimensions, 587 generating function, 446 geometric distortion, 310 geometric series, 617 sum, 522 Gibbs number, 205 Gibbs phenomenon, 201 Gram-Schmidt, 546 in\fnite dimensions, 598 H¨older continuity, 62 H¨older space, 62 H¨older's inequality, 573 H¨older- 1 INDEX 637 function,Rn , 526 linear combination, 529 linear equations \fnite dimensions, 23 linear functional, 576 linear model, 24 linear operator, 28 linear span, 529 linear system determined, 25 overdetermined, 25 underdetermined, 26 linear transformation, 531 linearly independent, 530 little `o' and big `O' notation, 620 logarithm complex, 68 mathematical phantom, 374 matrix, 532 change of basis, 533 multiplication, 532 positive de\fnite, 550 sparse, 410 mean of a random variable, 440 Mean Value Theorem for integrals, 623 measurable set, 425 measure Lebesgue-Stieltjes, 433 measure space, 424 measure zero, 50 mesh size, 621 method of projections, 412 metric, 537 modulation transfer function, 252 moment conditions, 157 moments of a random variable, 443 of Radon transform, 158 MRI sampling in, 227 multi-index notation, 117 multiplication scalar, 526 mutually exclusive events, 427 Neumann series, 63 noise quantization, 238 quantum, 501 non-measurable set, 427 norm, 536 normal equations, 411, 550 null space, 23, 531 Nyquist rate, 222 Nyquist width, 262 Nyquist's theorem noise, 498 sampling, 220 operator norm, 538 oriented hyperplanes, 170 oriented linesR2 ,15 orthogonal complement, 188 orthogonal matrix, 546 orthogonal projection, 188 orthogonal vectors, 540 oversampling, 222 overshoot, 201 parallel beam scanner, 342 Parseval formula Fourier series, 188, 197 Fourier series in higher dimensions, 216 Fourier transform, 80 partial sums, 556 partition, 621 passband, 257 passive circuit elements, 269 periodic convolution, 294 phantom, 374 phase, 249 phase shift, 253 picture element, 319 pixel, 319, 340 point source, 254 two dimensions, 38 point spread function, 254 Poisson summation formula, 226 n-dimensions, 241 dual, 228 polynomial approximation Bernstein polynomials, 597 Weierstrass theorem, 595 power series, 560 principal value integral, 627 prior information, 417 probability, 427 probability measure, 427 probability space, 427 Pythagoras Theorem in\fnite dimensional, 190 638 INDEX QR factorization, 546 quantization, 237 Radon inversion formulaR2 , 138 Radon transform, 47 adjoint, 141 and the Laplace operator, 172 and the wave equation, 172 convolution property, 132 de\fnition in higher dimensions, 170 inverse for radial functions, 57 inversion formula in higher dimensions, 171 natural domain, 47, 135 Parseval formula, 137 radial functions, 48 random process, 474 Bernoulli, 476 continuous parameter, 475 discrete parameter, 475 independent, 476 stationary, 478 weak sense stationary, 478 random variable, 439 Bernoulli, 455 binomial, 455 complex, 439 Gaussian, 442, 457 Poisson, 456 rank value \fltering, 322 Ratio test, 617 ray, 343 real computable function, 555 real part, 551 reconstruction algorithm, 337 reconstruction grid, 340 recti\fer, 245 regularized inverse, 154 relative error, 548 relaxation parameters, 419 resistor, 267 Riemann Lebesgue Lemma, 73 Fourier series, 181 Fourier series in higher dimension, 214 Riemann sum, 603 upper and lower, 622 Riesz Representation Theorem, 576 Root test, 617 sample path, 475 sample points, 220 sample space, 424 sample spacing, 220, 600 sampling, 600 sampling rate, 220 Schwartz class, 106 semi-norm, 107 sequences real, 613 series, 616 shadow function, 16 side lobes, 94 signal-to-noise ratio, 456 simple function, 430 Simpson's rule, 605 sinc function, 95 sinogram, 346 SIRT, 420 smoothness principle, 394 Sobolev embedding theorem, 128 spectral density, 479 spectral function, 399 splines, 602 square integrable, 571 standard basis, 526 standard deviation, 444 state variables, 3 stationary increments, 482 wide sense, 482 step function, 595 strip integral, 367 subsequence, 613 subspace, 529 summation by parts formula, 559 support line, 8 support of a function, 35 Taylor's theorem, 621 tempered distributions, 586 tempered growth, 586 test function, 97, 127 thermal noise, 497 time average, 434 tomography, 34 transfer function, 252, 286 transpose, 23, 543 trapezoidal rule, 605 triangle inequality, 521 undersampling, 222 uniform continuity, 619 uniform sampling, 600 unit impulse, 92, 254 INDEX 639 upper triangular, 545 variance, 444 vector, 526 vector space complete normed, 574 complex, 553 dual, 576 normed, 536 real, 528 viewfourth generation machine, 346 parallel beam, 343 third generation fan beam, 346 Volterra operator, 63 voxel, 340 wave equation, 172 weak convergence, 578 weak derivative, 60, 97, 584 higher dimensions, 127 partial, 588 white noise, 491 Wiener \flter, 499 Wiener process, 482 zero padding, 298 higher dimensional, 303 640 INDEX","libVersion":"0.2.2","langs":""}